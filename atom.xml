<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tankcat</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tankcat2.com/"/>
  <updated>2017-12-10T14:51:12.059Z</updated>
  <id>http://tankcat2.com/</id>
  
  <author>
    <name>Tankcat</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>十二月的第二周</title>
    <link href="http://tankcat2.com/2017/12/10/diary1210/"/>
    <id>http://tankcat2.com/2017/12/10/diary1210/</id>
    <published>2017-12-10T14:50:51.493Z</published>
    <updated>2017-12-10T14:51:12.059Z</updated>
    
    <content type="html"><![CDATA[<h3 id="被各种杂事扰乱的工作日"><a href="#被各种杂事扰乱的工作日" class="headerlink" title="被各种杂事扰乱的工作日"></a>被各种杂事扰乱的工作日</h3><p>结束了上周五组会上的简陋survey，这周的工作日就不打算继续读新的paper了，给自己定了下面几个需要完成的目标：</p><ul><li style="list-style: none"><input type="checkbox" checked> 明珠这周讨论班要将讨论班，再读08年的CRB，把checkpoint的过程梳理一遍；</li><li style="list-style: none"><input type="checkbox" checked> Demo文章初稿，和蒋程交涉好前端需要做的；</li><li style="list-style: none"><input type="checkbox" checked> 关注NVMe；</li><li style="list-style: none"><input type="checkbox" checked> 思考并设计survey的维度；</li><li style="list-style: none"><input type="checkbox" checked> 开始学习Algorithm Specialization课程的学习</li></ul><p>周一一早来老板便交代了要开始准备1月初的Demo投稿，因为这个工作的后端实现是和房叔一起做的，已经出了两篇paper了，前端明珠之前实现了一个相对简单的版本，所以论文写起来应该不费事，所以上午先搞了注册的事情。今年有点懵逼啊，按照Demo track给的submission链接注册好个人信息了之后发现没有任何提交paper信息的按钮？？？最后只在Research track板块里给的链接找到了能投稿的地方，占了一个坑。paper的名字也是随意起的。正式写之前先找了几篇已经发表的demo paper看了看，页数限制，就简单安排了一下论文框架。然后抽出了之前工作的关键技术，简要介绍了一番。画系统架构图废了点时间，虽然以前的图也能用，但是太Low了，还是要好好设计一下，毕竟中了的话还能去土澳浪一波~下午三四点的时候在大众点评上选中了两家轰趴馆，晚上约了欢欢去“实地考察”了一番，第一家新开的，环境很不错但是价格不低；第二家环境一般价格也不是很低。回来之后和实验室的小伙伴讨论了一下，大家觉得场地费平摊下来人均不低，所以当天晚上没有确定结果。</p><p>周二早上6点半就起床了，因为天气预报说有太阳，于是洗漱之前把床单被单拿到楼下洗衣房洗，洗漱收拾好把被子和洗好的床被单拿出去晒。上午就继续写demo。中午回去发现晒被子的地方没有阳光了，所以就转移场地，拿到宿舍西边楼道的窗台上晒了。下午抽空继续商讨轰趴馆的选择。由于现在是黄金档，就近的几家价格都不便宜，大家因为价格原因不太愿意去，有的同学甚至建议直接组织出去唱歌然后举个餐就算了事了。但是毕竟有的人不喜欢唱歌啊，这样他们参与不进来，所以迟迟出不了结果。最后学姐让步了，她说她和小龙愿意承担1000块，算作是她毕业请吃饭。我把情况和大家说明了一下，可能是觉得学姐都让步到这个程度了，所以最终确定了还去轰趴。几个学弟还是很好说话的，他们本来也建议去轰趴，也不是很在乎钱的问题。轰趴馆就选的第一家，和店家确定之后交了五百押金，这事就算是安定了。晚饭期间去操场跑了三公里，快走了一公里。</p><p>周三是最忙的一天了。早上化了妆出门，以为下午颁奖典礼需要上台。上午本来打算把CRB好好再看一遍的，老板一个消息发过来要帮她买一桶羽毛球。一开始以为大活那边有得卖，哪知道体育器材店没有开门。遇到一个保安大叔，他说校门那边的体育馆有得卖，我就急匆匆地跑过去，发现就剩几只了，没有成桶卖的。然后又跑到校外的联华超市，被告知又没有！最后只能寄希望于教育超市了，如果在没有只能让老板自己淘宝了。万幸，教超有得卖。买到手的时候内心mmp，为啥一开始不先去教超呢，明明离得最近……买完已经10点半了，迅速回宿舍补了妆，然后去河东食堂吃了午饭，就去停车场等校车去闵行路。颁奖典礼一直开到下午三点多，根本用不着我们上台，事先确定好了学生代表上台领奖。返程路上看完了行尸走肉这周新更，全程高度紧张，片尾又留悬念，看完更饿了。到了学校之后赶紧去全家买了一个鸡肉卷饼。晚饭之前迅速浏览了一遍CRB，弄清楚了一些基本概念，比如COW、WAL。晚上我们FToS小分队三人继续了这周的小讨论班，帮明珠解答了论文中看不懂的疑惑，我自己也顺带加深一遍理解。</p><p>周四也是不消停的一天。这天的原计划是把CRB没看完的看完，再关注一下NVMe，顺带复习一下存储技术的基础知识，于是把去年翁老师的课件翻出来又刷了一遍。中途想起来还没问教务老师，转博要提交的材料到底有哪些，邮件上也没说明。跑过去问清楚了，缺的材料不少，专家推荐信还缺一份，于是加了老金的微信，老金当天去北京出差了，晚上还抽时间帮我填好了材料，感动。这天上午男票还犯傻，想骗我说521，被我识破，这个小插曲我写在前一篇日记里了。晚上去参加了三位同学的转预备党员小会。</p><p>周五上午继续学习NVMe。这天没什么特别的事情，下午讨论班也顺利进行了。除了将论文，李学长和老师还一起帮我梳理了后面利用NVMe可以考虑的研究点，跟李学长要了几篇文章，下周看看。晚上整理轰趴需要准备的食材、娱乐项目清单整理出来，在实验室群里说明了一下各项事宜。</p><p>PS：每天早上来实验室刷完单词之后花四十分钟左右的时间学习算法课程，并整理笔记，笔记以及相关的学习资料我挂在github上了(<a href="https://github.com/tankcat/algorithm-specialization-notes)，看视频的时候也想锻炼自己的英语，所以放的是英文字幕，希望自己以后能脱离字幕看视频吧。除了这个，这周把饮食健身打卡也从豆瓣小组的帖子转移到github上了，因为这样本地编辑起来更加方便(https://github.com/tankcat/diet-record)。" target="_blank" rel="noopener">https://github.com/tankcat/algorithm-specialization-notes)，看视频的时候也想锻炼自己的英语，所以放的是英文字幕，希望自己以后能脱离字幕看视频吧。除了这个，这周把饮食健身打卡也从豆瓣小组的帖子转移到github上了，因为这样本地编辑起来更加方便(https://github.com/tankcat/diet-record)。</a></p><h3 id="日程满满的周六周日"><a href="#日程满满的周六周日" class="headerlink" title="日程满满的周六周日"></a>日程满满的周六周日</h3><p>这次周六周日日程安排得满满当当：周六中午和原来313的两个学姐、一个学弟(欢欢、妍虹和高竹)一起去避风塘吃饭，周六晚上约了小慧去吃上周抽中的一家霸王餐；周日要组织实验室的小伙伴们一起去轰趴。</p><p>周六早上晚起了一会儿，发现牛油果再放就要坏了，于是打奶昔的时候顺便给早来实验室的一个学弟也做了一杯。学习好算法视频，写好周报，收拾了一下就出门去环球港的避风塘占座了。记得三四年前和本科室友们第一次在南京马群吃避风塘的时候，觉得超级好吃，今天发现质量严重下降，点心太甜菜太咸，连云吞面的汤也是，烧鹅肉质一点也不新鲜。席间学弟学姐们讲了不少工作上的事情。饭后他们三人去Coco各自买了奶茶，后来转战到宝珠奶酪，我又买了一杯牛油果雪酿，这次喝得比较慢，看着它慢慢融化，喝到最后越来越美味~在宝珠奶酪待了很久，听学弟说互联网的加班很辛苦，最近这段时间他经常加班到晚上十一二点才回家，说工作了之后才发现，真的比在学校辛苦太多，在学校就算老师给了deadline，其实没有按时交出结果也不是大问题，但是工作上很不一样，没有人会设身处地地为你想，没有结果就等于没做。想想自己决定转博，也不乏逃避找工作这一因素。聊完之后就在环球港逛了逛衣服，中午让老妈资助了五百，去买了上周相中的那件粉色卫衣。因为中午吃得太饱，下午又喝了一大杯雪酿，晚饭就没吃，回宿舍锻炼了半小时的HIIT。洗完澡回到实验室把临时不去轰趴吃午饭的人记录一下，确定了周日早上去菜市场采购的人员和时间点。安排好所有事情之后又看了会算法视频。</p><p>周日早上6点30起来，洗漱完撸了一个简易妆，然后就和采购小分队去买菜了。从枣阳路门出去，到光复西路左转，穿过强家角桥，发现了好多家崇明蔬菜市场，按照周五晚上定的食材清单采购完，就骑了单车去轰趴馆。由于时间还早，交接的人还没到，我们五个人就先在新客来大食堂坐了一会儿，给大家买了大肉包当早饭吃，后来直接在店里买了一些打包饭盒和一次性筷子。进了轰趴馆后，分配好洗菜任务，就继续和其中一个学弟去环球港买了点零食、主食和酱料。还买了一瓶牛奶，因为今天带了自己的电炖锅，给大家做了奶茶喝~陆陆续续人到了，中午就涮起火锅吃，16个人围着一个小吧台、两口锅还是有点挤的，不过吃得还算开心，口味还不错，嘿嘿，自夸一下。吃完大家就开始游戏了：有玩狼人杀的，有打桌球的，有打麻将的，有玩街机的，还有唱K的。这家KTV效果还是不错的，今天唱得很尽兴，哲神和韩易唱歌都超级厉害，周杰伦、陈奕迅的每首歌都唱的很6~很开心今天和哲神合唱了青峰和Ella的《你被写在我的歌里》，也很开心他们俩唱双截棍，我、小慧还有卷积在旁边附和，真心嗨爆了，可爱又迷人~再有就是后来祝翔加入，一起唱霍元甲~唱到一半累了，出去和鹏鑫打了两局桌球，平手~只要不让我设计那种可以反弹折射的角度打法，我还是可以的哈哈。晚上就把中午没吃掉的饺子煮了，由于锅不大，两大袋饺子足足分了四次才煮完。怕不够吃，还点了三个达美乐披萨，土豆味的一如既往地好吃~吃完饭又继续进去唱K，一直到最后，大家合影留念。这是我第一次主动组织大家聚会，我这个人吧喜欢简单，所以安排事情的时候就可能没有考虑很全面，反思一下，以后说话态度也要和善一点~总之，大家玩得还算尽兴，也辛苦起早陪我一起买菜得小慧、杨康、蒋程和汤路明，辛苦可人给大家拍照，也辛苦参与和支持的每一位小伙伴~</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      一周的流水线，只是记录，文笔很差。
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="周记" scheme="http://tankcat2.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="自省" scheme="http://tankcat2.com/tags/%E8%87%AA%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>十二月的第一周</title>
    <link href="http://tankcat2.com/2017/12/03/diary1203/"/>
    <id>http://tankcat2.com/2017/12/03/diary1203/</id>
    <published>2017-12-03T12:44:00.000Z</published>
    <updated>2017-12-03T12:49:09.142Z</updated>
    
    <content type="html"><![CDATA[<p>十一月就这么静悄悄地溜走了，回头一看，这一个月好像正经事没做成，但是仔细数数又似乎达成了不少目标。</p><ol><li>拿到了研究生国国家奖学金、校优秀学生、全国研究生数学建模二等奖</li><li>给爸爸换了手机，买了种草许久的Mate 9</li><li>体重下百了</li><li>订制了心仪的中古包、买了一整套Martiderm的平衡和臻活安瓶、买了性价比高的牛角梳</li></ol><h3 id="工作日的日常"><a href="#工作日的日常" class="headerlink" title="工作日的日常"></a>工作日的日常</h3><p>这周周五的讨论班轮到我讲，因为十一月的主要工作就是读paper，于是就准备做个简短的survey。算下来看了有二十多篇，由于没找到读paper的技巧，为了赶进度导致很多篇都没有真正理解，做起survey的时候才发现很多东西自己也讲不清楚。</p><p>一开始给自己这周的规划是，新论文就不读了，专心整理已经读过的。然而，周一又把时间花在自己的个人博客上了。无意间发现有人利用Github Issue改写了一个评论系统，我觉得还不错就想着借鉴一下，替换现有的disqus。一些配置问题占用了我一整个下午。晚上参加数模的三个团队，其实也就是实验室的小伙伴，说拿了奖一起约顿饭，于是就去秋林阁三楼点了一桌子菜。每次都必点青椒炒蛋和冬瓜排骨汤。吃完洗了澡，回到实验室继续看paper。但是又被一个妹纸的个人博客吸引住了，她的首页是 一张cv，而博客是二级域名。她把cv的制作公开在github上了，由于一些域名设置问题，我捣鼓了不少时间，最终还是放弃，十点半的时候回寝室睡觉了。</p><p>周二早上醒来觉得还是不甘心，cv我肯定是也要做一张的，但是不换域名了，直接利用HTML按照她的样式做了一个一模一样的，简单粗暴，替换成了博客的首页。博客的改造终于完成了，就继续做起了survey。原本打算先把上周没看完的一篇02年的survey看完，可是越看越觉得不理解，心情很烦躁。不巧中午来了大姨妈，肚子很疼，就荒废了一下午躺在床上休息。</p><p>周三算是真正开始写survey的presentation了。 没有跳出前人整理的框架，我继续沿用，只是合并了一些小分类，在整理具体系统的容错实现时发现自己还是无从下手，不知道怎么才能讲的清楚，又陷入了烦躁的情绪，没忍住还和男票哭诉了一番。晚上洗完澡发现自己感冒了，真是祸不单行。</p><p>周四继续写presentation，但是不巧，遇到老板开始帮我改老早之前的期刊文章。仍旧是定理证明的那块儿写的不好。这一块儿反反复复修改了不知道多少遍了，我早已厌烦。我承认，想要投稿期刊论文，又或者说以后想投稿顶会论文，没有两三个定理与证明是真说不过去的，这一块技能我确实欠缺了。下午老板把我叫过去，讲完问题所在之后我主动和她交流了一下这一个月来读论文的感受，说明了自己的矛盾所在，其实烦躁的根源在于自己平日偷懒、效率低下导致论文没有理解透彻，后来回了实验室，老板在微信了给我打了一剂鸡血。</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/jixue.jpg" alt=""></p><p>男票跟我说，做presentation的时候就尽力止损吧，把自己理解的讲清楚，不理解的说明一下情况。没想到，presentation竟然顺利进行了，主要也有李学长的助攻。李学长真的是个神童，经常就是我们自己讲presentation的人还没有他听的人理解得透彻。这大概就是知识储备的差距吧。讨论班结束之后，学弟还反映说这次连他也听懂了，嘿嘿。</p><p>这一周的工作日也就这些了，主要就是对看过的文章做了整理，但是层次还很浅显；这一周锻炼和背单词也搁置了没进行。接下来又重新制定工作计划，把没有理解的概念、技术重新弄懂，paper也要继续读，设计分类的维度，整理好每一维度上各个系统、技术的实现、优缺点等等。</p><p>ps：看到一段鸡血，或者说拖延症的治疗技巧。</p><blockquote><p>朋友们，我的一点切身经验，如果你觉得某个任务让你特别焦虑，压得你喘不过气来，那么最好的排解方法就是直接去做这事，什么都别管，就是使劲做，努力地推进其进度，这棘手的事情在进度上每发展一点，你的焦虑就会少一分，同时你的焦虑越少，推进的速度也就越快，只要咬紧牙关，不停地推进，总会有解脱的那一天，而且你每完成一个棘手的任务，你或多或少都会比之前牛逼强大那么一点，这件苦差事总是会改变你一些。<br>真的，诸位，有什么难事千万别耗着，别等着，那只会让人在无尽的焦虑中煎熬，你就先大吼一句：“去你妈的。” 然后两眼血丝地去推进，去做事，做着做着就有出路了。 </p></blockquote><h3 id="短暂的周末相聚"><a href="#短暂的周末相聚" class="headerlink" title="短暂的周末相聚"></a>短暂的周末相聚</h3><p>原本周五晚上讨论班结束就要去无锡的，老早就跟男票定好这周末去无锡玩，正好两人折中路程。然后计划赶不上变化，今天他公司有年会，再加上我感冒了，于是去无锡的计划就推迟了。一个月没见面了，甚是想念，最后决定这周他来上海找我。这次来上海就不打算去较远的地方拔草了，就带带他去我我比较喜欢的黄焖鸡、汤包店还有甜品店吃吃喝喝，顺便去商场逛逛，买点衣服，还要带他去枣阳路的那家星巴克，我实在是太喜欢这家店了。</p><p>周五晚上关掉了闹钟，早早爬床，周六早上睡到自然醒，睡前用了臻活安瓶，洗脸的时候发现毛孔又小了欸~早饭去全家买了一个黑胡椒鸡排三明治，里面有鱼子蛋沙拉，木有喜士多的好吃~买完回实验室，在Coursera上找了一门Standford的算法课，打算以后每周周末抽出半天时间恶补一下薄弱的算法基础。中午去河东食堂吃了一碗冒菜，十二点了人还是很多，这次没吃全素，加了一根里脊肉和两块午餐肉，可能是人多煮得比较急吧，番茄汤底又油又咸……吃完回实验室继续啃算法，中途微信上不断骚扰男票，问他酒宴吃得怎么样了。原来说好坐四点的高铁过来的，可以他们吃完酒宴还要聊天，他就改签了五点。想到他周一上午要开例会，所以周日下午就得回去，这次相聚的时间本来就比较短，他还改签推迟了！于是当然要生气啦！反正后面他给我发微信我都没鸟他，自己一个人收拾好洗漱用品，去宾馆办了入住，然后就静静地看起了电视。。。他将近七点多才到，我自己饿得受不了了就点了一份老乡家香酥鸡柳和炸里脊，里脊还是小时候在文峰大世界妈妈买给我吃的味道~接到他之后，两人就去环球港觅食啦~去宝珠奶酪家发现没有牛油果奶昔了T^T，想吃点菜的但是想到我们是贫穷夫妻俩，就决定去大时代随便吃吃，买了一笼富春苏式汤包，还点了三个蒸菜，都很难吃！后悔了，还不如去吃点菜呢！敲生气！吃完就在环球港随便逛了逛，发现了一家新开的衣服店叫MM麦檬，衣服都挺不错了，试了一件白色的韩系羽绒服，穿着像披了一条棉被哈哈，不过店里的镜子都是显瘦的，虽然臃肿，但是时尚感不减~由于价格原因，又因为上周刚在网上预定了一件羽绒服，没买就回去了~上楼之前又去林间小屋买了一个巧克力杯子蛋糕，一如既往的好吃~</p><p>周日早上也是早早起床，因为想带他去吃很多我觉得好吃的小吃。他昨晚没怎么吃，蔡师傅汤包店又有点远，就先去林间小屋买了一个巧克力麻薯填填肚子~意外得好吃呀，难怪大众点评上不少人推荐。穿过了金沙江路，到与枣阳路的交汇处就是蔡师傅汤包店了。早上人还是挺多的，中老年人居多，大概都是周围的居民，点了两碗小馄饨、一笼汤包还有一个小小的甜口烧饼。小馄饨的汤里估计是放了猪油，很好喝，汤包有点酸，不知道怎么回事。我吃了两个汤包和几颗小馄饨就饱了，他估计是真饿了，把我剩下的小馄饨也都吃光光了，哈哈哈~吃完带他去买了老香斋的蝴蝶酥和榴莲酥，然后步行回宿舍放下书包，两人就又去环球港逛吃逛吃了。主要原因是前一天晚上决定还是去把那件羽绒服买下来，网上预定的那一件可以退了。先带他去吃说了好几次的Godiva的冰淇淋，今天没有优惠券，50块买了一只全巧克力味的，好甜好甜，我自己是不太喜欢吃啦。吃完就去买衣服了，为了下周的奖学金颁奖典礼，还特地买了一条正式一点的半身裙。买完在店里休息了一会儿，顺便又试了一件粉色的卫衣，外加N件大衣~哎，要是有钱就全拿下啦，都很喜欢~时间还早，两人就去宝珠奶酪坐了会儿，买了一杯新出的酒酿酸奶。上上周在店里有店员拿给我试吃，当时觉得好不错，今天吃感觉一言难尽……午饭就带他去吃很喜欢的一家黄焖鸡，点了两碗小份，我的另加了一份土豆和金针菇，他不喜欢吃金针菇，就给换成了青菜。中途还遇到了室友，哈哈，这也是她第一次和男票打招呼。吃完在学校里逛了逛，坐在河边看看风景，落叶、小河还有垂钓的老爷爷，好不休闲~因为感冒还没好，原本买的六点的车票，让他提前改签到三点了。这次没送他去车站，看着他进地铁的时候实在是太难过了，忍不住又哭了起来。现在相聚的次数越来越少，每次相聚的时间也很短暂。原来刚在一起的时候不觉得异地恋有啥，现在感情越来越深，对他越来越依赖，就感觉异地恋真的很辛苦。送他走后，我实在是太累了，回去一觉睡到五点。</p><p>洗完澡回到实验室，更新了这篇日记。这一周就要过去啦，明天迎来新的一周，继续奋斗吧，为了美好的将来~</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      一周的流水线，只是记录，文笔很差。
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="周记" scheme="http://tankcat2.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="自省" scheme="http://tankcat2.com/tags/%E8%87%AA%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>十一月的第四周</title>
    <link href="http://tankcat2.com/2017/11/26/diary1126/"/>
    <id>http://tankcat2.com/2017/11/26/diary1126/</id>
    <published>2017-11-26T10:29:45.000Z</published>
    <updated>2017-11-27T13:25:19.994Z</updated>
    
    <content type="html"><![CDATA[<h3 id="工作日的survey"><a href="#工作日的survey" class="headerlink" title="工作日的survey"></a>工作日的survey</h3><p>这个月进行survey的第四周，工作日的五天细读了两篇微软的流处理系统paper，一篇Naiad和一篇Falkirk Wheel；一篇很经典的Dynamo，十年前就发表的这篇今年喜获SIGOPS名人堂奖，重点关注了它的高可用是如何实现的；一篇综述，其中一个作者和Naiad、Falkirk Wheel联系密切，这篇综述关注的点很基础。</p><p>算下来从开始做survey，看过的文章也有二十多篇了，关于容错，心中确实有了一点感觉，但觉得还不够，有的时候回头想想看过的文章，竟然不能一句话讲出它做了什么，看过就忘成了很大的问题。男票告诉我，这是因为我没有理解透彻。所以，现在就有点迷茫，到底什么文章该精读，精读到什么程度是够的，要想真正理解一篇文章的proposal，耗时不短，这当中该怎么做权衡呢？</p><p>还有一点需要反省，就是效率问题，一周只看了四篇文章，很大一部分时间还是被我浪费了，经常看文章看到一半就去找人聊天、逛豆瓣微博，没有完整、高效率的学习时间，这应该也是导致文章理解不透彻的原因之一吧。</p><p>不过，有进步的是，开始定制周计划了，也切实进行了，希望在次基础上提高效率并继续保持。</p><h3 id="周六周日的吃吃玩玩"><a href="#周六周日的吃吃玩玩" class="headerlink" title="周六周日的吃吃玩玩"></a>周六周日的吃吃玩玩</h3><p>周六中午出门，骑单车去武康路的一家星巴克臻选店买工业风的杯子，哪里知道好不容易找到这家店却被告之没有存货，感觉委屈的不行。直接返程肯定是不行的，不然就白出来了，于是就在武昌路和湖南路附近逛逛，后来又去一家网红面店吃了一碗辣肉面。然而，十个网红店九个是垃圾，这家面店很荣幸也是垃圾，38块一碗的面还不如河西食堂3块一碗的阳春面。返程的时候选择步行，正好记一记路线。从武康路-兴国路-华山路-江苏路-愚园路-长宁路-凯旋路-万航渡路-光复西路-枣阳路，短短五公里串联了这么多道路，有些路很小资、适合拍照，有些路就普普通通、大众化。走到长宁路上的兆丰广场，看到有Bose专柜就想顺便试试音质，试玩就心动了，比我的大法轻多了。走之前顺便又去nitori买了一口雪平锅，想着宿舍厨房里有电磁炉，以后可以熬奶茶、煮泡面吃，可是买的时候没看清楚，今天早上发现这锅只能用煤气加热。晚上懒得回实验室了，和男票商量了一下要讨论微信小程序的需求问题，于是又背着包出门，选择在枣阳路的这家星巴克讨论。环境还是不错的，有无线，周六晚上人很少，买了一杯太妃榛果，店员还给了一些蛋糕试吃。讨论完肚子又饿了，就去蔡师傅汤包店点了一碗小馄饨和一块素鸡，美味又划算。最后，回寝室洗漱。哦对了，昨天是老爸生日，舅舅一家去家里，燕子姐姐又给爸爸买了礼物。听老妈说，爸爸这次很感动，不仅燕子姐姐送了礼物，我又送了手机，好像老爸都哭了。</p><p>周日赖了床，早上8点半才醒，准备用雪平锅煮奶茶的时候发现电磁炉不能加热，没办法只能换成电炖锅来做了。牛奶是在盒马鲜生上买的明治鲜奶，加了两包立顿红茶包，这次冰糖又放多了，不过口感还是很醇厚的，虽然比较甜，和外面比起来还是健康很多的。给实验室的小伙伴带了一点，评价都不错。中午去了实验室，忙着把Hexo个人博客需要的环境在新笔记本上重新搭建了一下，遇到了不少问题，但是都解决了。搞完已经三点了，在大众点评上选择了一家中江路那边的日料店，骑了单车过去，由于不是饭点，店里除了我没其他顾客，点了三文鱼刺身、炸土豆饼、煎饺和炸鸡肉串，很快就消灭了~</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>这一周过得还是挺充实了，周一早上体重还下百了，激动得我发了一条朋友圈。可是下百就一天，接着几天就无所顾忌地吃，周一把剩余的牛油果继续做了奶昔喝，周二吃了冒菜，周三又跑去吃了煲宫，周五还吃了麻辣香锅和芝麻糊小圆子，周六周日前面已经说了。减肥我还是会继续的，只是不会再像一开始那么严格了，该吃吃还是要吃，该锻炼也还是会锻炼，健康的前提下保持身材。技能上，</p><p>就这样吧，明天又是新的一周，下周五轮到我讲survey，还有四天半的时间好好准备，加油吧二筒子~</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      一周的流水线，只是记录，文笔很差。
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="周记" scheme="http://tankcat2.com/tags/%E5%91%A8%E8%AE%B0/"/>
    
      <category term="自省" scheme="http://tankcat2.com/tags/%E8%87%AA%E7%9C%81/"/>
    
  </entry>
  
  <entry>
    <title>冬日就该来杯热乎的焦糖奶茶~</title>
    <link href="http://tankcat2.com/2017/11/25/milktea/"/>
    <id>http://tankcat2.com/2017/11/25/milktea/</id>
    <published>2017-11-25T01:22:47.000Z</published>
    <updated>2017-11-27T13:43:01.445Z</updated>
    
    <content type="html"><![CDATA[<p>关注的好几个美食博主都各自分享了自制奶茶的教程，我选择了大胃爱丽丝的焦糖奶茶，因为最简单，除了原材料就只需要一个微博里就可以了，我这么懒，怎么会还专门买个奶锅回来呢~买了奶锅回来可能就只煮方便面了吧hahaha…</p><p>下面是我交的作业，我的材料和微波炉和教程里有出入，所以每个步骤耗时也会有所不同。</p><p><strong>首先是原材料：</strong></p><ol><li><p>安佳全脂牛奶一瓶 250ml。这里大家可以自行选择牛奶的品牌，我是喝惯了安佳，觉得奶香很足。</p></li><li><p>冰糖 6颗。我没用白砂糖的原因是宿舍里只有冰糖，我懒，不想出去买，哈哈哈。</p></li><li><p>立顿红茶包 2袋。</p></li><li><p>玻璃杯，可微波炉加热。</p></li><li><p>微波炉。宿舍厨房的微波炉功率感觉不行，每个加热步骤我都比原教程里的长。</p></li></ol><p><strong>其次是具体加热步骤：</strong></p><ol><li>冰糖加一点儿水，放进微波炉叮4分半钟。因为冰糖不容易化，加上微波炉功率不行，原教程只要叮1分半钟就能熬出焦糖色，我足足叮了三次。</li><li>取出杯子，加入全脂牛奶和茶包，放进微波炉里叮2分钟。</li><li>再次取出杯子，用勺子轻轻挤压茶包，轻轻搅拌杯底的焦糖，放进微波炉里叮2分钟。这里一定要注意！立顿的茶包好容易挤破，我就是挤破了QAQ茶叶碎都溢出来了，最后只能等到沉淀了才能喝。</li><li>开喝。</li></ol><p>PS: 个人觉得冰糖放得还是有点儿多，因为喝的时候觉得好甜啊，连室友也说太甜了…下次我要试试不放冰糖，看看是不是真的很影响口感~</p><p>以上，谢谢观看~</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关注的好几个美食博主都各自分享了自制奶茶的教程，我选择了大胃爱丽丝的焦糖奶茶，因为最简单，除了原材料就只需要一个微博里就可以了，我这么懒，怎么会还专门买个奶锅回来呢~买了奶锅回来可能就只煮方便面了吧hahaha…&lt;/p&gt;
&lt;p&gt;下面是我交的作业，我的材料和微波炉和教程里有出入，所以每个步骤耗时也会有所不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;首先是原材料：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;安佳全脂牛奶一瓶 250ml。这里大家可以自行选择牛奶的品牌，我是喝惯了安佳，觉得奶香很足。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;冰糖 6颗。我没用白砂糖的原因是宿舍里只有冰糖，我懒，不想出去买，哈哈哈。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;立顿红茶包 2袋。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;玻璃杯，可微波炉加热。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;微波炉。宿舍厨房的微波炉功率感觉不行，每个加热步骤我都比原教程里的长。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;其次是具体加热步骤：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;冰糖加一点儿水，放进微波炉叮4分半钟。因为冰糖不容易化，加上微波炉功率不行，原教程只要叮1分半钟就能熬出焦糖色，我足足叮了三次。&lt;/li&gt;
&lt;li&gt;取出杯子，加入全脂牛奶和茶包，放进微波炉里叮2分钟。&lt;/li&gt;
&lt;li&gt;再次取出杯子，用勺子轻轻挤压茶包，轻轻搅拌杯底的焦糖，放进微波炉里叮2分钟。这里一定要注意！立顿的茶包好容易挤破，我就是挤破了QAQ茶叶碎都溢出来了，最后只能等到沉淀了才能喝。&lt;/li&gt;
&lt;li&gt;开喝。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;PS: 个人觉得冰糖放得还是有点儿多，因为喝的时候觉得好甜啊，连室友也说太甜了…下次我要试试不放冰糖，看看是不是真的很影响口感~&lt;/p&gt;
&lt;p&gt;以上，谢谢观看~&lt;/p&gt;
    
    </summary>
    
      <category term="Skills" scheme="http://tankcat2.com/categories/Skills/"/>
    
    
      <category term="焦糖奶茶" scheme="http://tankcat2.com/tags/%E7%84%A6%E7%B3%96%E5%A5%B6%E8%8C%B6/"/>
    
  </entry>
  
  <entry>
    <title>Martiderm安瓶使用感</title>
    <link href="http://tankcat2.com/2017/11/24/Martiderm/"/>
    <id>http://tankcat2.com/2017/11/24/Martiderm/</id>
    <published>2017-11-24T01:28:31.000Z</published>
    <updated>2017-11-27T13:18:12.937Z</updated>
    
    <content type="html"><![CDATA[<p>这又是一记安利~</p><p>先交代一下本人的肤质吧：</p><ol><li>初中开始长青春痘</li><li>高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印</li><li>前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干</li><li>目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长</li></ol><p>总结一句话，就是<strong>混油痘肌</strong>。</p><p>半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，<strong>臻活</strong>和<strong>平衡</strong>系列各五只，我的目的很明确，祛痘印+提亮肤色。</p><p>臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用<strong>伊索的绿茶水和无油保湿精华</strong>打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。</p><p>可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这又是一记安利~&lt;/p&gt;
&lt;p&gt;先交代一下本人的肤质吧：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初中开始长青春痘&lt;/li&gt;
&lt;li&gt;高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印&lt;/li&gt;
&lt;li&gt;前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干&lt;/li&gt;
&lt;li&gt;目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总结一句话，就是&lt;strong&gt;混油痘肌&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，&lt;strong&gt;臻活&lt;/strong&gt;和&lt;strong&gt;平衡&lt;/strong&gt;系列各五只，我的目的很明确，祛痘印+提亮肤色。&lt;/p&gt;
&lt;p&gt;臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用&lt;strong&gt;伊索的绿茶水和无油保湿精华&lt;/strong&gt;打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。&lt;/p&gt;
&lt;p&gt;可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~&lt;/p&gt;
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="护肤" scheme="http://tankcat2.com/tags/%E6%8A%A4%E8%82%A4/"/>
    
      <category term="安瓶" scheme="http://tankcat2.com/tags/%E5%AE%89%E7%93%B6/"/>
    
      <category term="Martiderm" scheme="http://tankcat2.com/tags/Martiderm/"/>
    
  </entry>
  
  <entry>
    <title>视频导出音频小技能</title>
    <link href="http://tankcat2.com/2017/11/22/ffmped/"/>
    <id>http://tankcat2.com/2017/11/22/ffmped/</id>
    <published>2017-11-22T02:44:51.000Z</published>
    <updated>2017-11-27T13:24:11.915Z</updated>
    
    <content type="html"><![CDATA[<p>早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。</p><p>视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——<a href="http://audio-extractor.net/cn/" target="_blank" rel="noopener">http://audio-extractor.net/cn/</a>，需要先上传视频，再点击转换，最后再下载音频。</p><p>视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！<a href="http://ffmpeg.org/" target="_blank" rel="noopener">http://ffmpeg.org/ </a>这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~</p><p>我是把MP4转换成MP3，在别人的博客里找到了下面的命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3</span><br></pre></td></tr></table></figure><p>以上，谢谢阅读。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。&lt;/p&gt;
&lt;p&gt;视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——&lt;a href=&quot;http://audio-extractor.net/cn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://audio-extractor.net/cn/&lt;/a&gt;，需要先上传视频，再点击转换，最后再下载音频。&lt;/p&gt;
&lt;p&gt;视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！&lt;a href=&quot;http://ffmpeg.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://ffmpeg.org/ &lt;/a&gt;这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~&lt;/p&gt;
&lt;p&gt;我是把MP4转换成MP3，在别人的博客里找到了下面的命令：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;以上，谢谢阅读。&lt;/p&gt;
    
    </summary>
    
      <category term="Skills" scheme="http://tankcat2.com/categories/Skills/"/>
    
    
      <category term="ffmpeg" scheme="http://tankcat2.com/tags/ffmpeg/"/>
    
  </entry>
  
  <entry>
    <title>鼓楼半日记</title>
    <link href="http://tankcat2.com/2017/08/15/gulou/"/>
    <id>http://tankcat2.com/2017/08/15/gulou/</id>
    <published>2017-08-15T12:11:31.000Z</published>
    <updated>2017-11-27T13:23:57.620Z</updated>
    
    <content type="html"><![CDATA[<p>今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。</p><p>快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。<br><a id="more"></a><br><img src="http://7xwggp.com1.z0.glb.clouddn.com/yisuo.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。&lt;/p&gt;
&lt;p&gt;快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。&lt;br&gt;
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
  </entry>
  
  <entry>
    <title>Tragic Ending or Peace Ending ?</title>
    <link href="http://tankcat2.com/2017/07/21/my%20chester/"/>
    <id>http://tankcat2.com/2017/07/21/my chester/</id>
    <published>2017-07-20T16:00:00.000Z</published>
    <updated>2017-11-27T13:17:58.461Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人….</p></blockquote><p>收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。</p><p>各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。</p><p>有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。</p><p>西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。</p><p>他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。</p><p>I wanna let go of the pain I’ve felt so long…</p><p>somewhere i belong…</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人….&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。&lt;/p&gt;
&lt;p&gt;各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。&lt;/p&gt;
&lt;p&gt;有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。&lt;/p&gt;
&lt;p&gt;西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。&lt;/p&gt;
&lt;p&gt;他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。&lt;/p&gt;
&lt;p&gt;I wanna let go of the pain I’ve felt so long…&lt;/p&gt;
&lt;p&gt;somewhere i belong…&lt;/p&gt;
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="Linkin Park" scheme="http://tankcat2.com/tags/Linkin-Park/"/>
    
      <category term="Chester Bennington" scheme="http://tankcat2.com/tags/Chester-Bennington/"/>
    
  </entry>
  
  <entry>
    <title>Storm UI详解</title>
    <link href="http://tankcat2.com/2017/05/22/storm_ui/"/>
    <id>http://tankcat2.com/2017/05/22/storm_ui/</id>
    <published>2017-05-22T08:32:31.000Z</published>
    <updated>2017-11-27T13:08:44.357Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://7xwggp.com1.z0.glb.clouddn.com/storm_ui_config_summary.png" alt=""></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/storm_ui_config_summary.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="Storm" scheme="http://tankcat2.com/tags/Storm/"/>
    
  </entry>
  
  <entry>
    <title>Storm Kafka之KafkaSpout</title>
    <link href="http://tankcat2.com/2017/05/18/KafkaSpout/"/>
    <id>http://tankcat2.com/2017/05/18/KafkaSpout/</id>
    <published>2017-05-18T12:11:31.000Z</published>
    <updated>2017-11-27T13:20:19.551Z</updated>
    
    <content type="html"><![CDATA[<p><code>storm-kafka-XXX.jar</code>提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。</p><p>对于核心Storm与Trident两个版本的Spout实现，提供了<code>BrokerHost</code>接口，跟踪Kafka broker host$\rightarrow$partition的映射，并提供<code>KafkaConfig</code>接口来控制Kafka相关的参数。下面就这以上两点进行讲解。</p><h3 id="BrokerHost"><a href="#BrokerHost" class="headerlink" title="BrokerHost"></a>BrokerHost</h3><p>为了对Kafka spout进行初始化，我们需要创建一个<code>BrokerHost</code>的实例，Storm共提供了两种实现方式：</p><ol><li><p>ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZkHosts</span><span class="params">(String brokerZkStr,String brokerZkPath)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ZkHosts</span><span class="params">(String brokerZkStr)</span></span></span><br></pre></td></tr></table></figure><p>其中，<code>brokerZkStr</code>是<code>ip:host</code>(主机:端口)，<code>brokerZkPath</code>是存放所有topic和partition信息的根目录，默认值为<code>\broker</code>。默认地，Zookepper每60秒刷新一次broker$\rightarrow$partition，通过<code>host:refreshFreqSecs</code>可以改变这个时间。</p></li><li><p>StaticHosts。这是另一个选择，不过broker$\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建<code>GlobalPartitionInformation</code>类的实例，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Broker brokerForPartition0 = <span class="keyword">new</span> Broker(<span class="string">"localhost"</span>);<span class="comment">//localhost:9092,端口号默认为9092</span></span><br><span class="line">Broker brokerForPartition1 = <span class="keyword">new</span> Broker(<span class="string">"localhost"</span>,<span class="number">9092</span>);<span class="comment">//localhost:9092,显示地指定端口号</span></span><br><span class="line">Broker brokerForPartition2 = <span class="keyword">new</span> Broker(<span class="string">"localhost:9092"</span>);</span><br><span class="line">GlobalPartitionInformation partitionInfo = <span class="keyword">new</span> GlobalPartitionInformation();</span><br><span class="line">partitionInfo.addPartition(<span class="number">0</span>, brokerFroPartition0);<span class="comment">// partition0 到 brokerForPartition0的映射</span></span><br><span class="line">partitionInfo.addPartition(<span class="number">1</span>, brokerFroPartition1);</span><br><span class="line">partitionInfo.addPartition(<span class="number">2</span>, brokerFroPartition2);</span><br><span class="line">StaticHosts hosts = <span class="keyword">new</span> StaticHosts(partitionInfo);</span><br></pre></td></tr></table></figure></li></ol><h3 id="KafkaConfig"><a href="#KafkaConfig" class="headerlink" title="KafkaConfig"></a>KafkaConfig</h3><p>创建KafkaSpout需要的另一个参数是<code>KafaConfig</code>，通过调用以下两个函数进行对象创建：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConfig</span><span class="params">(BrokerHosts host,String topic)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">KafkaConfig</span><span class="params">(BrokerHosts host,String topic,String clientId)</span></span></span><br></pre></td></tr></table></figure><p>其中，<code>host</code>可以为BrokerHost的任何一种实现，<code>topic</code>是一个topic的名称，<code>clientId</code>是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。</p><p>目前，KafkaConfig有两种扩展形式，<code>SpoutcConfig</code>提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。<code>zkRoot</code>用于存储consumer的offset，<code>id</code>用于唯一标识当前的spout。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">SpoutConfig</span><span class="params">(BrokerHosts hosts,String topic,String zkRoot,String id)</span></span></span><br></pre></td></tr></table></figure><p>除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将当前的offset保存到Zookeeper的频率</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">long</span> stateUpdateIntervals = <span class="number">2000</span>;</span><br><span class="line"><span class="comment">//用于失效消息的重试策略</span></span><br><span class="line"><span class="keyword">public</span> String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();</span><br><span class="line"><span class="comment">//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">long</span> retryInitialDetails = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">double</span> retryDelayMultiplier = <span class="number">1.0</span>;</span><br><span class="line"><span class="comment">//连续重试之间的最大延时</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">long</span> retryDelayMaxMs = <span class="number">60</span> * <span class="number">1000</span>;</span><br><span class="line"><span class="comment">//当retryLimit低于0时，不停地重新发送失效的消息</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span> retryLimit = -<span class="number">1</span>;</span><br></pre></td></tr></table></figure><a id="more"></a><p>KafkaSpout只接收一个SpoutConfig的实例作为参数。</p><p>下面给出一个实例：</p><ol><li><p>首先创建一个名为<code>couple</code>的topic，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper localhost:3030 --partitions 4 --replication-factor 1 --topic couple</span><br></pre></td></tr></table></figure></li><li><p>写一个简单的Producer，将文件<code>string_data.txt</code>的记录发送到<code>couple</code>中，如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">kafkaFileProducer</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic_name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String file_name;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> KafkaProducer&lt;String,String&gt; producer;</span><br><span class="line">    <span class="keyword">private</span> Boolean isAsync;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">kafkaFileProducer</span><span class="params">(String topic_name,String file_name,Boolean isAsync)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.file_name=file_name;</span><br><span class="line">        <span class="keyword">this</span>.topic_name=topic_name;</span><br><span class="line">        Properties properties=<span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        properties.put(<span class="string">"client.id"</span>,<span class="string">"CoupleProducer"</span>);</span><br><span class="line">        properties.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        properties.put(<span class="string">"value.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        producer=<span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line">        <span class="keyword">this</span>.isAsync=isAsync;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">(String key,String value)</span></span>&#123;</span><br><span class="line">        <span class="keyword">long</span> start_time=System.currentTimeMillis();</span><br><span class="line">        <span class="keyword">if</span>(isAsync)&#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic_name,key),<span class="keyword">new</span> CoupleCallBack(start_time,key,value));</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic_name,key,value)).get();</span><br><span class="line">                System.out.println(<span class="string">"Sent message : ( "</span>+key+<span class="string">" , "</span>+value+<span class="string">" )"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        String file_name=<span class="string">"DataSource/Data/string_data.txt"</span>;</span><br><span class="line">        String topic_name=<span class="string">"couple"</span>;</span><br><span class="line">        kafkaFileProducer producer=<span class="keyword">new</span> kafkaFileProducer(topic_name,file_name,<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> lineCount=<span class="number">0</span>;</span><br><span class="line">        FileInputStream fis=<span class="keyword">null</span>;</span><br><span class="line">        BufferedReader br=<span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            fis=<span class="keyword">new</span> FileInputStream(file_name);</span><br><span class="line">            br=<span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(fis));</span><br><span class="line">            String line=<span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">while</span>((line=br.readLine())!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                lineCount++;</span><br><span class="line">                producer.sendMessage(lineCount+<span class="string">""</span>,line);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (FileNotFoundException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">CoupleCallBack</span> <span class="keyword">implements</span> <span class="title">Callback</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> start_time;</span><br><span class="line">        <span class="keyword">private</span> String key;</span><br><span class="line">        <span class="keyword">private</span> String message;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">CoupleCallBack</span><span class="params">(<span class="keyword">long</span> start_time, String key, String message)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.start_time = start_time;</span><br><span class="line">            <span class="keyword">this</span>.key = key;</span><br><span class="line">            <span class="keyword">this</span>.message = message;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">            A callback method</span></span><br><span class="line"><span class="comment">            The user can implement to provide asynchronous handling of request completion.</span></span><br><span class="line"><span class="comment">            The method will be called when the record sent to the server has been acknowledged.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">long</span> elapsed_time=System.currentTimeMillis()-start_time;</span><br><span class="line">            <span class="keyword">if</span>(recordMetadata!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                System.out.println(<span class="string">"Message( "</span>+key+<span class="string">" , "</span>+ message+<span class="string">" ) sent to partition("</span>+recordMetadata.partition()+<span class="string">" ) , offset("</span> +recordMetadata.offset()+<span class="string">" ) in "</span>+elapsed_time+<span class="string">" ms"</span>);</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>编写一个简单的Storm Topology，利用KafkaSpout读取couple中的数据(一条条的句子)，并分割成一个个的单词，统计单词个数，如下：</p><ul><li><p>SplitSentenceBolt</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitSentenceBolt</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector basicOutputCollector)</span> </span>&#123;</span><br><span class="line">        String sentence=tuple.getStringByField(<span class="string">"msg"</span>);</span><br><span class="line">        System.out.println(tuple.getSourceTask()+<span class="string">":"</span>+sentence);</span><br><span class="line">        String[] words=sentence.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word:words)&#123;</span><br><span class="line">            basicOutputCollector.emit(<span class="keyword">new</span> Values(word));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>WordCountBolt</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountBolt</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String,Long&gt; counts=<span class="keyword">null</span>;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map conf,TopologyContext context)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.counts=<span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">super</span>.prepare(conf,context);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector basicOutputCollector)</span> </span>&#123;</span><br><span class="line">        String word=tuple.getStringByField(<span class="string">"word"</span>);</span><br><span class="line">        Long count=<span class="keyword">this</span>.counts.get(word);</span><br><span class="line">        <span class="keyword">if</span>(count==<span class="keyword">null</span>)</span><br><span class="line">            count=<span class="number">0L</span>;</span><br><span class="line">        count++;</span><br><span class="line">        <span class="keyword">this</span>.counts.put(word,count);</span><br><span class="line">        basicOutputCollector.emit(<span class="keyword">new</span> Values(word,count));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>,<span class="string">"count"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>ReportBolt</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReportBolt</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span></span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector basicOutputCollector)</span> </span>&#123;</span><br><span class="line">        String word=tuple.getStringByField(<span class="string">"word"</span>);</span><br><span class="line">        Long count=tuple.getLongByField(<span class="string">"count"</span>);</span><br><span class="line">        String reportMsg=<span class="string">"&#123; word : "</span>+word+<span class="string">" , count : "</span>+count+<span class="string">" &#125;"</span>;</span><br><span class="line">        basicOutputCollector.emit(<span class="keyword">new</span> Values(reportMsg));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer outputFieldsDeclarer)</span> </span>&#123;</span><br><span class="line">        outputFieldsDeclarer.declare(<span class="keyword">new</span> Fields(<span class="string">"message"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>KafkaWordCountTopology</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountKafkaTopology</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String KAFKA_SPOUT_ID=<span class="string">"kafka-spout"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String SPLIT_BOLT_ID=<span class="string">"split-bolt"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String WORD_COUNT_BOLT_ID=<span class="string">"word-count-bolt"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String REPORT_BOLT_ID=<span class="string">"report-bolt"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONSUME_TOPIC=<span class="string">"couple"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String PRODUCT_TOPIC=<span class="string">"test"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ROOT=<span class="string">"/couple"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK_ID=<span class="string">"wordcount"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPOLOGY_NAME=<span class="string">"word-count-topology"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">        BrokerHosts brokerHosts=<span class="keyword">new</span> ZkHosts(<span class="string">"192.168.1.118:3030"</span>);</span><br><span class="line">        SpoutConfig spoutConfig=<span class="keyword">new</span> SpoutConfig(brokerHosts,CONSUME_TOPIC,ZK_ROOT,ZK_ID);</span><br><span class="line"></span><br><span class="line">        spoutConfig.scheme = <span class="keyword">new</span> SchemeAsMultiScheme(<span class="keyword">new</span> MessageScheme());</span><br><span class="line"></span><br><span class="line">        TopologyBuilder builder=<span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        builder.setSpout(KAFKA_SPOUT_ID,<span class="keyword">new</span> KafkaSpout(spoutConfig),<span class="number">3</span>);<span class="comment">//需要注意的是，spout的并行度不能超过topic的partition个数！</span></span><br><span class="line">        builder.setBolt(SPLIT_BOLT_ID,<span class="keyword">new</span> SplitSentenceBolt(),<span class="number">1</span>).shuffleGrouping(KAFKA_SPOUT_ID);</span><br><span class="line">        builder.setBolt(WORD_COUNT_BOLT_ID,<span class="keyword">new</span> WordCountBolt()).fieldsGrouping(SPLIT_BOLT_ID,<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">        builder.setBolt(REPORT_BOLT_ID,<span class="keyword">new</span> ReportBolt()).shuffleGrouping(WORD_COUNT_BOLT_ID);</span><br><span class="line">        <span class="comment">//builder.setBolt(KAFKA_BOLT_ID,new KafkaBolt&lt;String,Long&gt;()).shuffleGrouping(REPORT_BOLT_ID);</span></span><br><span class="line"></span><br><span class="line">        Config config=<span class="keyword">new</span> Config();</span><br><span class="line">        Map&lt;String,String&gt; map=<span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="comment">//map.put("metadata.broker.list", "localhost:9092");</span></span><br><span class="line">        map.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">    map.put(<span class="string">"serializer.class"</span>,<span class="string">"kafka.serializer.StringEncoder"</span>);</span><br><span class="line">        config.put(<span class="string">"kafka.broker.properties"</span>,map);</span><br><span class="line">        config.setNumWorkers(<span class="number">3</span>);</span><br><span class="line">      </span><br><span class="line">        LocalCluster cluster=<span class="keyword">new</span> LocalCluster();</span><br><span class="line"> cluster.submitTopology(TOPOLOGY_NAME,config,builder.createTopology());</span><br><span class="line">        Utils.sleep(<span class="number">10000</span>);</span><br><span class="line">        cluster.killTopology(TOPOLOGY_NAME);</span><br><span class="line">        cluster.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​</p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;storm-kafka-XXX.jar&lt;/code&gt;提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。&lt;/p&gt;
&lt;p&gt;对于核心Storm与Trident两个版本的Spout实现，提供了&lt;code&gt;BrokerHost&lt;/code&gt;接口，跟踪Kafka broker host$\rightarrow$partition的映射，并提供&lt;code&gt;KafkaConfig&lt;/code&gt;接口来控制Kafka相关的参数。下面就这以上两点进行讲解。&lt;/p&gt;
&lt;h3 id=&quot;BrokerHost&quot;&gt;&lt;a href=&quot;#BrokerHost&quot; class=&quot;headerlink&quot; title=&quot;BrokerHost&quot;&gt;&lt;/a&gt;BrokerHost&lt;/h3&gt;&lt;p&gt;为了对Kafka spout进行初始化，我们需要创建一个&lt;code&gt;BrokerHost&lt;/code&gt;的实例，Storm共提供了两种实现方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ZkHosts&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String brokerZkStr,String brokerZkPath)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ZkHosts&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String brokerZkStr)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其中，&lt;code&gt;brokerZkStr&lt;/code&gt;是&lt;code&gt;ip:host&lt;/code&gt;(主机:端口)，&lt;code&gt;brokerZkPath&lt;/code&gt;是存放所有topic和partition信息的根目录，默认值为&lt;code&gt;\broker&lt;/code&gt;。默认地，Zookepper每60秒刷新一次broker$\rightarrow$partition，通过&lt;code&gt;host:refreshFreqSecs&lt;/code&gt;可以改变这个时间。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;StaticHosts。这是另一个选择，不过broker$\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建&lt;code&gt;GlobalPartitionInformation&lt;/code&gt;类的实例，如下：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Broker brokerForPartition0 = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Broker(&lt;span class=&quot;string&quot;&gt;&quot;localhost&quot;&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;//localhost:9092,端口号默认为9092&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Broker brokerForPartition1 = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Broker(&lt;span class=&quot;string&quot;&gt;&quot;localhost&quot;&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;9092&lt;/span&gt;);&lt;span class=&quot;comment&quot;&gt;//localhost:9092,显示地指定端口号&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Broker brokerForPartition2 = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Broker(&lt;span class=&quot;string&quot;&gt;&quot;localhost:9092&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;GlobalPartitionInformation partitionInfo = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; GlobalPartitionInformation();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;partitionInfo.addPartition(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, brokerFroPartition0);&lt;span class=&quot;comment&quot;&gt;// partition0 到 brokerForPartition0的映射&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;partitionInfo.addPartition(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, brokerFroPartition1);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;partitionInfo.addPartition(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, brokerFroPartition2);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;StaticHosts hosts = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; StaticHosts(partitionInfo);&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;KafkaConfig&quot;&gt;&lt;a href=&quot;#KafkaConfig&quot; class=&quot;headerlink&quot; title=&quot;KafkaConfig&quot;&gt;&lt;/a&gt;KafkaConfig&lt;/h3&gt;&lt;p&gt;创建KafkaSpout需要的另一个参数是&lt;code&gt;KafaConfig&lt;/code&gt;，通过调用以下两个函数进行对象创建：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;KafkaConfig&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(BrokerHosts host,String topic)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;KafkaConfig&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(BrokerHosts host,String topic,String clientId)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其中，&lt;code&gt;host&lt;/code&gt;可以为BrokerHost的任何一种实现，&lt;code&gt;topic&lt;/code&gt;是一个topic的名称，&lt;code&gt;clientId&lt;/code&gt;是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。&lt;/p&gt;
&lt;p&gt;目前，KafkaConfig有两种扩展形式，&lt;code&gt;SpoutcConfig&lt;/code&gt;提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。&lt;code&gt;zkRoot&lt;/code&gt;用于存储consumer的offset，&lt;code&gt;id&lt;/code&gt;用于唯一标识当前的spout。&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;SpoutConfig&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(BrokerHosts hosts,String topic,String zkRoot,String id)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//将当前的offset保存到Zookeeper的频率&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;long&lt;/span&gt; stateUpdateIntervals = &lt;span class=&quot;number&quot;&gt;2000&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//用于失效消息的重试策略&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;long&lt;/span&gt; retryInitialDetails = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;double&lt;/span&gt; retryDelayMultiplier = &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//连续重试之间的最大延时&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;long&lt;/span&gt; retryDelayMaxMs = &lt;span class=&quot;number&quot;&gt;60&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;//当retryLimit低于0时，不停地重新发送失效的消息&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; retryLimit = -&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="Kafka" scheme="http://tankcat2.com/tags/Kafka/"/>
    
      <category term="Storm" scheme="http://tankcat2.com/tags/Storm/"/>
    
  </entry>
  
  <entry>
    <title>列存储中常见压缩技术</title>
    <link href="http://tankcat2.com/2017/05/11/compression/"/>
    <id>http://tankcat2.com/2017/05/11/compression/</id>
    <published>2017-05-11T05:40:00.000Z</published>
    <updated>2017-11-27T13:25:45.231Z</updated>
    
    <content type="html"><![CDATA[<p>在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的.</p><p><strong>压缩的优势具体是什么呢？</strong>总结起来呢有两点：</p><ol><li>减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取.</li><li>提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大.</li></ol><p>列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种.</p><h3 id="行程编码-Run-length-Encoding"><a href="#行程编码-Run-length-Encoding" class="headerlink" title="行程编码 Run-length Encoding"></a>行程编码 Run-length Encoding</h3><p>行程编码的核心思想是将有序列中的相同元素转化成一个三元组&lt;属性值，该值第一次出现的位置，出现的次数&gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/run_length.png" alt=""></p><h3 id="位向量-Bit-Vector"><a href="#位向量-Bit-Vector" class="headerlink" title="位向量 Bit-Vector"></a>位向量 Bit-Vector</h3><p>位向量的核心思想是，将一个列中相同的值转为一个二元组&lt;属性值，在列中出现的位置的位图&gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/bit_vector.png" alt=""></p><h3 id="词典编码-Dictionary-Encoding"><a href="#词典编码-Dictionary-Encoding" class="headerlink" title="词典编码 Dictionary Encoding"></a>词典编码 Dictionary Encoding</h3><p>词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文<a href="http://dl.acm.org/citation.cfm?id=2350259&amp;CFID=761347277&amp;CFTOKEN=44019228" target="_blank" rel="noopener">Processing a trillion cells per mouse click</a>上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.</p><a id="more"></a><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/dictionary_encoding.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;压缩的优势具体是什么呢？&lt;/strong&gt;总结起来呢有两点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取.&lt;/li&gt;
&lt;li&gt;提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种.&lt;/p&gt;
&lt;h3 id=&quot;行程编码-Run-length-Encoding&quot;&gt;&lt;a href=&quot;#行程编码-Run-length-Encoding&quot; class=&quot;headerlink&quot; title=&quot;行程编码 Run-length Encoding&quot;&gt;&lt;/a&gt;行程编码 Run-length Encoding&lt;/h3&gt;&lt;p&gt;行程编码的核心思想是将有序列中的相同元素转化成一个三元组&amp;lt;属性值，该值第一次出现的位置，出现的次数&amp;gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/run_length.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;位向量-Bit-Vector&quot;&gt;&lt;a href=&quot;#位向量-Bit-Vector&quot; class=&quot;headerlink&quot; title=&quot;位向量 Bit-Vector&quot;&gt;&lt;/a&gt;位向量 Bit-Vector&lt;/h3&gt;&lt;p&gt;位向量的核心思想是，将一个列中相同的值转为一个二元组&amp;lt;属性值，在列中出现的位置的位图&amp;gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/bit_vector.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;词典编码-Dictionary-Encoding&quot;&gt;&lt;a href=&quot;#词典编码-Dictionary-Encoding&quot; class=&quot;headerlink&quot; title=&quot;词典编码 Dictionary Encoding&quot;&gt;&lt;/a&gt;词典编码 Dictionary Encoding&lt;/h3&gt;&lt;p&gt;词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2350259&amp;amp;CFID=761347277&amp;amp;CFTOKEN=44019228&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Processing a trillion cells per mouse click&lt;/a&gt;上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.&lt;/p&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="压缩" scheme="http://tankcat2.com/tags/%E5%8E%8B%E7%BC%A9/"/>
    
      <category term="行程编码" scheme="http://tankcat2.com/tags/%E8%A1%8C%E7%A8%8B%E7%BC%96%E7%A0%81/"/>
    
      <category term="词典编码" scheme="http://tankcat2.com/tags/%E8%AF%8D%E5%85%B8%E7%BC%96%E7%A0%81/"/>
    
      <category term="位向量" scheme="http://tankcat2.com/tags/%E4%BD%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>视图漫游</title>
    <link href="http://tankcat2.com/2017/05/10/view/"/>
    <id>http://tankcat2.com/2017/05/10/view/</id>
    <published>2017-05-10T06:17:31.000Z</published>
    <updated>2017-11-27T13:13:55.140Z</updated>
    
    <content type="html"><![CDATA[<p>百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”.</p><p>从用户的角度来看，一个视图是从一个<strong>特定的角度</strong>来查看数据库中的数据；</p><p>从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/view.jpg" alt=""></p><p>从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查.</p><p>那视图有优点呢？换句话说，<strong>为什么要使用视图呢？</strong>归纳起来有四点：</p><ol><li>简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系.</li><li>限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等.</li><li>支持列的动态计算.基本表一般都不支持这个功能，比如有一张<code>orders</code>订单表，包含产品数量<code>produce_num</code>与单价<code>produce_price_each</code>两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列<code>total_price(product_num*produce_price_each)</code>就可以直接查询出订单的总价了.</li><li>兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用.</li></ol><a id="more"></a><p>既然说视图也是一种SQL语句，<strong>那么它和查询的区别是什么呢？</strong>简单来说，有三点：</p><ol><li>存储上，视图存储是数据库设计的一部分，而查询则不是；</li><li>更新限制上，因为视图来自于基本表，所以可间接地对基本表进行更新，但是存在诸多限制，比如不能在使用了group by语句的视图中插入值.</li><li>排序结果上，通过查询，可以对一个基本表进行排序，而视图不可以.</li></ol><p>此外，经常对视图的增删改查还是会转换为对基本表的增删改查，会不会降低操作的效率呢？其实未必，尤其是对于多表关联，视图创建后数据库内部会作出相应处理，建立对应的查询路径，反而有利于查询的效率，<strong>这就涉及到物化视图的知识了</strong>.</p><p><a href="https://en.wikipedia.org/wiki/Materialized_view" target="_blank" rel="noopener">维基百科里解释道，</a>物化视图(Materialized View)，也叫做快照，是包含了查询结果的数据库对象，可能是一个远程数据的本地副本，或者是一张表或连接结果的行或者列的子集，等.创建物化视图的过程有时候也被称作是物化，一种缓存查询结果的形式，类似于函数式编程中将函数值进行缓存，有时也称作是“预计算”，用来提高查询的性能与效率.</p><p>在关系型数据库中，如果涉及到对基本视图的查找或修改，都会转化为与之对应的基本表的查找或修改.而物化视图采取不同的方法.查询的结果被缓存在一个实体表中，而不是一个视图里，<strong>实际存储着数据的</strong>，这个实体表会随着基本表的更新而更新. 这种方法利用额外的存储代价和允许部分数据过期的代价，使得查询时的数据访问更加高效.在数据仓库中，物化视图经常使用，尤其是代价较大的频繁基本表查询操作.</p><p>和基本视图还有一点不同的是，在物化视图中，可以在任何一列上建立索引，相反，基本视图通常只能在与基本表相关的列上建立索引.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”.&lt;/p&gt;
&lt;p&gt;从用户的角度来看，一个视图是从一个&lt;strong&gt;特定的角度&lt;/strong&gt;来查看数据库中的数据；&lt;/p&gt;
&lt;p&gt;从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/view.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查.&lt;/p&gt;
&lt;p&gt;那视图有优点呢？换句话说，&lt;strong&gt;为什么要使用视图呢？&lt;/strong&gt;归纳起来有四点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系.&lt;/li&gt;
&lt;li&gt;限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等.&lt;/li&gt;
&lt;li&gt;支持列的动态计算.基本表一般都不支持这个功能，比如有一张&lt;code&gt;orders&lt;/code&gt;订单表，包含产品数量&lt;code&gt;produce_num&lt;/code&gt;与单价&lt;code&gt;produce_price_each&lt;/code&gt;两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列&lt;code&gt;total_price(product_num*produce_price_each)&lt;/code&gt;就可以直接查询出订单的总价了.&lt;/li&gt;
&lt;li&gt;兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="视图" scheme="http://tankcat2.com/tags/%E8%A7%86%E5%9B%BE/"/>
    
      <category term="物化视图" scheme="http://tankcat2.com/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>近日爱读诗词</title>
    <link href="http://tankcat2.com/2017/05/09/poet/"/>
    <id>http://tankcat2.com/2017/05/09/poet/</id>
    <published>2017-05-09T04:44:31.000Z</published>
    <updated>2017-11-27T13:17:33.781Z</updated>
    
    <content type="html"><![CDATA[<center><br><strong>闲居初夏午睡起</strong><br><br>梅子留酸软齿牙，<br><br>芭蕉分绿与窗纱。<br><br>日长睡起无情思，<br><br>闲看儿童捉柳花。<br><br>ps:很喜欢杨万里的写景<br><br><strong>初夏即事</strong><br><br>石梁茅屋有弯碕，<br><br>流水溅溅度两陂。<br><br>晴日暖风生麦气，<br><br>绿阴幽草胜花时。<br><br>ps:读这首诗的那天正好是立夏<br><br><strong>苔</strong><br><br>白日不到处，<br><br>青春恰自来。<br><br>苔花如米小，<br><br>也学牡丹开。<br><br>ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。<br><br></center><p><br><br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;br&gt;&lt;strong&gt;闲居初夏午睡起&lt;/strong&gt;&lt;br&gt;&lt;br&gt;梅子留酸软齿牙，&lt;br&gt;&lt;br&gt;芭蕉分绿与窗纱。&lt;br&gt;&lt;br&gt;日长睡起无情思，&lt;br&gt;&lt;br&gt;闲看儿童捉柳花。&lt;br&gt;&lt;br&gt;ps:很喜欢杨万里的写景&lt;br&gt;&lt;br&gt;&lt;strong&gt;初夏即事&lt;/strong&gt;&lt;br&gt;&lt;br&gt;石梁茅屋有弯碕，&lt;br&gt;&lt;br&gt;流水溅溅度两陂。&lt;br&gt;&lt;br&gt;晴日暖风生麦气，&lt;br&gt;&lt;br&gt;绿阴幽草胜花时。&lt;br&gt;&lt;br&gt;ps:读这首诗的那天正好是立夏&lt;br&gt;&lt;br&gt;&lt;strong&gt;苔&lt;/strong&gt;&lt;br&gt;&lt;br&gt;白日不到处，&lt;br&gt;&lt;br&gt;青春恰自来。&lt;br&gt;&lt;br&gt;苔花如米小，&lt;br&gt;&lt;br&gt;也学牡丹开。&lt;br&gt;&lt;br&gt;ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。&lt;br&gt;&lt;br&gt;&lt;/center&gt;

&lt;p&gt;&lt;br&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="诗词" scheme="http://tankcat2.com/tags/%E8%AF%97%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>聚类索引与非聚类索引</title>
    <link href="http://tankcat2.com/2017/05/06/index/"/>
    <id>http://tankcat2.com/2017/05/06/index/</id>
    <published>2017-05-06T13:08:00.000Z</published>
    <updated>2017-11-27T13:22:30.291Z</updated>
    
    <content type="html"><![CDATA[<p>索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来<strong>定位</strong>的，加快数据库的查询速度.</p><h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><p>索引可分为聚集索引与非聚集索引.下面就两者分别介绍.</p><h3 id="聚集索引"><a href="#聚集索引" class="headerlink" title="聚集索引"></a>聚集索引</h3><p>在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样.</p><p>聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能.</p><p>索引是通过<a href="http://blog.jobbole.com/24006/" target="_blank" rel="noopener">二叉树</a>的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/cluster.JPG" alt=""></p><h3 id="非聚集索引"><a href="#非聚集索引" class="headerlink" title="非聚集索引"></a>非聚集索引</h3><p>非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/noncluster.JPG" alt=""></p><h2 id="两者的区别"><a href="#两者的区别" class="headerlink" title="两者的区别"></a>两者的区别</h2><p>实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别.</p><p>我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，<strong>字典的正文部分本身就是一个目录</strong>，您不需要再去查其他目录来找到您需要找的内容。我们<strong>把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”</strong>。</p><p>如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在<strong>非聚集索引中的排序</strong>，<strong>是字典正文中的字在非聚集索引中的映射</strong>。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们<strong>把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”</strong>。</p><p>通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。</p><h2 id="两种索引的应用场合"><a href="#两种索引的应用场合" class="headerlink" title="两种索引的应用场合"></a>两种索引的应用场合</h2><table><thead><tr><th>动作描述</th><th>聚集索引</th><th>非聚集索引</th></tr></thead><tbody><tr><td>经常对列进行分组排序</td><td>√</td><td>√</td></tr><tr><td>返回某个范围内的数据</td><td>√</td><td>×</td></tr><tr><td>一个或者极少不同的值</td><td>×</td><td>×</td></tr><tr><td>小数目的不同值</td><td>√</td><td>×</td></tr><tr><td>大数目的不同值</td><td>×</td><td>√</td></tr><tr><td>频繁更新的列</td><td>×</td><td>√</td></tr><tr><td>频繁更新索引列</td><td>×</td><td>√</td></tr><tr><td>外键列</td><td>√</td><td>√</td></tr><tr><td>主键列</td><td>√</td><td>√</td></tr><tr><td></td><td></td></tr></tbody></table><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来&lt;strong&gt;定位&lt;/strong&gt;的，加快数据库的查询速度.&lt;/p&gt;
&lt;h2 id=&quot;基本知识&quot;&gt;&lt;a href=&quot;#基本知识&quot; class=&quot;headerlink&quot; title=&quot;基本知识&quot;&gt;&lt;/a&gt;基本知识&lt;/h2&gt;&lt;p&gt;索引可分为聚集索引与非聚集索引.下面就两者分别介绍.&lt;/p&gt;
&lt;h3 id=&quot;聚集索引&quot;&gt;&lt;a href=&quot;#聚集索引&quot; class=&quot;headerlink&quot; title=&quot;聚集索引&quot;&gt;&lt;/a&gt;聚集索引&lt;/h3&gt;&lt;p&gt;在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样.&lt;/p&gt;
&lt;p&gt;聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能.&lt;/p&gt;
&lt;p&gt;索引是通过&lt;a href=&quot;http://blog.jobbole.com/24006/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;二叉树&lt;/a&gt;的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/cluster.JPG&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;非聚集索引&quot;&gt;&lt;a href=&quot;#非聚集索引&quot; class=&quot;headerlink&quot; title=&quot;非聚集索引&quot;&gt;&lt;/a&gt;非聚集索引&lt;/h3&gt;&lt;p&gt;非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/noncluster.JPG&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;两者的区别&quot;&gt;&lt;a href=&quot;#两者的区别&quot; class=&quot;headerlink&quot; title=&quot;两者的区别&quot;&gt;&lt;/a&gt;两者的区别&lt;/h2&gt;&lt;p&gt;实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别.&lt;/p&gt;
&lt;p&gt;我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，&lt;strong&gt;字典的正文部分本身就是一个目录&lt;/strong&gt;，您不需要再去查其他目录来找到您需要找的内容。我们&lt;strong&gt;把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在&lt;strong&gt;非聚集索引中的排序&lt;/strong&gt;，&lt;strong&gt;是字典正文中的字在非聚集索引中的映射&lt;/strong&gt;。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们&lt;strong&gt;把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。&lt;/p&gt;
&lt;h2 id=&quot;两种索引的应用场合&quot;&gt;&lt;a href=&quot;#两种索引的应用场合&quot; class=&quot;headerlink&quot; title=&quot;两种索引的应用场合&quot;&gt;&lt;/a&gt;两种索引的应用场合&lt;/h2&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;动作描述&lt;/th&gt;
&lt;th&gt;聚集索引&lt;/th&gt;
&lt;th&gt;非聚集索引&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;经常对列进行分组排序&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;返回某个范围内的数据&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;一个或者极少不同的值&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;小数目的不同值&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;大数目的不同值&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;频繁更新的列&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;频繁更新索引列&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;外键列&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;主键列&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;td&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="索引" scheme="http://tankcat2.com/tags/%E7%B4%A2%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>&lt;刀锋&gt;观后感</title>
    <link href="http://tankcat2.com/2017/04/20/daofeng/"/>
    <id>http://tankcat2.com/2017/04/20/daofeng/</id>
    <published>2017-04-20T12:11:31.000Z</published>
    <updated>2017-11-27T13:25:51.525Z</updated>
    
    <content type="html"><![CDATA[<p>先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.”</p><p>“因为人不论男男女女，都不仅仅是他们自身；他们也是自己出生的乡土，学步的农场或城市公寓，儿时玩的游戏，私下听来的山海经，吃的饭食，上的学校，关心的运动，吟哦的诗章，和信仰的上帝。这一切东西把他们造成现在这样，而这些东西都不是道听途说就可以了解的，你非得和那些人生活过。要了解这些，你就得是这些。 ”</p><p>无论中英文，都是一流的文字，解释了各种文化之间的冲突，以及冲突误解的永恒性。</p><p>很少有外国作品上让我读得这么舒服，这完全归功于周旭良老师的翻译功底，整本书翻译地非常好，读起来如沐春风。书写得很平淡，但每个角色都很有意思，拉里最为迷人。很奇怪，刚开始看的时候我脑子里对拉里的想象，居然是血战钢锯岭里的戴斯蒙特，这里也仅是人物形象。拉里一直追寻的答案，等同于追求终极真理，而这个问题最终都会归结到理性与精神的绝对满足。真的很难以想象，拉里这样的人，现实中又有多少，他们的生活又是怎样的？这种绝对的内心的泰然平和，我生生世世估计也做不到吧。<br><a id="more"></a></p>]]></content>
    
    <summary type="html">
    
      追随内心需求，探索人生价值与真谛。
    
    </summary>
    
      <category term="Jottings" scheme="http://tankcat2.com/categories/Jottings/"/>
    
    
      <category term="毛姆" scheme="http://tankcat2.com/tags/%E6%AF%9B%E5%A7%86/"/>
    
      <category term="刀锋" scheme="http://tankcat2.com/tags/%E5%88%80%E9%94%8B/"/>
    
  </entry>
  
  <entry>
    <title>&lt;简明美国史&gt;观后感</title>
    <link href="http://tankcat2.com/2017/04/16/historyUSA/"/>
    <id>http://tankcat2.com/2017/04/16/historyUSA/</id>
    <published>2017-04-16T12:11:31.000Z</published>
    <updated>2017-11-27T13:23:47.390Z</updated>
    
    <content type="html"><![CDATA[<p>相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。</p><p>有人说陈勤老师的这本美国史写得实在是太简太浅显，但是对我这种历史水平只停留在高中课本上的”史盲“来说，基本上是够了，从脉络上了解美国自1620年《五月花号公约》至2016年奥巴马最后的执政之间所发生的重大历史事件，这其中涵盖了美国从英属殖民地开始，到1776年《独立宣言》宣告独立，到1860年林肯领导南北战争，再到一战、二战、冷战，以及至今美国发生的种种。读完全书的第一感受是，陈勤老师应当是亲美派的，书中给我描述的美国是一个有趣、鲜活、有人味的美国，虽然对历史变革中发生的流血事件只是轻描淡写地带过，但还是能些许体会到”世界何尝不简单，历史从来不温柔“这一面。读完一遍脑海中对美国的历史线还是稍有混乱，有时间自己再整理整理。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      青山遮不住，毕竟东流去.
    
    </summary>
    
      <category term="Reviews" scheme="http://tankcat2.com/categories/Reviews/"/>
    
    
      <category term="历史" scheme="http://tankcat2.com/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>再见我的暴力女王</title>
    <link href="http://tankcat2.com/2017/02/27/evil/"/>
    <id>http://tankcat2.com/2017/02/27/evil/</id>
    <published>2017-02-27T12:11:31.000Z</published>
    <updated>2017-11-27T13:24:27.371Z</updated>
    
    <content type="html"><![CDATA[<p>初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字；</p><p>后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了；</p><p>到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她；</p><p>没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。</p><p>今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。</p><p>最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字；&lt;/p&gt;
&lt;p&gt;后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了；&lt;/p&gt;
&lt;p&gt;到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她；&lt;/p&gt;
&lt;p&gt;没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。&lt;/p&gt;
&lt;p&gt;今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。&lt;/p&gt;
&lt;p&gt;最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。&lt;/p&gt;
    
    </summary>
    
      <category term="Reviews" scheme="http://tankcat2.com/categories/Reviews/"/>
    
    
      <category term="生化危机" scheme="http://tankcat2.com/tags/%E7%94%9F%E5%8C%96%E5%8D%B1%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>Kafka快速入门</title>
    <link href="http://tankcat2.com/2017/02/27/kafka_quickstart/"/>
    <id>http://tankcat2.com/2017/02/27/kafka_quickstart/</id>
    <published>2017-02-27T12:11:31.000Z</published>
    <updated>2017-11-27T13:21:45.075Z</updated>
    
    <content type="html"><![CDATA[<p>翻译自kafka documentation的quick start 部分。</p><ol><li><p>下载Zookeeper</p><p>我使用的是<a href="http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz" target="_blank" rel="noopener">zookeeper-3.4.6</a>版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf zookeeper-3.4.6.tgz</span><br><span class="line">cd zookeeper-3.4.6/conf</span><br></pre></td></tr></table></figure><p>将<code>zoo_example.cfg</code>改名为<code>zoo.cfg</code>，并在<code>/etc/profile</code>中设置环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export ZK_HOME=/home/admin/zookeeper-3.4.6</span><br><span class="line">export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf</span><br></pre></td></tr></table></figure></li><li><p>下载Kafka</p><p>我使用的是<a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.10-0.10.2.1.tgz" target="_blank" rel="noopener">kafka_2.10-0.10.2.1</a>版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvzf kafka_2.10-0.10.2.1</span><br><span class="line">cd kafka_2.10-0.10.2.1/config</span><br></pre></td></tr></table></figure><p>接下来进行参数配置：<code>server.properties</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim server.properties</span><br><span class="line">...</span><br><span class="line"><span class="meta">#</span> 修改broker.id,全局唯一</span><br><span class="line"><span class="meta">#</span> 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔</span><br><span class="line">zookeeper.connect=192.168.115:2181</span><br><span class="line"><span class="meta">#</span> 设置话题的删除,默认值为false</span><br><span class="line">delete.topic.enable=true</span><br><span class="line"><span class="meta">#</span> 设置数据日志路径</span><br><span class="line">log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs</span><br></pre></td></tr></table></figure></li><li><p>启动</p><p>Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p>接着启动Kafka:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure></li><li><p>创建topic</p><p>使用下面的命令创建名为<code>single_node</code>的topic，副本数为1，分区数为1，命令执行结束后，<code>kafka-logs</code>路径下就会生成一个<code>single_node-0</code>的文件夹。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node</span><br></pre></td></tr></table></figure></li><li><p>发布与消耗数据</p><p>执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_node</span><br><span class="line">wxt</span><br><span class="line">zf</span><br><span class="line">i love u</span><br></pre></td></tr></table></figure><p>执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginning</span><br><span class="line">wxt</span><br><span class="line">zf</span><br><span class="line">i love u</span><br></pre></td></tr></table></figure></li></ol><p>以上均是单机版的Kafka配置与使用。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;翻译自kafka documentation的quick start 部分。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载Zookeeper&lt;/p&gt;
&lt;p&gt;我使用的是&lt;a href=&quot;http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;zookeeper-3.4.6&lt;/a&gt;版本&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -xvzf zookeeper-3.4.6.tgz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cd zookeeper-3.4.6/conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;将&lt;code&gt;zoo_example.cfg&lt;/code&gt;改名为&lt;code&gt;zoo.cfg&lt;/code&gt;，并在&lt;code&gt;/etc/profile&lt;/code&gt;中设置环境变量：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/profile&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export ZK_HOME=/home/admin/zookeeper-3.4.6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;下载Kafka&lt;/p&gt;
&lt;p&gt;我使用的是&lt;a href=&quot;https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.10-0.10.2.1.tgz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kafka_2.10-0.10.2.1&lt;/a&gt;版本&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;tar -xvzf kafka_2.10-0.10.2.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cd kafka_2.10-0.10.2.1/config&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;接下来进行参数配置：&lt;code&gt;server.properties&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim server.properties&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;...&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 修改broker.id,全局唯一&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;zookeeper.connect=192.168.115:2181&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 设置话题的删除,默认值为false&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;delete.topic.enable=true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 设置数据日志路径&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;启动&lt;/p&gt;
&lt;p&gt;Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;zkServer.sh start&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;接着启动Kafka:&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/kafka-server-start.sh config/server.properties&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;创建topic&lt;/p&gt;
&lt;p&gt;使用下面的命令创建名为&lt;code&gt;single_node&lt;/code&gt;的topic，副本数为1，分区数为1，命令执行结束后，&lt;code&gt;kafka-logs&lt;/code&gt;路径下就会生成一个&lt;code&gt;single_node-0&lt;/code&gt;的文件夹。&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;发布与消耗数据&lt;/p&gt;
&lt;p&gt;执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_node&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wxt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;zf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;i love u&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据：&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginning&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;wxt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;zf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;i love u&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上均是单机版的Kafka配置与使用。&lt;/p&gt;
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="Kafka" scheme="http://tankcat2.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems</title>
    <link href="http://tankcat2.com/2017/01/17/Robust%20and%20Skew-resistant%20Parallel%20Joins%20in%20Shared-Nothing%20Systems/"/>
    <id>http://tankcat2.com/2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/</id>
    <published>2017-01-17T05:11:31.000Z</published>
    <updated>2017-11-27T13:13:22.777Z</updated>
    
    <content type="html"><![CDATA[<h2 id="并行连接处理的两种基本框架"><a href="#并行连接处理的两种基本框架" class="headerlink" title="并行连接处理的两种基本框架"></a>并行连接处理的两种基本框架</h2><ul><li><p><strong>hash-based</strong> 基于哈希，如下图所示，分为四个步骤：</p><ol><li>partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集；</li><li>distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R<em>{ik}$ 和 $S</em>{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$R<em>k=\bigcup</em>{i=1}^{n}R_{ik}$,$S<em>k=\bigcup</em>{i=1}^{n}S_{ik}$;</li><li>build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表；</li><li>prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果.</li></ol><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/hash-based.png" alt="hash-based"></p></li><li><p><strong>duplication-based</strong> 基于副本，如下图所示，分为三个步骤：</p><ol><li>duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$R<em>k=\bigcup</em>{i=1}^{n}R_i=R$即为全集R；</li><li>build构建，构建哈希表，与hash-based相似；</li><li>prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似.</li></ol></li></ul><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/duplicated-based.png" alt="duplicated-based"></p><h2 id="PRPD连接算法"><a href="#PRPD连接算法" class="headerlink" title="PRPD连接算法"></a>PRPD连接算法</h2><ol><li><p>PRPD定义：partial redistribution &amp; partition duplication，即将hash-based和duplication-based相结合.</p></li><li><p>处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S<em>{loc}$是倾斜数据子集，$S</em>{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R<em>{dup}$是与$S</em>{loc}$连接键值相同的数据子集，$R<em>{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S</em>{redis}$进行连接.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/PRPD.jpg" alt="PRPD"></p></li><li><p>存在的问题：</p><ul><li>global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布；</li><li>broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加.</li></ul></li></ol><h2 id="本文提出算法"><a href="#本文提出算法" class="headerlink" title="本文提出算法"></a>本文提出算法</h2><p>PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进.</p><h3 id="Semijoin-based-连接"><a href="#Semijoin-based-连接" class="headerlink" title="Semijoin-based 连接"></a>Semijoin-based 连接</h3><ol><li><p>semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行<strong>不完全</strong>连接查询，即查找到匹配的数据行就返回，不再继续查找.</p></li><li><p>semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤：</p><ol><li>类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线);</li><li>对各个节点中的数据集$S_i$在属性b上做投影操作得到$\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中；</li><li>每个计算节点k收到数据集S的key 子集$\pi<em>b(S</em>{ik})$，和数据集R的子集$R<em>k=\bigcup</em>{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)；</li><li>各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果.</li></ol><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/semijoin.png" alt="semijoin"></p></li><li><p>特点：</p><ul><li>由于<strong>投影</strong>操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以<strong>解决数据倾斜</strong>；</li><li>第2和第3步骤，只传输key和能连接上的元组，因此<strong>减轻了网络传输代价</strong>.</li><li>对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量.</li></ul></li></ol><h3 id="Query-based-连接"><a href="#Query-based-连接" class="headerlink" title="Query-based 连接"></a>Query-based 连接</h3><ol><li><p>根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下：</p><ol><li>若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value；</li><li>返回的value和本地的S数据集做最后的实际连接操作，输出连接结果.</li></ol></li><li><p>特点：</p><ul><li>对于高选择性的连接处理，优势大，减轻网络通信负载；</li><li>对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证<key,value>的序列以便最后的连接处理，因此可能降低处理速度.</key,value></li></ul></li><li><p>折中综合：通过一个<strong>计数器</strong>来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based.</p><h3 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h3><p>本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷.</p><h3 id="PRPQ连接算法"><a href="#PRPQ连接算法" class="headerlink" title="PRPQ连接算法"></a>PRPQ连接算法</h3><ol><li><p>PRPQ定义：partial redistribution &amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤：</p><ol><li><p>R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)；</p></li><li><p>Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)；</p></li><li>Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$R<em>k=\bigcup</em>{i=1}^{n}R<em>{ik}$建立哈希表，(1). 对接收到的集合$\bigcup</em>{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\pi<em>b(h</em>{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i；</li><li>Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\bigcup$第4部分的连接结果.</li></ol><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/PRPQ.png" alt="PRPQ"></p></li><li><p>特点：</p><ul><li>与query-based算法相比<ol><li>当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性.</li><li>继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. </li></ol></li><li>与PRPD算法相比<ol><li>最主要的区别在于，使用query而不是duplication操作.</li><li>PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价.</li><li>对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载.</li></ol></li></ul></li></ol></li></ol><a id="more"></a><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>每个节点上skew元组的提取是基于局部倾斜量化，因此引入一个阈值参数，即当一个key出现的次数超过该阈值，则视这个key为skewed. 下面先整理如何处理阈值参数，再整理PRPQ算法的具体实现.</p><h3 id="局部数据倾斜"><a href="#局部数据倾斜" class="headerlink" title="局部数据倾斜"></a>局部数据倾斜</h3><p>有很多方法可实现局部数据倾斜的快速监测，比如采样，扫描等.但是这些与本文的思路无关，所以本文仅仅在每个节点中对key的出现次数进行<strong>计数</strong>，按照降序排列，并保存到文件中. 在每一次的参数测试中，每个节点预先读取出现次数超过t的key，写入一个ArrayList中，并视它们为skew key.</p><h3 id="PRPQ具体实现"><a href="#PRPQ具体实现" class="headerlink" title="PRPQ具体实现"></a>PRPQ具体实现</h3><p>具体算法和前面的四个步骤一一对应，如下：</p><ol><li><p>在每个原节点中，将所有的元组读取到一个ArrayList中，处理数据集R的元组. 首先初始化一个R_c，用于收集分组的元组，R_c的初始化大小为计算节点的数量.接着，各个线程读取ArrayList中的R集元组，根据连接key的哈希值对元组进行分组.最后，将分好组的元组分发到对应的计算节点中(算法中的here表示当前计算节点的id).</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/RDistribution.png" alt="RDistribution"></p></li><li><p>根据给定的阈值参数t，对数据集S进行划分，倾斜的key被读入一个hashset，并且所有对应的元组被存储到一个hashmap中，剩余的非倾斜元组存储到一个$S^{’}_c$中.接着对hashmap进行投影操作，将所有的unique keys保存到key_c中.最后将key_c和$S^{’}_c$按照key的哈希值分发到对应的计算节点上.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/PushQueryKey.png" alt="PushQueryKey"></p></li><li><p>在计算节点中，对接收到的R集元组建立一个哈希表T’，对数据集$S’$元组进行遍历，并查找哈希表，若有匹配的key，则输出连接结果.同时遍历key集key_c，并查找哈希表，若不存在匹配的key，则返回值为null的value到对应的原节点，否则返回实际key对应的value.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/ReturnQueryValue.png" alt="ReturnQueryValue"></p></li><li><p>倾斜元组的连接结果可以通过遍历查询返回的value集合，若value为null，则不存在能连接上的S集元组，否则输出最终连接结果.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/ResultLookup.png" alt="ResultLookup"></p></li></ol><h2 id="实验对比"><a href="#实验对比" class="headerlink" title="实验对比"></a>实验对比</h2><p>数据集的选取：用作基准的数据集模仿决策支援系统下的连接操作.数据集R的cardinality为64M，数据集S的cardinality为1GB.由于数据仓储中数据一般以面向列的形式存储，所以实验中将数据格式设置为<key,value>的键值对，其中key和value均是8字节整型.</key,value></p><p>工作负载的选取：设置数据集R和S之间存在外键的关系，保持R的主键的unique，而在S中为对应的外键增加skew.除此之外，若S是统一分布的，它们中的每一个以相同的概率匹配关系R中的元组.对于倾斜的元组，它们的unique key在节点间均匀分布，并且每一个均能与R匹配上.下表给出了数据集S的分布情况.</p><table><thead><tr><th>S</th><th>key distribution</th><th>Partition</th><th>Size</th></tr></thead><tbody><tr><td>Zipf</td><td>skew=0,1,1.4</td><td>均匀evenly</td><td>512M</td></tr><tr><td>Linear</td><td>f(r)=46341-r,23170</td><td>排序范围sort-range</td><td>1GB,2GB</td></tr></tbody></table><p>Zipf分布中，skew=0表示统一分布，skew=1表示排名前十的key占据总量14%，skew=1.4表示排名前十的key占据总量68%.线性分布中，使用f(r)来描述key的分布情况，其中f(r)=46341-r表示频率最高的key出现46341次，频率第二的key出现46340次.使用该函数生成的数据集可以看作low-skewed的数据集.f(r)=23170表示所有的key都是均匀分布的，但是重复次数较高.f(r)对应的两个数据集均为1GB的大小，有46341个unique key.</p><p>R和S在计算节点中的分布情况：R均匀分布在所有的节点上，而S使用均匀和排序范围分布.均与分布保证每个计算节点上skewed元组的数量相同；排序范围分布是先将所有的元组按照键的频率排序，然后等分成大小一样的块，再将块按照次序分配到每个计算节点上.因此每个节点上skewed元组的数量差距可能会比较大.</p><p>实验共从运行时间、网络通信、负载均衡、可扩展性四个方面来进行比较.这里只就运行时间稍作整理.</p><h3 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h3><p>记录Hash-based算法、PRPD、PRPQ和query-based算法的运行时间，如下图所示.当S是均匀分布(第一组数据skew=0)，Hash、PRPD和PRPQ算法的性能相近，远远优于Query算法；当S是low skewed时，PRPD和PRPQ均比另外两种算法快；当S是high skewed时，Hash算法性能最差，而其余三种性能相近，则可得出结论，其余PRPD、PRPQ和Query可以较好地处理数据倾斜.随着skew程度的增加，Hash算法的执行时间增长剧烈，而Query算法呈现下降趋势.而PRPD和PRPQ算法呈现平稳的下降趋势.</p><p><img src="http://7xwggp.com1.z0.glb.clouddn.com/Runtime.png" alt="Runtime"></p><p>上图展示是选择最佳频率阈值t的性能，原文中关于不同阈值的实验这里不再整理，基本情况是无论t值如何变化以及分区计划如何，PRPQ的运行时间是低于PRPD的.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;并行连接处理的两种基本框架&quot;&gt;&lt;a href=&quot;#并行连接处理的两种基本框架&quot; class=&quot;headerlink&quot; title=&quot;并行连接处理的两种基本框架&quot;&gt;&lt;/a&gt;并行连接处理的两种基本框架&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;hash-based&lt;/strong&gt; 基于哈希，如下图所示，分为四个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集；&lt;/li&gt;
&lt;li&gt;distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R&lt;em&gt;{ik}$ 和 $S&lt;/em&gt;{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$R&lt;em&gt;k=\bigcup&lt;/em&gt;{i=1}^{n}R_{ik}$,$S&lt;em&gt;k=\bigcup&lt;/em&gt;{i=1}^{n}S_{ik}$;&lt;/li&gt;
&lt;li&gt;build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表；&lt;/li&gt;
&lt;li&gt;prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/hash-based.png&quot; alt=&quot;hash-based&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;duplication-based&lt;/strong&gt; 基于副本，如下图所示，分为三个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$R&lt;em&gt;k=\bigcup&lt;/em&gt;{i=1}^{n}R_i=R$即为全集R；&lt;/li&gt;
&lt;li&gt;build构建，构建哈希表，与hash-based相似；&lt;/li&gt;
&lt;li&gt;prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/duplicated-based.png&quot; alt=&quot;duplicated-based&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;PRPD连接算法&quot;&gt;&lt;a href=&quot;#PRPD连接算法&quot; class=&quot;headerlink&quot; title=&quot;PRPD连接算法&quot;&gt;&lt;/a&gt;PRPD连接算法&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;PRPD定义：partial redistribution &amp;amp; partition duplication，即将hash-based和duplication-based相结合.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S&lt;em&gt;{loc}$是倾斜数据子集，$S&lt;/em&gt;{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R&lt;em&gt;{dup}$是与$S&lt;/em&gt;{loc}$连接键值相同的数据子集，$R&lt;em&gt;{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S&lt;/em&gt;{redis}$进行连接.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/PRPD.jpg&quot; alt=&quot;PRPD&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;存在的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布；&lt;/li&gt;
&lt;li&gt;broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;本文提出算法&quot;&gt;&lt;a href=&quot;#本文提出算法&quot; class=&quot;headerlink&quot; title=&quot;本文提出算法&quot;&gt;&lt;/a&gt;本文提出算法&lt;/h2&gt;&lt;p&gt;PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进.&lt;/p&gt;
&lt;h3 id=&quot;Semijoin-based-连接&quot;&gt;&lt;a href=&quot;#Semijoin-based-连接&quot; class=&quot;headerlink&quot; title=&quot;Semijoin-based 连接&quot;&gt;&lt;/a&gt;Semijoin-based 连接&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行&lt;strong&gt;不完全&lt;/strong&gt;连接查询，即查找到匹配的数据行就返回，不再继续查找.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线);&lt;/li&gt;
&lt;li&gt;对各个节点中的数据集$S_i$在属性b上做投影操作得到$\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中；&lt;/li&gt;
&lt;li&gt;每个计算节点k收到数据集S的key 子集$\pi&lt;em&gt;b(S&lt;/em&gt;{ik})$，和数据集R的子集$R&lt;em&gt;k=\bigcup&lt;/em&gt;{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)；&lt;/li&gt;
&lt;li&gt;各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/semijoin.png&quot; alt=&quot;semijoin&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于&lt;strong&gt;投影&lt;/strong&gt;操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以&lt;strong&gt;解决数据倾斜&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;第2和第3步骤，只传输key和能连接上的元组，因此&lt;strong&gt;减轻了网络传输代价&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Query-based-连接&quot;&gt;&lt;a href=&quot;#Query-based-连接&quot; class=&quot;headerlink&quot; title=&quot;Query-based 连接&quot;&gt;&lt;/a&gt;Query-based 连接&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value；&lt;/li&gt;
&lt;li&gt;返回的value和本地的S数据集做最后的实际连接操作，输出连接结果.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于高选择性的连接处理，优势大，减轻网络通信负载；&lt;/li&gt;
&lt;li&gt;对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证&lt;key,value&gt;的序列以便最后的连接处理，因此可能降低处理速度.&lt;/key,value&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;折中综合：通过一个&lt;strong&gt;计数器&lt;/strong&gt;来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based.&lt;/p&gt;
&lt;h3 id=&quot;性能问题&quot;&gt;&lt;a href=&quot;#性能问题&quot; class=&quot;headerlink&quot; title=&quot;性能问题&quot;&gt;&lt;/a&gt;性能问题&lt;/h3&gt;&lt;p&gt;本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷.&lt;/p&gt;
&lt;h3 id=&quot;PRPQ连接算法&quot;&gt;&lt;a href=&quot;#PRPQ连接算法&quot; class=&quot;headerlink&quot; title=&quot;PRPQ连接算法&quot;&gt;&lt;/a&gt;PRPQ连接算法&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;PRPQ定义：partial redistribution &amp;amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$R&lt;em&gt;k=\bigcup&lt;/em&gt;{i=1}^{n}R&lt;em&gt;{ik}$建立哈希表，(1). 对接收到的集合$\bigcup&lt;/em&gt;{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\pi&lt;em&gt;b(h&lt;/em&gt;{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i；&lt;/li&gt;
&lt;li&gt;Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\bigcup$第4部分的连接结果.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;http://7xwggp.com1.z0.glb.clouddn.com/PRPQ.png&quot; alt=&quot;PRPQ&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;与query-based算法相比&lt;ol&gt;
&lt;li&gt;当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性.&lt;/li&gt;
&lt;li&gt;继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. &lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;与PRPD算法相比&lt;ol&gt;
&lt;li&gt;最主要的区别在于，使用query而不是duplication操作.&lt;/li&gt;
&lt;li&gt;PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价.&lt;/li&gt;
&lt;li&gt;对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Papers" scheme="http://tankcat2.com/categories/Papers/"/>
    
    
      <category term="Join" scheme="http://tankcat2.com/tags/Join/"/>
    
  </entry>
  
  <entry>
    <title>使用Storm遇到的问题以及解决方案</title>
    <link href="http://tankcat2.com/2016/12/30/stormproblems/"/>
    <id>http://tankcat2.com/2016/12/30/stormproblems/</id>
    <published>2016-12-30T05:45:31.000Z</published>
    <updated>2017-11-27T13:14:15.109Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016</span>-<span class="number">12</span>-<span class="number">30</span> <span class="number">12</span>:<span class="number">41</span>:<span class="number">17.269</span> b.s.event [ERROR] Error when processing event</span><br><span class="line">java.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: File <span class="string">'/home/admin/stormdata/data/supervisor/localstate/1480504905565'</span> does not exist</span><br><span class="line">at backtype.storm.utils.LocalState.partialSnapshot(LocalState.java:<span class="number">118</span>) ~[storm-core-<span class="number">0.10</span>.0.jar:<span class="number">0.10</span>.0]</span><br><span class="line">at backtype.storm.utils.LocalState.get(LocalState.java:<span class="number">126</span>) ~[storm-core-<span class="number">0.10</span>.0.jar:<span class="number">0.10</span>.0]</span><br><span class="line">at backtype.storm.local_state$ls_local_assignments.invoke(local_state.clj:<span class="number">83</span>) ~[storm-core-<span class="number">0.10</span>.0.jar:<span class="number">0.10</span>.0]</span><br><span class="line">at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:<span class="number">321</span>) ~[storm-core-<span class="number">0.10</span>.0.jar:<span class="number">0.10</span>.0]</span><br><span class="line">at clojure.lang.AFn.applyToHelper(AFn.java:<span class="number">154</span>) ~[clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at clojure.lang.AFn.applyTo(AFn.java:<span class="number">144</span>) ~[clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at clojure.core$apply.invoke(core.clj:<span class="number">626</span>) ~[clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at clojure.core$partial$fn__4228.doInvoke(core.clj:<span class="number">2468</span>) ~[clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at clojure.lang.RestFn.invoke(RestFn.java:<span class="number">397</span>) ~[clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at backtype.storm.event$event_manager$fn__7258.invoke(event.clj:<span class="number">40</span>) [storm-core-<span class="number">0.10</span>.0.jar:<span class="number">0.10</span>.0]</span><br><span class="line">at clojure.lang.AFn.run(AFn.java:<span class="number">22</span>) [clojure-<span class="number">1.6</span>.0.jar:?]</span><br><span class="line">at java.lang.Thread.run(Thread.java:<span class="number">744</span>) [?:<span class="number">1.7</span>.0_45]</span><br></pre></td></tr></table></figure><p>找不到’/home/admin/stormdata/data/supervisor/localstate/1480504905565’这个文件夹，网上找了下原因，给出的答案是<strong>stop the server without previously stop the supervisor</strong>，就是说可能是由于不正常关机造成状态不一致，具体原因不知，解决方案是<strong>删除stormdata/data/supervisor整个目录即可</strong>.</p><a id="more"></a></li><li><p>在集群环境下日志清理，自己写了一个脚本clear-log.sh，主要是删除apache-storm-XXX下的logs文件里的日志文件，如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">STORM_HOME=/home/admin/apache-storm-0.10.0　</span><br><span class="line">HOSTS_FILE=/home/admin/hosts.txt</span><br><span class="line">cat $HOSTS_FILE | while read line</span><br><span class="line">do</span><br><span class="line">ssh $line "rm -rf $STORM_HOME/logs/*" &lt; /dev/null</span><br><span class="line">done</span><br><span class="line">echo "remove log files...done"</span><br></pre></td></tr></table></figure></li></ol><p>其中STORM_HOME是storm的安装路径，hosts.txt是集群中各个节点的地址，我自己的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">admin@10.11.1.53</span><br><span class="line">admin@10.11.1.40</span><br><span class="line">admin@10.11.1.41</span><br><span class="line">admin@10.11.1.42</span><br><span class="line">admin@10.11.1.45</span><br><span class="line">admin@10.11.1.46</span><br><span class="line">admin@10.11.1.51</span><br><span class="line">admin@10.11.1.53</span><br><span class="line">admin@10.11.1.54</span><br><span class="line">admin@10.11.1.55</span><br><span class="line">admin@10.11.1.56</span><br><span class="line">admin@10.11.1.58</span><br><span class="line">admin@10.11.1.60</span><br><span class="line">admin@10.11.1.64</span><br></pre></td></tr></table></figure><p>编辑完之后执行<code>chmod +x clear-log.sh</code>命令使得该文件获得可执行权限，再执行<code>./clear-log.sh</code>运行该脚本即可.</p>]]></content>
    
    <summary type="html">
    
      本文是我在使用Storm的过程中遇到的各种问题以及对应的解决方案，有些问题可能无法给出理由，不定期更新.
    
    </summary>
    
      <category term="Techniques" scheme="http://tankcat2.com/categories/Techniques/"/>
    
    
      <category term="Storm" scheme="http://tankcat2.com/tags/Storm/"/>
    
  </entry>
  
</feed>
