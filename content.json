{"meta":{"title":"Tankcat","subtitle":"I'll try anything once...","description":"Big Data, Apache Storm, Stream Processing, Join Processing","author":"Tankcat","url":"http://tankcat2.com"},"pages":[{"title":"","date":"2017-11-26T05:33:11.636Z","updated":"2016-07-10T17:29:36.000Z","comments":true,"path":"404.html","permalink":"http://tankcat2.com/404.html","excerpt":"","text":"404"},{"title":"","date":"2017-11-26T05:33:11.640Z","updated":"2017-03-16T11:25:50.000Z","comments":true,"path":"tool.js","permalink":"http://tankcat2.com/tool.js","excerpt":"","text":"\"use strict\"; var fs = require(\"fs\"); var path = \"photos/\"; fs.readdir(path, function (err, files) { if (err) { return; } var arr = []; (function iterator(index) { if (index == files.length) { fs.writeFile(\"photos/data.json\", JSON.stringify(arr, null, \"\\t\")); console.log('get img success!'); return; } fs.stat(path + files[index], function (err, stats) { if (err) { return; } if (stats.isFile()) { arr.push(files[index]); } iterator(index + 1); }) }(0)); });"},{"title":"About","date":"2017-11-26T05:33:11.645Z","updated":"2016-10-12T15:28:16.000Z","comments":true,"path":"about/index.html","permalink":"http://tankcat2.com/about/index.html","excerpt":"","text":""},{"title":"Archive","date":"2016-07-10T08:14:45.000Z","updated":"2016-07-17T06:27:18.000Z","comments":true,"path":"archive/index.html","permalink":"http://tankcat2.com/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-11-26T05:33:11.655Z","updated":"2016-07-17T06:27:32.000Z","comments":true,"path":"categories/index.html","permalink":"http://tankcat2.com/categories/index.html","excerpt":"","text":""},{"title":"Photos","date":"2016-09-16T07:07:04.000Z","updated":"2017-05-07T10:29:36.000Z","comments":true,"path":"favorite/index.html","permalink":"http://tankcat2.com/favorite/index.html","excerpt":"","text":"劝君莫惜金缕衣，劝君惜取少年时。花开堪折直须折，莫待无花空折枝。——杜秋娘《金缕衣》 ​​​​ .hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}逛街的两个人 南京市人民政府 行走在平江路 龙之梦的一家饰品店 回家 一天天长大的小叶子 胖嘟嘟的阿拉斯加 姐妹花 苏州图书馆 同德兴的拉面 星巴克的桃桃红茶 夜晚十一点的红房子 阿里家的面 上海城市规划馆 和府捞面 南京大屠杀纪念馆 沙面撸猫 广州的一家青旅 很高兴遇见你 太古汇的索尼体验店 Godiva的双色甜筒 丽娃河畔的樱花 顾村公园赏樱 不知名的紫色小花 五舍楼后的白月季 河畔芦苇 小黄花 河西食堂前的桃李 丽娃桥 依旧耀眼的落日 滴水湖的路标 家里的后院 十八舍前一抹红 云雾缭绕 南师西区操场 修剪的树枝 蝙蝠侠大战超人 缺月挂疏桐 敬文图书馆 中北的蓝天(一) 中北的蓝天(二) 广玉兰 南京97路公交 大行宫地铁站 南师晒太阳的老黄 顺和祥的汤包 爷爷的梅干菜扣肉 左庭右院的牛肉饼 南师西区食堂的炸酱汤面 $('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });"},{"title":"","date":"2017-11-26T05:33:11.672Z","updated":"2017-03-16T11:25:54.000Z","comments":true,"path":"photos/data.json","permalink":"http://tankcat2.com/photos/data.json","excerpt":"","text":"[\"14098405995146380.jpg\",\"227563740595514503.jpg\",\"340809002755226955.jpg\",\"775149960737431740.jpg\",\"IMG_20151027_202853.jpg\",\"IMG_20151105_184339.jpg\",\"IMG_20151105_202323.jpg\",\"spring.jpg\"]"},{"title":"Photos","date":"2017-03-16T08:14:45.000Z","updated":"2017-03-16T13:22:48.000Z","comments":true,"path":"photos/index.html","permalink":"http://tankcat2.com/photos/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-07-10T12:26:33.000Z","updated":"2016-07-17T06:27:58.000Z","comments":true,"path":"tags/index.html","permalink":"http://tankcat2.com/tags/index.html","excerpt":"","text":""},{"title":"Search","date":"2017-11-26T05:33:11.689Z","updated":"2016-07-17T06:27:46.000Z","comments":true,"path":"search/index.html","permalink":"http://tankcat2.com/search/index.html","excerpt":"","text":""}],"posts":[{"title":"鼓楼半日记","slug":"gulou","date":"2017-08-15T12:11:31.000Z","updated":"2017-08-21T14:25:40.000Z","comments":true,"path":"2017/08/15/gulou/","link":"","permalink":"http://tankcat2.com/2017/08/15/gulou/","excerpt":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","text":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[]},{"title":"Tragic Ending or Peace Ending ?","slug":"my chester","date":"2017-07-20T16:00:00.000Z","updated":"2017-07-21T01:44:52.000Z","comments":true,"path":"2017/07/21/my chester/","link":"","permalink":"http://tankcat2.com/2017/07/21/my chester/","excerpt":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","text":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"Linkin Park","slug":"Linkin-Park","permalink":"http://tankcat2.com/tags/Linkin-Park/"},{"name":"Chester Bennington","slug":"Chester-Bennington","permalink":"http://tankcat2.com/tags/Chester-Bennington/"}]},{"title":"Storm的容错","slug":"fault tolerance of storm","date":"2017-07-18T16:00:00.000Z","updated":"2017-07-20T00:44:58.000Z","comments":true,"path":"2017/07/19/fault tolerance of storm/","link":"","permalink":"http://tankcat2.com/2017/07/19/fault tolerance of storm/","excerpt":"当某个worker失效时当某个worker失效时，Supervisor会将这个worker重启。如果在重启的过程中，这个worker不停地失效，并且不能向Nimbus汇报心跳，那么Nimbus会将这个worker重新分配到其他的机器。 当某个node失效时当某个node(物理节点)失效时，该node上的所有task都将超时，Nimbus会把这些task重新分配到其他的node上。 当Nimbus或者Supervisor守护进程失效时Nimbus和Supervisor的守护进程都是快速失败和无状态的，无论何时当发生意想不到的情况时，进程会自行终止，所有的状态信息都是存储在Zookeeper或者磁盘上。Nimbus和Supervisor的守护进程必须在监视器或虚拟光驱的监督下允许，所以一旦守护进程失效，就会立刻被重启。 ps: 守护进程是指一直在后台运行，且不受任何终端控制的进程，用于执行特点的系统任务。 Nimbus是否是单点失效如果Nimbus所在的node失效，worker进行将继续运行；若worker失效，Supervisor也会继续对它们进行重启。但是没有Nimbus，在必要的时候worker不能被重新分配到其他的node。","text":"当某个worker失效时当某个worker失效时，Supervisor会将这个worker重启。如果在重启的过程中，这个worker不停地失效，并且不能向Nimbus汇报心跳，那么Nimbus会将这个worker重新分配到其他的机器。 当某个node失效时当某个node(物理节点)失效时，该node上的所有task都将超时，Nimbus会把这些task重新分配到其他的node上。 当Nimbus或者Supervisor守护进程失效时Nimbus和Supervisor的守护进程都是快速失败和无状态的，无论何时当发生意想不到的情况时，进程会自行终止，所有的状态信息都是存储在Zookeeper或者磁盘上。Nimbus和Supervisor的守护进程必须在监视器或虚拟光驱的监督下允许，所以一旦守护进程失效，就会立刻被重启。 ps: 守护进程是指一直在后台运行，且不受任何终端控制的进程，用于执行特点的系统任务。 Nimbus是否是单点失效如果Nimbus所在的node失效，worker进行将继续运行；若worker失效，Supervisor也会继续对它们进行重启。但是没有Nimbus，在必要的时候worker不能被重新分配到其他的node。 Storm如何保证消息的可靠性处理Storm提供了某些机制来保证消息的可靠性处理，即使存在node失效或者信息丢失，包括“尽力服务best effort”、“至少处理一次at least once”和“只处理一次exactly once”。下面讲讲Storm是如何保证数据至少处理一次的。 消息被“完全处理fully processed”是什么意思从spout发出的一个tuple可能会触发成千上万个tuple的创建。结合下面这个例子考虑： 1234TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"sentences\", new KestrelSpout(\"kestrel.backtype.com\", 22133, \"sentence_queue\", new StringScheme()));builder.setBolt(\"split\", new SplitSentence(), 10).shuffleGrouping(\"sentences\");builder.setBolt(\"count\", new WordCount(), 20).fieldGrouping(\"split\", new Fields(\"word\")); 这个topology从一个Kestrel队列中读取sentence，将sentence划分成若干个word，接着将每个word和该word出现的次数发送给下一个bolt。这种情况下“从spout发出的一个tuple可能会触发成千上万个tuple的创建”：sentence中的每次word对应的tuple和携带有计数的word对应的tuple。构成的消息树如下所示： 如果这棵tuple树发送完成，并且树中的每一条消息都被正确地处理，那么就认为从spout发出的tuple被“完全处理”了。若某个tuple的消息树在给定的时间内没有被完全处理，那么就认为这个tuple失效了。这个超时的时间可以在构建topology时通过参数Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS来配置，默认值为30秒。 消息被完全处理或处理失败后会发生什么为了理解这个问题，我们先回顾一下从spout发出的tuple的生命周期。下面给出spout的接口： 1234567public interface ISpout extends Serializable&#123; void open(Map conf, TopologyContext context, SpoutOutputCollector collector); void close(); void nextTuple(); void ack(Object msgId); void fail(Object msgId);&#125; 首先，通过调用Spout的nextTuple()方法，Storm向Spout请求一个tuple。Spout使用open()方法中的SpoutOutputCollector向它的一个输出流中发送一个tuple。在发送时，Spout会为每个tuple提供一个”消息id”，用来在后面的处理过程中识别tuple。例如，KestrelSpout从kestrel队列中读取一条消息，然后发送一条带有“消息id”的消息，这个消息id是由Kestrel提供的。使用SpoutOutputCollector发送消息一般是这样的形式： 1_collector.emit(new Values(\"field1\", \"fields2\", 3),msgId); 接着，tuple会被发送到下游的消费bolt，Storm会追踪其创建的消息树。如果检测到某个tuple被完全处理了，根据消息id，Storm调用最初发送该tuple的Spout任务的ack()方法。类似地，若该tuple超时了，Storm会调用fail()方法。需要注意的是，对于每个tuple，响应ack和失败处理fail只会由最初创建这个tuple的Spout任务执行。换言之，在整个集群中，若Spout存在多个实例，但是tuple只会被创建它的那个Spout任务实例响应或者失效处理。 再以KestrelSpout为例来看看Spout需要进行哪些操作来保证消息的可靠性处理。当kestrelSpout从kestrel队列中读取一条消息时，可以看作是它“打开”了这条消息。这意味着，消息并没有真正从队列中取出，而是被标记为“挂起”的状态，等待消息被完全处理的信号。在挂起状态时，消息不会被发送到其他消费者。此外，若消费者断开了连接，所有挂起的消息会被放回队列中。消息在打开时，Kestrel会为每个消息提供一个唯一的id。当把tuple发送给SpoutOutputCollector时，KestrelSpout用“消息id”作为tuple的唯一标识。一段时间后，当KestrelSpout中的ack()或者fail()被调用时，KestrelSpout会根据消息id向Kestrel请求将消息从队列中移除(ack)，或者将消息重新放回队列中(fail)。 Storm的可靠性API使用Storm的可靠性机制时需要注意两点：第一，在tuple树上创建新的连接时需要通知Storm；第二，当处理完每个tuple时需要通知Storm。通过以上两点操作，Storm可以检测到tuple树何时被完全处理，并在适当的时候调用ack和fail。Storm API以一种非常精确的方式来实现这两个操作。 在tuple树上指定一个连接的操作称为锚定anchoring。当发送一个新的tuple时进行锚定。让我们用下面这个bolt为例，该bolt将包含sentence的tuple分割成若干包含word的tuple： 12345678910111213141516public class SplitSentence extends BaseRichBolt&#123; OutputCollector _collector; public void prepare(Map conf, TopologyContext context, OutputCollector collector)&#123; _collector = collector; &#125; public void execute(Tuple tuple)&#123; String sentence = tuple.getString(0); for(String word : sentence.split(\" \"))&#123; _collector.emit(tuple, new Values(word)); &#125; _collector.ack(tuple); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; declarer.declare(new Fields(\"word\")); &#125;&#125; 通过将输入tuple指定为emit()的第一个参数对每个单词对应的tuple进行锚定。这样，在下游的后续处理中若该tuple处理失败了，位于tuple树根节点的spout tuple就会被重新处理。相应地，如果按照下面的方法发送tuple： 1_collector.emit(new Values(word)); 这种方式被称为“非锚定”。在这种情况下，下游的tuple处理失败不会触发根节点tuple的重新处理。但这种非锚定的发送在某些场合下也是合适的，这取决于我们对topology的容错要求。 一个输出的tuple可以被锚定到多个输入tuple。这在数据流连接或者聚合操作时很有用。一个失败的多锚定的tuple会导致spout中多个tuple被重新处理。多锚定是通过指定一个tuple列表，而不是单一的tuple，来实现的，示例如下： 1234List&lt;Tuple&gt; anchors = new ArrayList&lt;Tuple&gt;();anchors.add(tuple1);anchors.add(tuple2);_collector.emit(anchors, new Values(1, 2, 3)); 多锚定把输出tuple加入到多个tuple树中。需要注意的是，多锚定可能会破坏tuple树的结构，并创建tuple的有向无环图，如下所示： Storm的实现既支持对tuple树的处理，也支持有向无环图的处理。 锚定就是如何把一个消息加入到指定的tuple树中——Storm可靠性处理API的接下来部分将具体说明何时完全处理完tuple树中的一个tuple。这是通过调用OutputCollector中的ack()和fail()方法实现的。回顾SplitSentence的例子，可以发现输入tuple是在左右单词tuple被发送出去之后被ack的。 我们可以使用OutputCollector的fail()方法使得位于tuple树根节点的spout tuple立即失效。比如，某个应用可能选择补货来自数据库客户端的一个异常，使得输入tuple立刻失效。通过这种方式，重新处理spout tuple就会比等待tuple超时更快。 处理的每个tuple都必须被ack或fail。Storm使用内存来追踪每个tuple，所以，如果不对其进行ack或者fail，负责追踪的任务很会就会发生内存溢出。 Bolt处理tuple的通用模式是：读取输入tuple，发送基于该tuple而生成的tuple，在execute()方法执行结束时ack输入tuple。大多数bolt都采取这样的处理方式。这些bolt大多属于过滤器或者简单的函数一类。Storm提供了接口BasicBolt涵盖了以上操作。例如，若使用BasicBolt，SplitSentence可以是如下所示： 123456789101112public class SplitSentence extends BaseBasicBolt&#123; public void execute(Tuple tuple, BasicOutputCollector collector)&#123; String sentence = tuple.getString(0); for(String word : sentence.split(\" \"))&#123; collector.emit(new Values(word)); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; declarer.declare(new Fields(\"word\")); &#125;&#125; 以上这种实现方式比之前的方法要简单许多，并且在语义上可以达到相同的效果。发送给BasicOutputCollector的tuple被自动锚定到输入tuple，并且在execute()执行结束后会自动ack输入tuple。 相反，做连接或者聚合操作的bolt可能需要推迟ack，直到一批tuple完成了某种计算结果。聚合和连接操作一般也需要对输出tuple进行多锚定。这个操作超出了IBasicBolt的应用范围。 在tuple被重新处理时如何使得应用正确运行按照软件设计的一般思路，答案是“取决于实际情况”。如果想要保证计算过程都能满足恰好一次的语义，可以使用Trident API。在某些情况下，比如数据分析任务中，丢弃一些数据是可以接受的，因此通过将参数Config.TOPOLOGY_ACKERS置为0，使得acker bolt的数量为0，从而取消应用的容错性。但是在其他情况下，需要保证每个计算过程满足至少处理一次的语义，并且不允许丢弃任何数据。如果所有的操作都是idenpotent，或者未来会发生重复删除的操作，那么这将十分有用。(an indemponent operation is one that has no additional effect if it is called more than once with the same input paramenters). Storm是如何以一种高效的方式实现可靠性的一个topology拥有一组特殊的acker任务，用来追踪每个spout tuple发出tuple的DAG。当一个acker发现一个DAG完成时，它会向创建该spout tuple的spout任务实例发送消息，让这个任务来ack消息。我们可以通过参数Config.TOPOLOGY_ACKERS来配置acker的数量，Storm默认值为1。 理解Storm可靠性实现的最好的方法是了解tuple和tuple DAG的生命周期。当一个tuple被创建时，无论是在spout还是bolt中，它被分配一个64位的id。这个id将被acker用来追踪每个spout tuple的tuple DAG。 当在bolt中发送一个新的tuple时，输入tuple 锚定的所有spout tuple的id将会被复制到新的tuple中。tuple被ack时，它会向适合的acker任务发送消息，通知如何修改tuple树。实际上，它通知acker的是“在这个spout tuple所在的tuple树中，我已经被完全处理了，这些是被锚定到我的新的tuple，需要加到这个树中”。 以下图为例，如果tuple D和 E是基于tuple C创造出来的，当C被ack时，tuple树需要做如下修改： 由于C被移除的同时D和E被加进来，所以tuple树不会过早地结束。 关于Storm是如何跟踪tuple树还有许多细节，正如前面所说的，在一个topology中我们可以设置任意数量的acker，但这带来一个新的问题：当某个tuple被ack，它如何知晓让哪个acker来发送消息？ Storm使用哈希算法将spout tuple匹配到acker上。由于每个tuple携带了它所在tuple树的spout tuple的id，因此它们知道需要和哪个acker通信。 另一个细节问题是acker是如何知道它们追踪的spout tuple是来自哪个spout 任务的。实际上spout task在发送一个新的tuple时，它会向适合的acker发送一个简单的消息，通知它这个spout tuple是和自己的任务id相关联的。当acker发现某个tuple 树被完全处理了，它就知道需要向哪个task发送完成消息。 acker 实际上不会直接追踪tuple树。对于拥有成千上万个节点的tuple树而言，追踪整个树会消耗完所有的内存。相反，acker采用一种特殊的策略，只要占用固定的内存空间来追踪每个spout tuple，大概20字节。这个追踪算法是Storm运行的关键，也是突破技术之一。 一个acker存储了spout tuple id到一组值得映射表。 第一个值是创建这个spout tuple的task id号，用于后面发送完成信号。第二个值是一个称为“ack val”的64位数字。这个值代表了整个tuple树的状态，与树的大小无关。因为这个值仅仅是这个tuple树中所有被创建或者被ack的tuple的tuple id进行异或运算得到的结果值。 当一个acker发现ack val的值变为0时，它就知道tuple树被完全处理了。由于tuple id是一个随机的64位数字，因此ack val碰巧为0是一种极小概率的事件。理论计算可得，在每秒ack一万次的情况下，需要5000万年才会发生一次错误。而且即使是这样，也只会在tuple碰巧失败的时候才会发生数据丢失的情况。 假设现在我们已经理解了这个可靠性算法，让我们再分析一下所有失败的情况，看看这些情况下Storm是如何避免数据丢失的： task失效时tuple没被ack：在这种情况下，位于根节点的spout tuple会在任务超时后重新处理。 Acker进程失效：在这种情况下，acker追踪的所有spout tuple都会由于超时而被重新处理。 Spout进程失效：在这种情况下，Spout任务的来源会重新处理这些消息。举个例子，Kestrel和RabbitMQ之类的队列会将所有挂起的消息重新放回队列中。 综上，Storm的可靠性机制是完全分布式的，可扩展的，且支持容错的。 调整可靠性由于acker进程是轻量级的，所以在topology中不需要很多的ack任务。我们可以通过Storm UI来监控它们的性能，它们的任务id为_acker。如果发现观察结果存在问题，就需要增加更多的acker任务。 如果不关心可靠性，也就是说，不担心在失败的情形下发生tuple的丢失，那么可以通过不跟踪tuple树来提升topology的性能。由于tuple树中的每个tuple都会携带一个ack消息，不追踪tuple树会使得传输的消息数量减半。同时，下游数据流中的id也会变少，降低网络带宽的消耗。 有三种方法可以移除Storm的可靠性机制。第一种方法是将Config.TOPOLOGY_ACKERS的值设置为0，在这种情况下，Storm会在spout发送tuple之后立即调用ack，这样tuple树就不会被追踪了。 第二种方法是基于消息本身移除可靠性。我们可以通过在SpoutOutputCollector.emit()方法中省略消息id来关闭spout tuple的追踪功能。 最后，如果不关系topology下游的tuple是否会失败，可以在发送tuple时选择发送发送非锚定unanchored的tuple。由于这些tuple不会被锚定到到任何一个spout tuple，因此如果它们没有被ack，也不会导致spout tuple的重新处理。","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://tankcat2.com/tags/storm/"},{"name":"容错","slug":"容错","permalink":"http://tankcat2.com/tags/容错/"}]},{"title":"Storm UI详解","slug":"storm_ui","date":"2017-05-22T08:32:31.000Z","updated":"2017-07-20T00:45:32.000Z","comments":true,"path":"2017/05/22/storm_ui/","link":"","permalink":"http://tankcat2.com/2017/05/22/storm_ui/","excerpt":"","text":"","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://tankcat2.com/tags/storm/"},{"name":"storm ui","slug":"storm-ui","permalink":"http://tankcat2.com/tags/storm-ui/"}]},{"title":"Storm Kafka之KafkaSpout","slug":"KafkaSpout","date":"2017-05-18T12:11:31.000Z","updated":"2017-07-20T00:46:14.000Z","comments":true,"path":"2017/05/18/KafkaSpout/","link":"","permalink":"http://tankcat2.com/2017/05/18/KafkaSpout/","excerpt":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。","text":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。 StaticHosts。这是另一个选择，不过broker$\\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建GlobalPartitionInformation类的实例，如下： 12345678Broker brokerForPartition0 = new Broker(\"localhost\");//localhost:9092,端口号默认为9092Broker brokerForPartition1 = new Broker(\"localhost\",9092);//localhost:9092,显示地指定端口号Broker brokerForPartition2 = new Broker(\"localhost:9092\");GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();partitionInfo.addPartition(0, brokerFroPartition0);// partition0 到 brokerForPartition0的映射partitionInfo.addPartition(1, brokerFroPartition1);partitionInfo.addPartition(2, brokerFroPartition2);StaticHosts hosts = new StaticHosts(partitionInfo); KafkaConfig创建KafkaSpout需要的另一个参数是KafaConfig，通过调用以下两个函数进行对象创建： 12public KafkaConfig(BrokerHosts host,String topic)public KafkaConfig(BrokerHosts host,String topic,String clientId) 其中，host可以为BrokerHost的任何一种实现，topic是一个topic的名称，clientId是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。 目前，KafkaConfig有两种扩展形式，SpoutcConfig提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。zkRoot用于存储consumer的offset，id用于唯一标识当前的spout。 1public SpoutConfig(BrokerHosts hosts,String topic,String zkRoot,String id) 除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout： 1234567891011//将当前的offset保存到Zookeeper的频率public long stateUpdateIntervals = 2000;//用于失效消息的重试策略public String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。public long retryInitialDetails = 0;public double retryDelayMultiplier = 1.0;//连续重试之间的最大延时public long retryDelayMaxMs = 60 * 1000;//当retryLimit低于0时，不停地重新发送失效的消息public int retryLimit = -1; KafkaSpout只接收一个SpoutConfig的实例作为参数。 下面给出一个实例： 首先创建一个名为couple的topic，如下： 1bin/kafka-topics.sh --create --zookeeper localhost:3030 --partitions 4 --replication-factor 1 --topic couple 写一个简单的Producer，将文件string_data.txt的记录发送到couple中，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class kafkaFileProducer&#123; private final String topic_name; private final String file_name; private final KafkaProducer&lt;String,String&gt; producer; private Boolean isAsync; public kafkaFileProducer(String topic_name,String file_name,Boolean isAsync)&#123; this.file_name=file_name; this.topic_name=topic_name; Properties properties=new Properties(); properties.put(\"bootstrap.servers\", \"localhost:9092\"); properties.put(\"client.id\",\"CoupleProducer\"); properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); producer=new KafkaProducer&lt;String, String&gt;(properties); this.isAsync=isAsync; &#125; public void sendMessage(String key,String value)&#123; long start_time=System.currentTimeMillis(); if(isAsync)&#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key),new CoupleCallBack(start_time,key,value)); &#125;else&#123; try &#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key,value)).get(); System.out.println(\"Sent message : ( \"+key+\" , \"+value+\" )\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args)&#123; String file_name=\"DataSource/Data/string_data.txt\"; String topic_name=\"couple\"; kafkaFileProducer producer=new kafkaFileProducer(topic_name,file_name,false); int lineCount=0; FileInputStream fis=null; BufferedReader br=null; try &#123; fis=new FileInputStream(file_name); br=new BufferedReader(new InputStreamReader(fis)); String line=null; while((line=br.readLine())!=null)&#123; lineCount++; producer.sendMessage(lineCount+\"\",line); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; class CoupleCallBack implements Callback&#123; private long start_time; private String key; private String message; public CoupleCallBack(long start_time, String key, String message) &#123; this.start_time = start_time; this.key = key; this.message = message; &#125; /* A callback method The user can implement to provide asynchronous handling of request completion. The method will be called when the record sent to the server has been acknowledged. */ @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; long elapsed_time=System.currentTimeMillis()-start_time; if(recordMetadata!=null)&#123; System.out.println(\"Message( \"+key+\" , \"+ message+\" ) sent to partition(\"+recordMetadata.partition()+\" ) , offset(\" +recordMetadata.offset()+\" ) in \"+elapsed_time+\" ms\"); &#125;else&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 编写一个简单的Storm Topology，利用KafkaSpout读取couple中的数据(一条条的句子)，并分割成一个个的单词，统计单词个数，如下： SplitSentenceBolt 1234567891011121314151617public class SplitSentenceBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String sentence=tuple.getStringByField(\"msg\"); System.out.println(tuple.getSourceTask()+\":\"+sentence); String[] words=sentence.split(\" \"); for(String word:words)&#123; basicOutputCollector.emit(new Values(word)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\")); &#125;&#125; WordCountBolt 123456789101112131415161718192021222324public class WordCountBolt extends BaseBasicBolt&#123; private Map&lt;String,Long&gt; counts=null; @Override public void prepare(Map conf,TopologyContext context)&#123; this.counts=new ConcurrentHashMap&lt;&gt;(); super.prepare(conf,context); &#125; public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=this.counts.get(word); if(count==null) count=0L; count++; this.counts.put(word,count); basicOutputCollector.emit(new Values(word,count)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\",\"count\")); &#125;&#125; ReportBolt 1234567891011121314public class ReportBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=tuple.getLongByField(\"count\"); String reportMsg=\"&#123; word : \"+word+\" , count : \"+count+\" &#125;\"; basicOutputCollector.emit(new Values(reportMsg)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"message\")); &#125;&#125; KafkaWordCountTopology 123456789101112131415161718192021222324252627282930313233343536373839public class WordCountKafkaTopology &#123; private static final String KAFKA_SPOUT_ID=\"kafka-spout\"; private static final String SPLIT_BOLT_ID=\"split-bolt\"; private static final String WORD_COUNT_BOLT_ID=\"word-count-bolt\"; private static final String REPORT_BOLT_ID=\"report-bolt\"; private static final String CONSUME_TOPIC=\"couple\"; private static final String PRODUCT_TOPIC=\"test\"; private static final String ZK_ROOT=\"/couple\"; private static final String ZK_ID=\"wordcount\"; private static final String TOPOLOGY_NAME=\"word-count-topology\"; public static void main(String[] args)&#123; BrokerHosts brokerHosts=new ZkHosts(\"192.168.1.118:3030\"); SpoutConfig spoutConfig=new SpoutConfig(brokerHosts,CONSUME_TOPIC,ZK_ROOT,ZK_ID); spoutConfig.scheme = new SchemeAsMultiScheme(new MessageScheme()); TopologyBuilder builder=new TopologyBuilder(); builder.setSpout(KAFKA_SPOUT_ID,new KafkaSpout(spoutConfig),3);//需要注意的是，spout的并行度不能超过topic的partition个数！ builder.setBolt(SPLIT_BOLT_ID,new SplitSentenceBolt(),1).shuffleGrouping(KAFKA_SPOUT_ID); builder.setBolt(WORD_COUNT_BOLT_ID,new WordCountBolt()).fieldsGrouping(SPLIT_BOLT_ID,new Fields(\"word\")); builder.setBolt(REPORT_BOLT_ID,new ReportBolt()).shuffleGrouping(WORD_COUNT_BOLT_ID); //builder.setBolt(KAFKA_BOLT_ID,new KafkaBolt&lt;String,Long&gt;()).shuffleGrouping(REPORT_BOLT_ID); Config config=new Config(); Map&lt;String,String&gt; map=new HashMap&lt;&gt;(); //map.put(\"metadata.broker.list\", \"localhost:9092\"); map.put(\"bootstrap.servers\", \"localhost:9092\"); map.put(\"serializer.class\",\"kafka.serializer.StringEncoder\"); config.put(\"kafka.broker.properties\",map); config.setNumWorkers(3); LocalCluster cluster=new LocalCluster(); cluster.submitTopology(TOPOLOGY_NAME,config,builder.createTopology()); Utils.sleep(10000); cluster.killTopology(TOPOLOGY_NAME); cluster.shutdown(); &#125;&#125; ​","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://tankcat2.com/tags/kafka/"},{"name":"storm","slug":"storm","permalink":"http://tankcat2.com/tags/storm/"}]},{"title":"列存储中常见压缩技术","slug":"compression","date":"2017-05-11T05:40:00.000Z","updated":"2017-07-20T00:46:44.000Z","comments":true,"path":"2017/05/11/compression/","link":"","permalink":"http://tankcat2.com/2017/05/11/compression/","excerpt":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种.","text":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种. 行程编码 Run-length Encoding行程编码的核心思想是将有序列中的相同元素转化成一个三元组&lt;属性值，该值第一次出现的位置，出现的次数&gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势. 位向量 Bit-Vector位向量的核心思想是，将一个列中相同的值转为一个二元组&lt;属性值，在列中出现的位置的位图&gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩. 词典编码 Dictionary Encoding词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文Processing a trillion cells per mouse click上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"压缩","slug":"压缩","permalink":"http://tankcat2.com/tags/压缩/"},{"name":"行程编码","slug":"行程编码","permalink":"http://tankcat2.com/tags/行程编码/"},{"name":"词典编码","slug":"词典编码","permalink":"http://tankcat2.com/tags/词典编码/"},{"name":"位向量","slug":"位向量","permalink":"http://tankcat2.com/tags/位向量/"}]},{"title":"“视图”漫游","slug":"view","date":"2017-05-10T06:17:31.000Z","updated":"2017-07-20T00:47:04.000Z","comments":true,"path":"2017/05/10/view/","link":"","permalink":"http://tankcat2.com/2017/05/10/view/","excerpt":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select.","text":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select. 从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查. 那视图有优点呢？换句话说，为什么要使用视图呢？归纳起来有四点： 简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系. 限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等. 支持列的动态计算.基本表一般都不支持这个功能，比如有一张orders订单表，包含产品数量produce_num与单价produce_price_each两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列total_price(product_num*produce_price_each)就可以直接查询出订单的总价了. 兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用. 既然说视图也是一种SQL语句，那么它和查询的区别是什么呢？简单来说，有三点： 存储上，视图存储是数据库设计的一部分，而查询则不是； 更新限制上，因为视图来自于基本表，所以可间接地对基本表进行更新，但是存在诸多限制，比如不能在使用了group by语句的视图中插入值. 排序结果上，通过查询，可以对一个基本表进行排序，而视图不可以. 此外，经常对视图的增删改查还是会转换为对基本表的增删改查，会不会降低操作的效率呢？其实未必，尤其是对于多表关联，视图创建后数据库内部会作出相应处理，建立对应的查询路径，反而有利于查询的效率，这就涉及到物化视图的知识了. 维基百科里解释道，物化视图(Materialized View)，也叫做快照，是包含了查询结果的数据库对象，可能是一个远程数据的本地副本，或者是一张表或连接结果的行或者列的子集，等.创建物化视图的过程有时候也被称作是物化，一种缓存查询结果的形式，类似于函数式编程中将函数值进行缓存，有时也称作是“预计算”，用来提高查询的性能与效率. 在关系型数据库中，如果涉及到对基本视图的查找或修改，都会转化为与之对应的基本表的查找或修改.而物化视图采取不同的方法.查询的结果被缓存在一个实体表中，而不是一个视图里，实际存储着数据的，这个实体表会随着基本表的更新而更新. 这种方法利用额外的存储代价和允许部分数据过期的代价，使得查询时的数据访问更加高效.在数据仓库中，物化视图经常使用，尤其是代价较大的频繁基本表查询操作. 和基本视图还有一点不同的是，在物化视图中，可以在任何一列上建立索引，相反，基本视图通常只能在与基本表相关的列上建立索引.","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"视图","slug":"视图","permalink":"http://tankcat2.com/tags/视图/"},{"name":"物化视图","slug":"物化视图","permalink":"http://tankcat2.com/tags/物化视图/"}]},{"title":"近日爱读诗词","slug":"poet","date":"2017-05-09T04:44:31.000Z","updated":"2017-05-09T08:13:28.000Z","comments":true,"path":"2017/05/09/poet/","link":"","permalink":"http://tankcat2.com/2017/05/09/poet/","excerpt":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景","text":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景初夏即事石梁茅屋有弯碕，流水溅溅度两陂。晴日暖风生麦气，绿阴幽草胜花时。ps:读这首诗的那天正好是立夏苔白日不到处，青春恰自来。苔花如米小，也学牡丹开。ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"诗词","slug":"诗词","permalink":"http://tankcat2.com/tags/诗词/"}]},{"title":"聚类索引与非聚类索引","slug":"index","date":"2017-05-06T13:08:00.000Z","updated":"2017-07-20T00:47:26.000Z","comments":true,"path":"2017/05/06/index/","link":"","permalink":"http://tankcat2.com/2017/05/06/index/","excerpt":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能.","text":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能. 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示. 非聚集索引非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示. 两者的区别实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别. 我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。我们把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。 如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”。 通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。 两种索引的应用场合 动作描述 聚集索引 非聚集索引 经常对列进行分组排序 √ √ 返回某个范围内的数据 √ × 一个或者极少不同的值 × × 小数目的不同值 √ × 大数目的不同值 × √ 频繁更新的列 × √ 频繁更新索引列 × √ 外键列 √ √ 主键列 √ √","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://tankcat2.com/tags/索引/"}]},{"title":"<刀锋>观后感","slug":"daofeng","date":"2017-04-20T12:11:31.000Z","updated":"2017-05-05T12:09:24.000Z","comments":true,"path":"2017/04/20/daofeng/","link":"","permalink":"http://tankcat2.com/2017/04/20/daofeng/","excerpt":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.”","text":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.” “因为人不论男男女女，都不仅仅是他们自身；他们也是自己出生的乡土，学步的农场或城市公寓，儿时玩的游戏，私下听来的山海经，吃的饭食，上的学校，关心的运动，吟哦的诗章，和信仰的上帝。这一切东西把他们造成现在这样，而这些东西都不是道听途说就可以了解的，你非得和那些人生活过。要了解这些，你就得是这些。 ” 无论中英文，都是一流的文字，解释了各种文化之间的冲突，以及冲突误解的永恒性。 很少有外国作品上让我读得这么舒服，这完全归功于周旭良老师的翻译功底，整本书翻译地非常好，读起来如沐春风。书写得很平淡，但每个角色都很有意思，拉里最为迷人。很奇怪，刚开始看的时候我脑子里对拉里的想象，居然是血战钢锯岭里的戴斯蒙特，这里也仅是人物形象。拉里一直追寻的答案，等同于追求终极真理，而这个问题最终都会归结到理性与精神的绝对满足。真的很难以想象，拉里这样的人，现实中又有多少，他们的生活又是怎样的？这种绝对的内心的泰然平和，我生生世世估计也做不到吧。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://tankcat2.com/categories/读书笔记/"}],"tags":[{"name":"传记","slug":"传记","permalink":"http://tankcat2.com/tags/传记/"}]},{"title":"<简明美国史>观后感","slug":"historyUSA","date":"2017-04-16T12:11:31.000Z","updated":"2017-05-05T12:08:22.000Z","comments":true,"path":"2017/04/16/historyUSA/","link":"","permalink":"http://tankcat2.com/2017/04/16/historyUSA/","excerpt":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。","text":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。 有人说陈勤老师的这本美国史写得实在是太简太浅显，但是对我这种历史水平只停留在高中课本上的”史盲“来说，基本上是够了，从脉络上了解美国自1620年《五月花号公约》至2016年奥巴马最后的执政之间所发生的重大历史事件，这其中涵盖了美国从英属殖民地开始，到1776年《独立宣言》宣告独立，到1860年林肯领导南北战争，再到一战、二战、冷战，以及至今美国发生的种种。读完全书的第一感受是，陈勤老师应当是亲美派的，书中给我描述的美国是一个有趣、鲜活、有人味的美国，虽然对历史变革中发生的流血事件只是轻描淡写地带过，但还是能些许体会到”世界何尝不简单，历史从来不温柔“这一面。读完一遍脑海中对美国的历史线还是稍有混乱，有时间自己再整理整理。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://tankcat2.com/categories/读书笔记/"}],"tags":[{"name":"历史","slug":"历史","permalink":"http://tankcat2.com/tags/历史/"}]},{"title":"MIT 6.824 Lab 1 MapReduce","slug":"Lab1_MapReduce","date":"2017-03-14T14:57:31.000Z","updated":"2017-05-06T13:05:26.000Z","comments":true,"path":"2017/03/14/Lab1_MapReduce/","link":"","permalink":"http://tankcat2.com/2017/03/14/Lab1_MapReduce/","excerpt":"引言在这个实验中，我们将构建一个MapReduce的类库，作为使用Go语言进行编程与构建容错分布式系统的介绍。首先，我们将编写一个简单的MapReduce程序。接着，编写一个Master程序，手动向MapReduce的worker分发任务，解决worker的失效。类库的接口与容错的方法，和论文MapReduce: Simplified Data Processing on Large Clusters类似。","text":"引言在这个实验中，我们将构建一个MapReduce的类库，作为使用Go语言进行编程与构建容错分布式系统的介绍。首先，我们将编写一个简单的MapReduce程序。接着，编写一个Master程序，手动向MapReduce的worker分发任务，解决worker的失效。类库的接口与容错的方法，和论文MapReduce: Simplified Data Processing on Large Clusters类似。 软件工具我们可以使用 Go语言完成该实验(或所有实现).Go的网站上有许多教程，在个人计算机上安装与开发Go十分简便。评测实验代码使用的Go版本是1.7，因此我们也最好使用该版本进行代码编写，以免出现一些版本问题。 课程提供了部分MapReduce的实现代码，包括分布式与非分布式的操作。我们可以使用 git获取最初的类库版本。该课程的git仓库链接为git://g.csail.mit.edu/6.824-golabs-2017。我们可以使用下面的命令，克隆一份类库到自己的计算机上。 1234git clone git://g.csail.mit.edu/6.824-golabs-2017 6.824cd 6.824lsMakefile src Git提供了追踪代码修改的功能。举个例子，如果我们需要对编写的代码进行checkpoint，可执行下面的命令提交更改： 1git commit -am 'partial solution to lab 1' MapReduce的实现部分提供了两种模式操作，即顺序的与分布式的。前者，在一个时刻内只存在一个map或者reduce的task在执行：首先，第一个map的task执行结束，接着是第二个、第三个…；当所有的map task均执行完后，第一个reduce的task开始执行，接着是第二个、第三个…。这种模式的运行速度不快，但是对于debug是有用的。分布式的模式同时运行多个worker线程，首先并行执行map的task，接着是reduce的task。这种模式执行速度快，但是实现与调试较为困难。 熟悉源码mapreduce包提供了Map/Reduce的类库。应用程序调用Distributed()（位于master.go文件）以启动一个job作业，也可以调用Sequential()(也位于master.go)，在debug的时候启动一个顺序执行。 代码按照以下步骤执行一个job： 应用程序提供一些输入文件，一个map函数，一个reduce函数，以及reduce task的并行度(nReduce)； 使用以上信息，创建一个master。启动一个RPC服务器(见master rpc.go文件)，等待worker注册完毕(使用RPC调用Register()，定义在master.go文件)。当task可用时(步骤4与步骤5)，schedule()(见文件schedule.go)决定如何将这些task分配给worker，以及如何处理worker的失效； master将每个输入文件指定给一个map task，为每个map的task至少调用一次doMap()(见common_map.go)。当使用Sequential()或者是向一个worker发出DoTask的RPC请求时，采取同样的做法。每次调用doMap()会读取适当的文件，在该文件的内容上使用map 函数，并将中间结果key/value键值对写入nReduce个中间文件。doMap()对key进行哈希，挑选一个中间文件，则相应的reduce task会处理这个key。在所有的map task执行结束后将会存在$nMap \\times nReduce$个文件。每个文件名包含一个前缀：map task与reduce task的序号。如果有两个map task和三个reduce task，则map task会创建六个中间文件，如下所示： 123456mrtmp.xxx-0-0mrtmp.xxx-0-1mrtmp.xxx-0-2mrtmp.xxx-1-0mrtmp.xxx-1-1mrtmp.xxx-1-2 每个worker必须能够读取其他worker读写的文件。现实应用中的部署采用分布式存储系统来实现这一点，比如GFS，即使worker运行在不同的机器上。在实验中，我们在同一个机器上运行所有的worker，使用本地文件系统； 接下来master为每个reduce task至少调用一次doReduce()(见common_reduce.go)。与doMap()一样，直接或者通过一个worker进行操作。reduce task r 的doReduce()从每个map task中收集第r个中间文件，为文件中的每个key调用reduce 函数。reduce task会产生nReduce个结果文件； master调用mr.merge()(见master_splitmerge.go)，将nReduce个文件的内容合并到一个文件中； master向每个worker发送RPC Shutdown，接着关闭自己的RPC服务器。 注意：在接下来的练习中，我们需要自己重写/调整 doMap()，doReduce()和schedule()，这些方法分布位于common_map.go，common_reduce.go和schedule.go文件中。我们也需要在../main/wc.go文件中编写map和reduce函数。 我们不需要修改其他任何文件，但是阅读这些文件对理解其他方法是如何参与整体架构的运行是有帮助的。 第一部分：Map/Reduce的输入与输出在给定的Map/Reduce代码实现中存在代码缺失。在写我们的第一个Map/Reduce函数前，需要完成顺序实现模块。特别地，给定的代码中缺失了两个关键部分：划分一个map task的输出的函数，和收集一个reduce task的所有输入的函数。这两个模块分别被位于common_map.go和common_reduce.go中的 doMap()，doReduce()执行。这些文件中的注释可为我们指出正确的方向。 为了验证是否正确地实现了doMap()和doReduce()的编写，课程提供了一组用Go编写的测试套件，用于检测正确性。这些测试程序位于test_test.go文件中。可以执行下面的命令，运行顺序模式的测试程序： 12345cd 6.824export \"GOPATH=$PWD\" # go needs $GOPATH to be set to the project's working directorycd \"$GOPATH/src/mapreduce\"go test -run Sequentialok mapreduce 2.694s 如果运行测试程序的输出结果不是ok，则说明我们的程序中有bug。为了展示更详细的输出内容，在common.go文件中设置debugEnabled = true，并在上面的测试命令中添加-v。则我们会得出类似下面的更多输出信息： 123456789101112131415env \"GOPATH=$PWD/../../\" go test -v -run Sequential=== RUN TestSequentialSinglemaster: Starting Map/Reduce task testMerge: read mrtmp.test-res-0master: Map/Reduce task completed--- PASS: TestSequentialSingle (1.34s)=== RUN TestSequentialManymaster: Starting Map/Reduce task testMerge: read mrtmp.test-res-0Merge: read mrtmp.test-res-1Merge: read mrtmp.test-res-2master: Map/Reduce task completed--- PASS: TestSequentialMany (1.33s)PASSok mapreduce 2.672s 第二部分：单个worker的单词计数现在我们要实现一个简单的Map/Reduce程序——单词统计。在main/wc.go中，发现函数体空的mapF()和reduceF()。我们的任务是填充这两个函数，使得wc.go汇报每个单词出现的频数。一个单词是任何连续的字母序列，由unicode.IsLetter控制。 在~/6.824/src/main有一些输入文件，以pg-* txt形式为路径名，下载自 Project Gutenberg。下面是添加了输入文件运行wc： 1234567cd 6.824export \"GOPATH=$PWD\"cd \"$GOPATH/src/main\"go run wc.go master sequential pg-*.txtcommand-line-arguments./wc.go:14: missing return at end of function./wc.go:21: missing return at end of function 由于mapF()和reduceF()没有实现，所以出现编译错误。 回顾论文MapReduce: Simplified Data Processing on Large Clusters的第二部分。我们实现的mapF()和reduceF()与第2.1章节中描述的有一点区别。我们需要向mapF()传入文件名和文件内容，将内容分割成蛋醋，返回一个mapreduce.KeyValue的slice (Go的一种类型)。我们可以选择放入mapF()输出的key和value的内容。对于单词计数，使用单词作为key才有意义。对于每个key和由mapF()生成的该key的slice，会调用一次reduceF()，返回一个string，包含该key出现的频数总和。 提示：a good read on what strings are in Go is the Go Blog on strings. 提示：我们可以使用 strings.FieldsFunc 来将string划分成单词； 提示：我们可以使用strconv 包(http://golang.org/pkg/strconv/) ，很方便地将string转为integer。 我们可以使用下面的例子进行测试： 12345678cd \"$GOPATH/src/main\"time go run wc.go master sequential pg-*.txtmaster: Starting Map/Reduce task wcseqMerge: read mrtmp.wcseq-res-0Merge: read mrtmp.wcseq-res-1Merge: read mrtmp.wcseq-res-2master: Map/Reduce task completed14.59user 3.78system 0:14.81elapsed 输出结果将保存在文件mrtmp.wxseq中。如果代码实现是正确的，则执行下面的命令将输出下面的信息： 1234567891011sort -n -k2 mrtmp.wcseq | tail -10he: 34077was: 37044that: 37495I: 44502in: 46092a: 60558to: 74357of: 79727and: 93990the: 154024 执行下面的命令可以删除输出文件和所有中间文件： 1rm mrtmp.* 为了方便测试，可以执行下面的命令，并报告程序的正确与否。 1bash ./test-wc.sh 第三部分：分发MapReduce task我们当前的代码实现在一个时刻只运行一个map和reduce的task。Map/Reduce最大的卖点之一就是它可以自动地将顺序执行的代码并行化，而免去了开发者的额外工作。在这部分实验中，我们要实现另一个版本的MapReduce，将工作划分成多个worker线程，使用多核并行执行。尽管不能与现实MapReduce的部署一样，在多台机器上分布式部署，我们可以使用RPC来模拟分布式计算。 mapreduce/master.go中的代码主要用于管理一个MapReduce的job。课程在mapreduce/worker.go的代码中提供了一个完整的worker线程的代码，并在mapreduce/common_rpc.go中提供了处理RPC的代码。 我们的工作是实现mapreduce/schedule.go中的schedule()函数。master在一个MapReduce的job运行期间，两次调用schedule()，一次是为了Map操作，一次是为了Reduce操作。schedule()的功能是将task分发到可用的worker上执行。一般来说，task的数量多于worker线程的数量，因此schedule()必须每次给一个worker分配一组task，并且只有当所有的task都执行完时，schedule()才可以返回。 schedule()通过读取参数registerChan获取worker的集合。该channel为每个worker生成一个包含worker的RPC地址的字符串。某些worker可能在调用schedule()前就已经存在，而有些可能在schedule()运行期间被启动。schedule()需要使用所有的worker，包括那些在启动之后出现的。 使用mapreduce/common_rpc.go中的call()向一个worker发送一个RPC请求。第一个参数是worker的地址，从registerChan中获取。第二个参数是Worker.DoTask。第三个参数是结构体DoTaskArgs，最后一个参数是nil。 在这部分我们只需要修改schedule.go，如果修改了其他文件，比如debug部分，需要在提交测试之前还原到初始内容。 为了测试代码，我们使用与第一部分相同的测试套件，但是需要将-run Sequential替换成-run Basic。这回执行不包含worker失效的分布式测试代码，代替前面运行的顺序模式代码： 1go test -run TestBasic 提示：RPC package 记录了Go的RPC包。 提示：schedule()需要向并行的worker发送RPC，以便worker可能并发运行task。我们可以在 Concurrency in Go上找到实现这一目标的语句。 提示：schedule()必须等待一个worker执行结束，才能给该worker分配下一个task。我们可以发现Go的channel很有用。 提示：我们可以发现 sync.WaitGroup 很有帮助。 提示：追踪bug最简单的方法是插入打印状态信息的语句(可调用common.go中的debug)，执行命令go test -run TestBasic &gt; out收集文件中的输出数据，并思考输出的内容与理解实现的代码是否匹配。最后一步思考是很重要的。 提示：为了检测代码是否包含竞争状态，运行Go的race detector：go test -race -run TestBasic &gt; out 注意：课程给出的代码是在一个UNIX进程下执行worker线程，并可利用一台机器的多核。需要进行某些调整，以在多台机器上利用网络通信运行worker。RPC需要使用TCP而不是UNIX-domain的socket。需要在所有的机器上启动worker，并且所有的机器需要通过某种网路文件系统共享存储。 第四部分：处理worker失效这部分我们将使master处理失效的worker。MapReduce以一种相对容易的方式处理，因为worker不会保存持久的状态信息。如果一个worker失效了，master发送给该worker的任何RPC也将失效，由于一个timeout。因此，如果master发送的RPC失效了，master需要将分配给失效worker的task重新分配给另一个worker。 一个RPC失效不一定代表对应的worker不行执行task，worker可能已经执行了，但是回应丢失了，或者work仍然在执行而master的RPC超时了。因此，可能导致两个worker接收到相同的task，计算，并生成输出数据。一个map或者reduce函数的两次调用，会生成相同的输出结果(即map和reduce函数是函数式的)。所以如果后续处理去读一个输出或者读取另一个，不会发生不一致性。此外，MapReduce的框架保证了map和reduce的函数输出操作是原子性的：输出文件要么是不存在，要么是包含map或者reduce函数的一次执行的全部输出内容(实验代码并非真正实现了这一点，只是在task结束时worker失效，因此不存在一个任务的并发执行)。 我们不需要处理master的失效。使master具备容错较为困难，因为master需要维持状态信息，以便在失效后进行操作恢复。后面的实验致力于解决这一问题。 我们实现的代码必须通过test_test.go中的两个测试案例。第一个案例测试了一个worker的失效，第二个测试了多个worker的失效。这些测试案例会阶段性地启动新的worker，这样master可以继续向下运行，但是在处理完一部分task后这些worker失效。执行下面的命令运行这些测试： 1go test -run Failure 在这部分我们只需要修改schedule.go，如果修改了其他文件，比如debug部分，需要在提交测试之前还原到初始内容。 运行所有的测试我们可以执行脚本src/main/test-mr.sh来运行所有的测试案例。正确的代码实现输出信息类似于下面的内容： 123456789101112131415bash ./test-mr.sh==&gt; Part Iok mapreduce 3.053s==&gt; Part IIPassed test==&gt; Part IIIok mapreduce 1.851s==&gt; Part IVok mapreduce 10.650s==&gt; Part V (challenge)Passed test","categories":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"http://tankcat2.com/categories/MIT6-824/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://tankcat2.com/tags/MapReduce/"}]},{"title":"MIT 6.824 Lecture 1 MapReduce","slug":"Lecture1_MapReduce","date":"2017-03-14T14:52:31.000Z","updated":"2017-05-06T13:05:38.000Z","comments":true,"path":"2017/03/14/Lecture1_MapReduce/","link":"","permalink":"http://tankcat2.com/2017/03/14/Lecture1_MapReduce/","excerpt":"Distributed System Engineering 什么是分布式系统？ 多个协同工作的计算机 大规模数据库，P2P文件共享，MapReduce，DNS，等 许多关键性基础设施也是分布式的！","text":"Distributed System Engineering 什么是分布式系统？ 多个协同工作的计算机 大规模数据库，P2P文件共享，MapReduce，DNS，等 许多关键性基础设施也是分布式的！ 为什么需要分布式？ 为了连接物理上独立的实体 为了通过隔离实现安全 为了通过副本实现容错 为了通过并行CPU/内存/磁盘/网络提高吞吐量 分布式遇到的问题？ 复杂：许多并发的部分 必须解决部分失效 难以实现潜在性能 为什么开这门课？ 趣味性：难题+强大的解决方案 实用性：在实际系统中使用，受大型web网站的兴起驱动 科研性：许多进程+未解决的大问题 动手能力：在实验中构建复杂的系统 Course Structure 课程网址：http://pdos.csail.mit.edu/6.824 课题组人员 Robert Morris, lecturer Frans Kaashoek, lecturer Lara Araujo, TA Anish Athalye, TA Srivatsa Bhat, TA Daniel Ziegler, TA 课程的组成部分 课堂讲解 文献阅读 中期检测(2) 实验 期末考试(可选) 课堂讲解 ​大概念，论文讨论，实验 文献阅读 研究性论文，经典+最新的 某些论文论述了关键概念和重要细节 课堂大部分时间是有关文献的讲解，请做好课前预习工作 每篇论文都会有一个简短的问题，需要向我们提交你的回答，问题与答案的提交截止时间为晚上10点 中期检测是随堂的；期末考试定在考试周 实验目标 对某些重要技术的深入理解 分布式编程 第一个实验室定于周五起 一个星期后一段时间 实验内容 Lab 1: MapReduce Lab 2: 使用Raft，利用副本进行容错 容错key/value存储 共享key/value存储 学期末可选的最终项目，一组2-3人 最终的项目替代Lab 4 每个人考虑好一个项目，向我们清楚地阐述 实验的份数取决于通过的测试案例个数 我们会提供测试案例，因此你需要知道你是否能完成地很好 注意：如果一个案例大部分情况下是通过的，少数情况不能通过，则取决于我们运行这个案例是否是通过的 实验的调试是耗时的 早日动手 在Piazza上进行提问 在TA办公时间进行答疑 Main Topics 这个课程是关于基础架构的，可被应用使用。将分布式的概念从应用程序中抽象出来。三大抽象分别是： 存储 通信 计算 [图解：用户，应用服务器，存储服务器] 一些话题会反复出现 话题：实现 RPC，线程，并发控制 话题：性能 理想目标：可扩展的吞吐 Nx 服务器 $\\rightarrow$ Nx 总吞吐，通过并行CPU，磁盘，网络 因此处理更大的负载只需购买更多的机器 N的增大增加了扩展的难度 负载不均衡 不可并行化的代码：初始化，交互 共享资源的瓶颈，比如网络 话题：容错 1000个服务器，负载的网络 $\\rightarrow$ 总会有故障发生 我们想对应用程序隐藏这些失效 我们总需要： 可用性—— 应用程序可以继续运行 持续性—— 当故障被修复时，应用程序的数据可被恢复使用 大概念：备份服务器 如果有服务器宕机了，客户机可使用其他服务器 话题：一致性 通用基础设施需要定义明确的行为，比如“Get(k) yields the value from the most recent Put(k,v)” 实现良好的行为是困难的！ 多个副本服务器难以保持一致 在多级更新的过程中客户机可能出现宕机 服务器在比较尴尬的时出现宕机，比如在执行结束后和发送回复前 网络可能会使活跃的服务器看起来是不活跃的；存在“分裂的集群”的风险 一致性与性能问题 一致性需要通信，比如“get latest Put()” 强一致性可能会造成系统处理速度降低 高性能可能会带来若一致性 Case Study：MapReduceMapReduce是6.824课程重要话题中的一个很好的示例，也是Lab 1主要内容。 MapReduce Overview 概念：在多个TB级数据集上进行长达多个小时的计算 比如对爬行网页的图结构分析 只有在数千台计算机的同时计算下可行 通常不是分布式系统的专家也可以进行开发 分布式是比较令人头疼的，比如处理故障失效 总体目标：非专家的程序员也可以轻松对数据处理进行划分，多个服务器并行处理，提高效率 用户自行定义Map和Reduce函数：顺序编码；通常是相当简单的 MR在数千台机器上运行，输入数据规模巨大，隐藏了分布实现细节 MapReduce的抽象视图 输入数据被划分成M个文件 Input 1 $\\rightarrow$ Map $\\rightarrow$ a, 1 b,1 c,1 Input 2 $\\rightarrow$ Map $\\rightarrow$ b,1 Input 3 $\\rightarrow$ Map $\\rightarrow$ a,1 c,1 ​ | | | ​ | $\\rightarrow$ Reduce $\\rightarrow$ c,2 ​ —- $\\rightarrow$ Reduce $\\rightarrow$ b,2 MR为每个输入文件调用Map( )，生成k2,v2的集合 中间临时数据 每一次Map( )的调用就是一个task MR为给定的可k2聚集所有的中间值v2，并将它们传送给Reduce( ) Reduce( )生成最终输出结果的集合，并存储在R个输出文件中 示例：单词统计 输入是数千个文本文件 Map(k,v) 将v划分成单个单词 对于每个单词w，emit(w,”1”) Reduce(k,v) emit(len(v)) MapReduce想用户隐藏了许多负载的实现细节 启动服务器 追踪完成作业的task 数据移动 故障恢复 MapReduce具有良好的可扩展性 N个机器可以提供N倍的吞吐量 假设M和R均大于N，即大量的输入文件与输出key Map( )可以并行运行，并且不存在交互，Reduce( )同样如此 因此可以通过部署更多的机器来获取更高的吞吐量，而不是每个应用程序配置专用的高效并行。机器的代价来的比程序员低！ 什么可能会限制性能？ 我们关注可以优化的地方 CPU？内存？磁盘？网络？ 在2004年，受限于“网络截面带款” [图解：服务器，网络交换机层次树] 需要注意的是所有的数据经过Map$\\rightarrow$Reduce操作后均流经网络 根节点的交换机：100-200 GB/s 1800台机器，因此是55MB/s/机器 因此当时是关注最小化网络数据传输量(如今的数据中心网络速度大大提高) 更多的细节(论文中的图1) master：将task分配给worker；记录所有中间数据的存放位置 M个Map task，R个Reduce task 输入数据存储在GFS中，每个Map的输入文件有三个拷贝副本 所有的机器均运行GFS和MR worker task的数量多余worker 当旧的task处理完毕后，master分发新的task Map worker通过哈希将中间数据划分为R个分区，并存储在本地磁盘 直到Map执行完毕后调回调用Reduce master需要告知Reduce，从Map worker中读取中间数据分区 Reduce 将最终的结果写入GFS，一个Reduce task对应一个文件 细节的设计是如何减少低速网络带来的影响？ Map的输入数据是读取自存储在本地磁盘GFS中的副本，无需经由网络 中间数据只会在网络上传输一次，Map worker将数据写入本地磁盘，而不是GFS 中间数据被划分成多个文件，单个文件中存储大量的key，一次传输尽量多的数据更为高效 如何获取良好的负载均衡？ 扩展的关键——N-1个服务器等待1个服务器的情况是较为糟糕的，但是某些task确实比其他task的处理时间更长 解决方法：令task的数量多余worker 当worker处理完之前的task时，master分发新的task 则不存在一个太大的task以至于占据了全部处理时间 处理速度快的服务器比慢速的服务器处理更多的任务 什么是容错？ 在MR作业期间如果一个服务器宕机了怎么办？ 隐藏故障是简易编程重要的一部分 为什么不从头开始重新执行作业呢？ MR只重新运行失效的Map和Reduce MR要求它们均是纯函数： 不需要在调用期间保存状态信息 除了输入与输出数据，不需要进行其他文件读写操作 在task之间不需要进行通信 则重新执行也可获得相同的输出结果 以上有关纯函数的需求是MR的的主要限制，与其他并行编程框架相比，但是对于MR的简易性是至关重要的 worker的故障恢复细节： Map worker 宕机： master发现ping worker无回应 宕机的worker的Map中间数据输出丢失，但是每个Reduce task需要这些数据 master重新运行，为输入数据在GFS上的副本分配task 某些Reduce worker可能已经读取了失效worker的中间数据，这里我们依赖于函数式与确定性的Map()！ 如果Reduce已经获取全部的中间数据，则master不要重新运行Map，尽管后续的Reduce宕机可能需要强制失效的Map重新执行 Reduce worker宕机： 已经执行结束的task没问题——与副本一同存储在GFS中 master重新启动位于其他worker上的没有完成的task Reduce worker在写输出的过程中出现宕机： GFS对输出进行原子重命名，以防止执行结束前数据可见，这样master在其他地方重新运行Reduce task 其他故障或者问题： 如果master为同一个Map( ) task指定两个worker怎么办？ 可能master错误地判断其中一个worker已经失效，从而只把其中一个worker的信息通知给Reduce worker 如果master为同一个Reduce( ) task指定两个worker怎么办？ 则这两个worker会同时向GFS写相同的输出文件！GFS原子重命名可防止mixing；一个完整的文件是可见的 如果某个worker处理速率很慢——straggler？ 可能是由于硬盘问题，此时master会为当前的几个task启动第二个副本 如果某个worker由于毁坏的h/w或者s/w导致输出结果不正确怎么办？ MR会认定为是CPU和软件的停止运行故障 对于哪些应用程序，MapReduce不能表现良好的性能？ 不是所有的应用程序都适合map/shuffle/reduce模式的 规模小的数据，处理代价高，不如不是网站后端的数据 大规模数据的少量更新，比如在一个大规模索引中添加少量文档 不可预知的读取 (Map和Reduce均不能选择输入) 频繁数据转移，比如page-rank，可以使用多个MR但是效率低下 结论 MapReduce单方面使大集群计算流行 不是最高效/灵活 良好的可扩展性 简易编程——故障与数据迁移均被隐藏 这些是实践中的良好权衡，我们将在课程后面看到一些更先进的改进，实验愉快！","categories":[{"name":"MIT6.824","slug":"MIT6-824","permalink":"http://tankcat2.com/categories/MIT6-824/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","permalink":"http://tankcat2.com/tags/MapReduce/"}]},{"title":"Go初探","slug":"go","date":"2017-03-12T13:31:31.000Z","updated":"2017-05-05T11:27:38.000Z","comments":true,"path":"2017/03/12/go/","link":"","permalink":"http://tankcat2.com/2017/03/12/go/","excerpt":"基础包，变量和函数 每个Go程序都是由包组成的，程序运行的入口是main。下面的程序使用并导入了包fmt和math/rand。一般地，包名与导入路径的最后一个目录一致，即math/rand包由package rand语句开始。","text":"基础包，变量和函数 每个Go程序都是由包组成的，程序运行的入口是main。下面的程序使用并导入了包fmt和math/rand。一般地，包名与导入路径的最后一个目录一致，即math/rand包由package rand语句开始。 1234567891011package mainimport ( \"fmt\" \"math/rand\")func main()&#123; fmt.Println(\"My favorite number is\", rand.Int(10))&#125;output:My favorite number is 1 上述的导入语句是使用了圆括号进行组合导入，即打包导入。也可以编写成多个导入语句，即： 12import \"fmt\"import \"math/rand\" 在Go中，首字母大写的名称是被导出的，比如Pi是从包math中导出的名称 。在导入包之后，你只能访问包所导出的名称，任何包未导出的名称均不能被包外的代码访问。执行下面的代码会出错，需要将math.pi替换成math.Pi。 123456789101112package mainimport( \"fmt\" \"math\")func main&#123; fmt.Println(math.pi)&#125;output:tmp/sandbox738653879/main.go:9: cannot refer to unexported name math.pitmp/sandbox738653879/main.go:9: undefined: math.pi 在Go中，函数可以没有参数，或者接受多个参数，在下面的例子中，add接受两个int类型的参数。需要注意的是，类型位于变量名之后。 1234567891011package mainimport \"fmt\"func add(x int, y int)int&#123; return x+y&#125;func main()&#123; fmt.Println(add(1,2))&#125;output:3 当函数的两个或者连续多个参数是同一类型时，可以省略除了最后一个参数的其他参数的类型，比如add函数的参数可以重新写成x, y int。 此外，函数的返回值数量可以是任意多个，比如下面的例子swap函数返回了两个字符串。 123456789101112package mainimport \"fmt\"func swap(x, y string)(string,string)&#123; return y,x&#125;func main()&#123; a, b := swap(\"hello\",\"world\") fmt.Println(a,b)&#125;output:world hello Go的返回值可以被命名，并可像在函数体开头声明的变量那样使用。下面的例子中，返回值是两个int类型的变量x和y，函数内的return语句没有参数，返回各个变量的当前值，称为裸返回。裸返回语句只能用在类似下面例子中的短函数中，在长函数中使用会影响代码的可读性。 12345678910111213package mainimport \"fmt\"func split(sum int)(x, y int)&#123; x = sum * 4 / 9 y = sum - x return&#125;func main()&#123; fmt.Println(split(17))&#125;output:7 10 关键字var用于定义变量，与函数的参数列表一样，类型放在变量名后面。下面的例子中定义了一个变量列表，var语句可以在包或者函数级别中使用。 12345678910package mainimport \"fmt\"var c, clojure, java intfunc main()&#123; var flag bool fmt.Println(flag, c, clojure, java)&#125;output:false 0 0 0 变量定义如果不包含初始值，则对应默认值，比如int的默认值为0，bool的默认值为false，string的默认值为””。 当变量定义包含初始值时，一个变量对应一个值。如果初始化使用的是表达式，则变量可以省略类型，从初始值中获得类型。 12345678910package mainimport \"fmt\"var flag, label bool = true, falsefunc main()&#123; var c, clojure, java = 1, \"great\", 2.1 fmt.Println(flag, label, c, clojure, java)&#125;output:true false 1 great 2.1 回顾swap例子中的main函数使用的操作符:=，对应地，a和b为短声明变量，该操作符可以在类型明确的地方替代var。需要注意的是，:=不能在函数外使用，函数外的每个语句都必须以关键字开始(var, func等)。 Go的基本类型如下： bool string int int8 int16 int32 int64 uint uint8 uint16 uint32 uint64 uinttr byte // unit8的别名 rune // int32的别名，代表一个Unicode码 float32 float64 complex64 complex128 int，uint和uintptr类型在32位的系统上一般是32位，在64位系统上是64位。当需要使用一个整数时，首选int，仅当有特殊情况时才使用定长整数类型或者无符号整数类型。 下面的例子展示了不同类型的变量，与打包导入一样，变量定义打包在一个语法块中。 123456789101112131415161718192021package mainimport ( \"fmt\" \"math/cmplx\")var( Flag bool = false MaxInt unit64 = 1 &lt;&lt; 64 - 1 z complex128 = cmplx.Sqrt(-5 + 12i))func main()&#123; const f = \"%T(%v)\\n\" fmt.Printf(f, Flag, Flag) fmt.Printf(f, MaxInt, MaxInt) fmt.Printf(f, z, z)&#125;output:bool(false)uint64(18446744073709551615)complex128((2+3i)) 表达式T(v)将值v转换为类型T，比如下列的数值转换： 12345678var i int = 42var f float64 = float64(i)var u uint = uint(f)or simplei := 42f := float64(i)u := uint(f) 与C语言不同的是，Go在不同类型的变量之间赋值时需要进行显示转换。比如下面的例子中，未将x*x+y*y显示转换成float64而报错。 1234567891011121314package mainimport ( \"fmt\" \"math\")func main() &#123; var x, y int = 3, 4 var f float64 = math.Sqrt((x*x + y*y)) var z uint = uint(f) fmt.Println(x, y, z)&#125;output:tmp/sandbox259072833/main.go:10: cannot use x * x + y * y (type int) as type float64 in argument to math.Sqrt 定义一个变量却不显示指定其类型时(使用:=或者var =表达式语法)，变量的类型由右侧的值推到得出。当右侧的数值定义了类型时，新变量的类型与其相同，如下： 12var i intj := i // j 也是一个 int 但是当右侧的数值包含了未指明类型的数字常量时，新变量可能是int，float64或者complex128。这取决于常量的精度： 123i := 42 // intf := 3.142 // float64g := 0.867 + 0.5i // complex128 在Go中，常量的定义使用const关键字，常量可以是字符，字符串，布尔或者数字类型的值，但是不可以用:=语法定义。 12345678910111213141516package mainimport \"fmt\"const Pi = 3.14func main()&#123; const World = \"世界\" fmt.Println(\"Hello\", World) fmt.Println(\"Happy\", Pi, \"Day\") const Truth = true fmt.Println(\"Go rules?\", Truth)&#125;output:Hello 世界Happy 3.14 DayGo rules? true 数值常量一般是高精度的值。一个未指定类型的常量由上下文来决定其类型。 12345678910111213141516171819202122package main import \"fmt\" const( Big = 1 &lt;&lt; 100 Small = Big &gt;&gt; 99 ) func needInt(x int)int&#123; return x * 10 + 1 &#125; func needFloat(x float64)float64&#123; return x * 0.1 &#125; func main()&#123; fmt.Println(needInt(Small)) fmt.Println(needFloat(Small)) fmt.Println(needFloat(Big)) &#125; output: 21 0.2 1.2676506002282295e+29 流程控制语句 Go只有一种循环结构for。基本for循环包含三个由分号分开的组成部分： 初始化语句：在第一次循环执行前被执行 循环条件表达式：每轮迭代开始前被求值 后置语句：每轮迭代后执行 初始化语句一般是一个短变量声明，该变量仅在整个循环语句中可见。如果表达式的值变为false，迭代终止。与C，Java等语言不同的是，for语句的三个组成部分并不需要用括号括起来，但循环体必须用{ }括起来。 123456789101112package mainimport \"fmt\"func main() &#123; sum := 0 for i := 0; i &lt; 10; i++ &#123; sum += i &#125; fmt.Println(sum)&#125;output:45 此外，循环的初始化语句与后置语句均是可选的，比如上面的例子可以改写成： 1234sum := 1 for ; sum &lt; 1000; &#123; sum += sum&#125; 基于以上，可以省略分号，则功能类似于其他语言中的while。 1234sum := 1for sum &lt; 1000 &#123; sum += sum&#125; 若省略了循环条件，循环不会结束，因此可用更简洁的方式表达死循环。 12for&#123;&#125; 与for一样，Go的if语句也不要求用( )将条件括起来，但是还是需要用{ }将语句块括起来。 12345678910111213141516package mainimport( \"fmt\" \"math\")func sqrt(x float64)string&#123; if x &lt; 0 &#123; return sqrt(-x)+ \"i\" &#125; return fmt.Sprint(math.Sqrt(x))&#125;func main()&#123; fmt.Println(sqrt(2), sqrt(-4))&#125;output: if语句可以在条件之前执行一个简单语句，该语句定义的变量的作用域仅在if范围之内，比如下面的例子，若把return lim改成return v便会报错。 1234567891011121314151617181920package mainimport ( \"fmt\" \"math\")func pow(x, n , lim float64)float64&#123; if v := math.Pow(x, n); v &lt; lim &#123; return v &#125; return lim&#125;func main()&#123; fmt.Println( pow(3, 2, 10), pow(3, 3, 20) )&#125;output:9 20 在if的便捷语句定义的变量同样可以在任何对应的else块中使用。 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"math\")func pow(x, n, lim float64) float64 &#123; if v := math.Pow(x, n); v &lt; lim &#123; return v &#125; else &#123; fmt.Printf(\"%g &gt;= %g\\n\", v, lim) &#125; // 这里开始就不能使用 v 了 return lim&#125;func main() &#123; fmt.Println( pow(3, 2, 10), pow(3, 3, 20), )&#125;output:27 &gt;= 20 //需要注意的是，两个pow调用都在main调用fmt.Println前执行完毕了。9 20 练习：循环和函数。 作为练习函数和循环的简单途径，用牛顿法实现开方函数。 在这个例子中，牛顿法是通过一个初始点z，然后然后重复以下过程求Sqrt(x)的近似值： $z = z - \\frac{z^2-x}{2z}$ 为了做到这个，只需要重复计算10次。并且观察不同的值(1,2,3,…)是如何逐步逼近结果的。然后，修改循环条件，使得当值停止改变(或者改变非常小)的时候退出循环。观察迭代次数是否变化。结果与math.Sqrt接近吗？ 指定迭代次数版本： 1234567891011121314151617package mainimport ( \"fmt\")func Sqrt(x float64) float64 &#123; z:=float64(1) for i:=1;i&lt;10;i++ &#123; z = z - (z*z-x)/(2*z) &#125; return z&#125;func main() &#123; fmt.Println(Sqrt(2))&#125;output:1.4142135623730951 无限逼近版本： 123456789101112func Sqrt(x float64) float64 &#123; z:=float64(1) y:= z - (z*z-x)/(2*z) for math.Abs(y-z)&gt; 1e-10 &#123; z = y ; y = z - (z*z-x)/(2*z) &#125; return y&#125;output:1.4142135623730951 switch分支语句，除非以fallthrough语句结束，否则分支会自动终止。 123456789101112131415161718192021package mainimport ( \"fmt\" \"runtime\")func main() &#123; fmt.Print(\"Go runs on \") switch os := runtime.GOOS; os &#123; case \"darwin\": fmt.Println(\"OS X.\") case \"linux\": fmt.Println(\"Linux.\") default: // freebsd, openbsd, // plan9, windows... fmt.Printf(\"%s.\", os) &#125;&#125;output:Go runs on nacl. switch的条件从上到下的执行，当匹配成功的时候停止。例如， 1234switch i &#123; case 0 : case f() :&#125; 当i==0时不会调用f。 1234567891011121314151617181920212223package mainimport ( \"fmt\" \"time\")func main() &#123; fmt.Println(\"When's Saturday?\") today := time.Now().Weekday()//计算当前的星期几 switch time.Saturday &#123; case today + 0: fmt.Println(\"Today.\") case today + 1: fmt.Println(\"Tomorrow.\") case today + 2: fmt.Println(\"In two days.\") default: fmt.Println(\"Too far away.\") &#125;&#125;output:When's Saturday?Too far away. 没有条件的switch与switch true一样。这一构造使得可以用更清晰的形式来编写长的it-then-elese链。 12345678910111213141516171819package mainimport( \"fmt\" \"math\")func main()&#123; t := time.Now() switch &#123; case t.Hour() &lt; 12: fmt.Println(\"Good Morning!\") case t.Hour &lt; 17 : fmt.Println(\"Good Afternoon!\") default: fmt.Println(\"Good Evening!\") &#125;&#125;output:Good Morning! defer语句会延迟函数的执行，直到上层函数返回，延迟调用的参数会立刻生成，但是在上层函数返回前，函数都不会调用。 12345678910package mainimport \"fmt\"func main() &#123; defer fmt.Println(\"world\") fmt.Println(\"hello\")&#125;output:helloworld 延迟的函数调用被压入一个栈中，当函数返回时，会按照先进后出的顺序调用被延迟的函数。 1234567891011121314151617181920212223package mainimport \"fmt\"func main()&#123; fmt.Println(\"counting\") for i := 0 ; i &lt; 10 ; i++ &#123; defer fmt.Println(i) &#125; fmt.Println(\"done\")&#125;output:countingdone9876543210 复杂类型 Go使用pointer来保存变量的内存地址，即指针。类型* T是指向类型T的值的指针，默认值为nil。 1var p *int ＆符号会生成一个指向其作用对象的指针。 12i := 42p = &amp;i *符号表示指针指向的底层的值 12fmt.Println(*p) // 通过指针p读取i*p = 21 //通过指针p设置i 以上就是常说的间接引用与或非直接引用。与C不同的是，Go没有指针运算。 12345678910111213141516package mainimport \"fmt\"func main() &#123; i, j := 42, 2701 p := &amp;i fmt.Println(*p)//通过指针读取i的值 *p = 21 //通过指针修改i的值 p = &amp;j //修改指针的指向 *p = *p / 37 // 修改j的值 fmt.Println(j)&#125;output:422173 结构体struct是一个字段的集合，如下： 12345678910111213package mainimport \"fmt\"//定义一个坐标的结构体type Vertex struct&#123; X int Y int&#125;func main()&#123; fmt.Println(Vertex&#123;1,2&#125;)&#125;output:&#123;1 2&#125; 结构体字段使用点号进行访问。 123456v := Vertex&#123;1, 2&#125;v.X = 4fmt.Println(v.X)output:4 结构体字段也可以通过指针来访问，通过指针间接的访问是透明的。 1234567v := Vertex&#123;1, 2&#125;p := &amp;vp.X = 1e9fmt.Println(v)output:&#123;1000000000 2&#125; 结构体文法表示通过结构体字段的值作为列表来分配一个结构体。语法可以只列出部分字段。字段名的顺序无关。 12345678910var( v1 = Vertex&#123;1, 2&#125; v2 = Vertex&#123;X : 1&#125; //Y : 0 被省略 v3 = Vertex&#123; &#125; //X : 0 和 Y : 0 p = &amp;Vertex&#123;1, 2&#125; //类型为*Vertex)fmt.Println(v1, p ,v2, v3)output:&#123;1 2&#125; &amp;&#123;1 2&#125; &#123;1 0&#125; &#123;0 0&#125; 数组是一个有n个类型为T的值的集合，记为[n]T。表达式var a [10]int 定义变量a是一个有十个整数的数组。数组的长度是类型的一部分，因此数组一旦定义不能改变大小。 12345678910111213package mainimport \"fmt\"func main() &#123; var a [2]string a[0] = \"Hello\" a[1] = \"World\" fmt.Println(a[0],a[1]) fmt.Prinlnt(a)&#125;output:Hello World[Hello World] [n]T指向一个序列的值，并包含长度信息。数组[]T是一个元素类型为T的slice。len(s)返回slice s的长度。slice的零值为nil，其长度和容量均为0。 123456789101112131415161718package mainimport \"fmt\"func main()&#123; s := []int&#123;2, 3, 5, 7 ,11, 13&#125; fmt.Println(\"s == \", s) for i := 0 ; i &lt; len(s); i++ &#123; fmt.Println(\"s[%d] == %d\\n\", i, s[i]) &#125;&#125;output:s == [2 3 5 7 11 13]s[0] == 2s[1] == 3s[2] == 5s[3] == 7s[4] == 11s[5] == 13 slice可以包含任意类型的元素，包括另外一个slice。 12345678910111213141516171819202122232425262728package mainimport ( \"fmt\" \"strings\")func printBoard(s [][]string)&#123; for i := 0 ; i &lt; len(s) ; i++ &#123; fmt.Printf(\"%s\\n\", strings.Join(s[i], \" \")) &#125;&#125;func main()&#123; game := [][]string&#123; []string&#123;\"_\", \"_\". \"_\"&#125;, []string&#123;\"_\", \"_\". \"_\"&#125;, []string&#123;\"_\", \"_\". \"_\"&#125;, &#125; game[0][0] = \"X\" game[2][2] = \"O\" game[2][0] = \"X\" game[1][0] = \"O\" game[0][2] = \"X\" printBoard(game)&#125;output:X _ XO _ _X _ O slice可以重新切片，创建一个新的slice值指向相同的数组。表达式s[lo:hi]表示从lo到hi-1的slice元素，含前端，不包含后端，因此，s[lo:lo]是空，s[lo:lo+1]有一个元素。 1234567891011121314151617package mainimport \"fmt\"func main()&#123; s := []int&#123;2, 3, 5, 7, 11, 13&#125; fmt.Println(\"s ==\", s) fmt.Println(\"s[1:4] == \", s[1 : 4]) //省略前端，表示从0开始 fmt.Println(\"s[:3] ==\", s[:3]) //省略后端，表示从当前位置开始，到len(s)结束 fmt.Println(\"s[4:] ==\", s[4:])&#125;output:s == [2 3 5 7 11 13]s[1:4] == [3 5 7]s[:3] == [2 3 5]s[4:] == [11 13] 可以使用函数make创建slice。这会分配一个全为零值的数组，并返回一个slice指向这个数组： 1a := make([]int, 5) 为了指定容量，可以传递第三个参数： 12345678910111213141516171819202122package mainimport \"fmt\"func printSlice(s string, x []int) &#123; fmt.Printf(\"%s len=%d cap=%d %v\\n\", s, len(x), cap(x), x)&#125;func main()&#123; a := make([]int, 5) printSlice(\"a\", a) b := make([]int, 0, 5) printSlice(\"b\", b) c := b[:2] printSlice(\"c\", c) d := c[2:5] printSlice(\"d\", d)｝ output:a len=5 cap=5 [0 0 0 0 0]b len=0 cap=5 []c len=2 cap=5 [0 0]d len=3 cap=3 [0 0 0] 向slice的末尾添加元素是一种常见的操作，使用函数append(s []T, vs ...T)[]T，第一个参数s是一个类型为T的slice，其余类型为T的值会附加到该slice的末尾，返回一个包含原slice所有元素加上新添加的元素的slice。如果s的底层数组太小，而不能容纳所有值时，会分配一个更大的数组，返回的slice会指向这个新分配的数组。 12345678910111213141516171819202122package mainimport \"fmt\"func printSlice(s string, x []int) &#123; fmt.Printf(\"%s len=%d cap=%d %v\\n\", s, len(x), cap(x), x)&#125;func main()&#123; var a []int printSlice(\"a\", a) a = append(a, 0) printSlice(\"a\", a) a = append(a, 1) printSlice(\"a\", a) a = append(a, 2, 3, 4) printSlice(\"a\", a)&#125;output:a len=0 cap=0 []a len=1 cap=2 [0]a len=2 cap=2 [0 1]a len=5 cap=8 [0 1 2 3 4] for循环的range格式可以对slice或者map进行迭代循环。当使用for循环遍历一个slice时，每次迭代range将返回两个值，一个是当前下标，一个是该下标所对应元素的一个拷贝。 123456789101112131415161718package mainimport \"fmt\"var pow = []int&#123;1, 2, 4, 8, 16 ,32, 64, 128&#125;func main()&#123; for i, v := range pow &#123; fmt.Printf(\"2**%d = %d\\n\", i, v) &#125;&#125;output:2**0 = 12**1 = 22**2 = 42**3 = 82**4 = 162**5 = 322**6 = 642**7 = 128 练习：slice 实现Pic。返回一个长度为dy的slice，其中每个元素是一个长度为dx且元素类型为8位无符号整数的slice。在运行程序时，它会将每个整数作为对应像素的灰度值，并显示这个slice所对应的图像。 计算每个像素的灰度值的方法由你决定；几个有意思的选择包括 (x+y)/2、x*y 和 x^y 。 123456789101112131415package mainimport \"golang.org/x/tour/pic\"func Pic(dx, dy int) [][]uint8 &#123; result := make([][]uint8, dy) for i:= 0 ; i &lt; dy ; i++ &#123; result[i] = make([]uint8, dx) for j := 0 ;j &lt; dx ; j++ &#123; result[i][j] = uint8(i*j) &#125; &#125; return result&#125;func main() &#123; pic.Show(Pic)&#125; map映射键到值，使用make函数来创建，值为nil的map是空的，并且不能对其赋值。 12345678910111213141516package mainimport \"fmt\"type Vertex struct&#123; Lat, Long float64&#125;//经纬度结构体var m map[string]Vertex //string到struct的映射func main()&#123; m = make(map[string]Vertex) m[\"Bell Labs\"] = Vertex&#123; 40.68433, -74.39967, &#125; fmt.Println(m[\"Bell Labs\"])&#125;output:&#123;40.68433 -74.39967&#125; map的文法与struct类似，但是必须要有键名。 12345678910111213141516171819package mainimport \"fmt\"type Vertex struct&#123; Lat, Long float64&#125;//经纬度结构体var m = map[string]Vertex&#123; \"Bell Labs\" : Vertex&#123; 40.68433, -74.39967,//逗号不可少 &#125;, \"Google\" : Vertex&#123; 37.42202, -122.08408,//逗号不可少 &#125;,&#125;func main()&#123; fmt.Println(m)&#125;output:map[Bell Labs:&#123;40.68433 -74.39967&#125; Google:&#123;37.42202 -122.08408&#125;] 顶级类型只是一个类型名，可以在文法的元素中省略它。 123456789101112131415package mainimport \"fmt\"type Vertex struct&#123; Lat, Long float64&#125;//经纬度结构体var m = map[string]Vertex&#123; \"Bell Labs\" : &#123;40.68433, -74.39967&#125;, \"Google\": &#123;37.42202, -122.08408&#125;,&#125;func main()&#123; fmt.Println(m)&#125;output:map[Bell Labs:&#123;40.68433 -74.39967&#125; Google:&#123;37.42202 -122.08408&#125;] 在map中插入，删除元素，检测元素是否存在。当从map中读取某个不存在的键时，结果是map的元素类型的零值。 12345678910111213141516171819package mainimport \"fmt\"func main() &#123; m := make(map[string]int) m[\"Answer\"] = 42 fmt.Println(\"The value:\", m[\"Answer\"]) m[\"Answer\"] = 48 fmt.Println(\"The value:\", m[\"Answer\"]) delete(m, \"Answer\") fmt.Println(\"The value:\", m[\"Answer\"]) v, ok := m[\"Answer\"] fmt.Println(\"The value:\", v, \"Present?\", ok)&#125;output:The value: 42The value: 48The value: 0The value: 0 Present? false 练习：map 实现WordCount。它应当返回一个含有 s 中每个 “词” 个数的 map。函数 wc.Test 针对这个函数执行一个测试用例，并输出成功还是失败。 123456789101112131415161718192021222324252627282930package mainimport ( \"golang.org/x/tour/wc\" \"strings\")func WordCount(s string) map[string]int &#123; result := make(map[string]int) words := strings.Fields(s) for _, word := range words &#123; result[word]++ &#125; return result&#125;func main() &#123; wc.Test(WordCount)&#125;output:PASS f(\"I am learning Go!\") = map[string]int&#123;\"I\":1, \"am\":1, \"learning\":1, \"Go!\":1&#125;PASS f(\"The quick brown fox jumped over the lazy dog.\") = map[string]int&#123;\"dog.\":1, \"the\":1, \"quick\":1, \"brown\":1, \"fox\":1, \"jumped\":1, \"over\":1, \"lazy\":1, \"The\":1&#125;PASS f(\"I ate a donut. Then I ate another donut.\") = map[string]int&#123;\"donut.\":2, \"Then\":1, \"another\":1, \"I\":2, \"ate\":2, \"a\":1&#125;PASS f(\"A man a plan a canal panama.\") = map[string]int&#123;\"canal\":1, \"panama.\":1, \"A\":1, \"man\":1, \"a\":2, \"plan\":1 函数也是值。可以像其他值一样传递，比如函数值可以作为函数的参数或者返回值。 1234567891011121314151617181920212223package mainimport ( \"fmt\" \"math\")//函数作为参数func compute(fn func(float64, float64) float64) float64 &#123; return fn(3, 4)&#125;func main() &#123; //函数作为返回值 hypot := func(x, y float64) float64 &#123; return math.Sqrt(x*x + y*y) &#125; fmt.Println(hypot(5, 12))//求平方和 fmt.Println(compute(hypot))//求平方和 fmt.Println(compute(math.Pow))//求3的4次方&#125;output:13581 在Go中，函数可以是一个闭包，闭包是一个函数值，它引用函数体之外的变量。这个函数可以对该引用的变量进行访问和赋值，换句话说，这个函数被绑定在这个变量上。例如下例中，函数adder返回一个闭包，每个返回的闭包都被绑定到其各自的sum变量上。 12345678910111213141516171819202122232425262728293031package mainimport \"fmt\"//返回值为函数func adder() func(int)int&#123; sum := 0 return func(x int)&#123; sum += x return sum &#125;&#125;func main()&#123; pos, neg := adder(), adder() for i := 0 ; i &lt; 10 ; i++ &#123; fmt.Println( pos(i), neg(-2*i), ) &#125;&#125;output:0 01 -23 -66 -1210 -2015 -3021 -4228 -5636 -7245 -90 练习：斐波那契闭包 实现一个 fibonacci 函数，返回一个函数（一个闭包）可以返回连续的斐波纳契数。 12345678910111213141516171819202122232425262728293031package mainimport \"fmt\"// fibonacci 函数会返回一个返回 int 的函数。func fibonacci() func() int &#123; num1 := 0 num2 := 1 return func() int &#123; sum := num1 + num2 num1 = num2 num2 = sum return sum &#125;&#125;func main() &#123; f := fibonacci() for i := 0; i &lt; 10; i++ &#123; fmt.Println(f()) &#125;&#125;output:0112358132134 方法和接口方法 Go中没有类的概念，但是仍然可以在结构体类型上定义方法。方法接收者位于func关键字和方法名之间的参数中。 123456789101112131415161718package mainimport ( \"fmt\" \"math\")type Vertex struct&#123; X, Y float64&#125;func (v *Vertex) Abs() float64&#123; return math.Sqrt(v.X * v.X + v.Y * v.Y)&#125;func main()&#123; v := &amp;Vertex&#123;3, 4&#125; fmt.Println(v.Abs())&#125;output:5 此外，可以对包中的任意类型定义任何方法，而不仅仅是结构体类型。但是不能对来自其他包的的类型或者基础类型定义方法。 12345678910111213141516171819package mainimport ( \"fmt\" \"math\")type MyFloat float64func (f MyFloat) Abs() float64&#123; if f &lt; 0 &#123; return float64(-f) &#125; return float64(f)&#125;func main()&#123; f:= MyFloat(-math.Sqrt(2)) fmt.Println(f.Abs())&#125;output:1.4142135623730951 方法可以与命名类型或者命名类型的指针关联。以上的两个例子中，一个是在*Vertex指针类型上，一个是在MyFloat值类型上。有两个原因需要使用指针接收者：① 避免在每个方法调用中拷贝值(如果值类型是大的结构体的话更有效率)；② 方法可以修改接收者指向的值。在下面的例子中，如果在Scale方法中使用Vertex代替* Vertex作为接收者。当v是Vertex的时候Scale方法不会有任何作用，其看到的只是Vertex的副本，而无法修改原始值。 12345678910111213141516171819202122232425package mainimport ( \"fmt\" \"math\")type Vertex struct &#123; X, Y float64&#125;func (v *Vertex) Scale(f float64) &#123; v.X = v.X * f v.Y = v.Y * f&#125;func (v *Vertex) Abs() float64 &#123; return math.Sqrt(v.X*v.X + v.Y*v.Y)&#125;func main() &#123; v := &amp;Vertex&#123;3, 4&#125; fmt.Printf(\"Before scaling: %+v, Abs: %v\\n\", v, v.Abs()) v.Scale(5) fmt.Printf(\"After scaling: %+v, Abs: %v\\n\", v, v.Abs())&#125;output:Before scaling: &amp;&#123;X:3 Y:4&#125;, Abs: 5After scaling: &amp;&#123;X:15 Y:20&#125;, Abs: 25 接口 接口类型是一组方法定义的组合，其类型的值可以存放实现这些方法的任何值。下面的示例中，接口Abser中声明了一个方法Abs，而后给出Abs的两种定义实现，一个是针对*Vertex指针类型，一个是针对MyFloat值类型。 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( \"fmt\" \"math\")type Abser interface &#123; Abs() float64&#125;type MyFloat float64func (f MyFloat) Abs() float64 &#123; if f &lt; 0 &#123; return float64(-f) &#125; return float64(f)&#125;type Vertex struct &#123; X, Y float64&#125;func (v *Vertex) Abs() float64 &#123; return math.Sqrt(v.X*v.X + v.Y*v.Y)&#125;func main() &#123; var a Abser f := MyFloat(-math.Sqrt2) a = f // a MyFloat 实现了 Abser fmt.Println(a.Abs()) v := Vertex&#123;3, 4&#125; a = &amp;v // a *Vertex 实现了 Abser fmt.Println(a.Abs()) // 下面一行，v 是一个 Vertex（而不是 *Vertex） // 所以没有实现 Abser。 //a = v&#125;output:1.41421356237309515 类型通过实现那些方法实现接口，没有显示声明的必要；所以就没有关键字implements。隐式接口解耦了实现接口的包和定义接口的包：互补依赖。因此，也就无需在每一个实现上新增新的接口名称，这样同时也鼓励了明确的接口定义。 1234567891011121314151617181920212223package mainimport ( \"fmt\" \"os\")type Reader interface&#123; Read(b []byte)(n int, err error)&#125;type Writer interface&#123; Write(b []byte)(n int, err error)&#125;type ReaderWriter interface&#123; Reader Writer&#125;func main()&#123; var w Writer w = os.Stdout // os.Stdout 实现了 Writer fmt.Fprintf(w, \"hello, writer\\n\")&#125;output:hello, writer 一个普遍存在的接口是fmt包中定义的Stringer。 123type Stringer interface&#123; String() string&#125; Stringer是一个可以用字符串描述自己的类型。fmt包(还有许多其他包)使用这个来进行输出。 1234567891011121314151617package mainimport \"fmt\"type Person struct&#123; Name string Age int&#125;func (p Person) String() string&#123; return fmt.Sprintf(\"%v (%v years\", p.Name, p.Age)&#125;func main()&#123; a := Person&#123;\"Xiaotong Wang\", 23&#125; z := Person&#123;\"Feng Zhu\", 24&#125; fmt.Println(a, z)&#125;output:Xiaotong Wang (23 years) Feng Zhu (24 years) 练习：Stringers 让IPAddr类型实现fmt.Stringer以便用点分格式输出地址。 例如，IPAddr{1, 2, 3, 4}应当输出”1.2.3.4”。 1234567891011121314151617181920package mainimport \"fmt\"type IPAddr [4]byte// TODO: Add a \"String() string\" method to IPAddr.func (ip IPAddr)String() string&#123; return fmt.Sprintf(\"%v.%v.%v.%v\",ip[0],ip[1],ip[2],ip[3])&#125;func main() &#123; addrs := map[string]IPAddr&#123; \"loopback\": &#123;127, 0, 0, 1&#125;, \"googleDNS\": &#123;8, 8, 8, 8&#125;, &#125; for n, a := range addrs &#123; fmt.Printf(\"%v: %v\\n\", n, a) &#125;&#125;output:loopback: 127.0.0.1googleDNS: 8.8.8.8 错误 在Go中程序使用 error值来表示错误状态。与 fmt.Stringer 类似， error类型也是一个内建接口： 123type error interface&#123; Error() string&#125; 通常，函数会返回一个error值，调用它的代码应当判断这个错误是否等于nil，来进行错误处理。 123456i, err := strconv.Atoi(\"42\")if err != nil&#123; fmt.Printf(\"couldn't convert number: %v\\n\", err) return&#125;fmt.Println(\"Converted integer:\", i) error 为 nil 时表示成功；非 nil 的 error 表示错误。 123456789101112131415161718192021222324252627package mainimport ( \"fmt\" \"time\")type MyError struct&#123; When time.Time What string&#125;func (e MyError) Error() string&#123; return fmt.Sprintf(\"at %v, %s\", e.When, e.What)&#125;func run() error&#123; return MyError&#123; time.Now(), \"it didn't work\". &#125;&#125;func main()&#123; if err:= run;err != nil&#123; fmt.Println(err) &#125;&#125;output:at 2009-11-10 23:00:00 +0000 UTC, it didn't work 练习：错误 从先前的练习中复制 Sqrt 函数，并修改使其返回 error值。 由于不支持复数，当 Sqrt 接收到一个负数时，应当返回一个非 nil 的错误值。 创建一个新类型 12&gt; type ErrNegativeSqrt float64&gt; 为其实现 12&gt; func (e ErrNegativeSqrt) Error() string&gt; 使其成为一个 error， 该方法就可以让 ErrNegativeSqrt(-2).Error() 返回 &quot;cannot Sqrt negative number: -2&quot;。注意： 在 Error 方法内调用 fmt.Sprint(e) 将会让程序陷入死循环。可以通过先转换 e 来避免这个问题：fmt.Sprint(float64(e))。请思考这是为什么呢？修改 Sqrt 函数，使其接受一个负数时，返回 ErrNegativeSqrt 值。 1234567891011121314151617181920212223242526package mainimport ( \"fmt\")type ErrNegativeSqrt float64func (e ErrNegativeSqrt)Error() string&#123; return fmt.Sprintf(\"cannot Sqrt negtive number: %v\",float64(e)) &#125;func Sqrt(x float64) (float64, error) &#123; if x &lt; 0 &#123; return 0, ErrNegativeSqrt(x) &#125; z := float64(1) for i:= 0 ; i&lt; 10 ;i++ &#123; z = z - (z*z-x)/(2*z) &#125; return z, nil&#125;func main() &#123; fmt.Println(Sqrt(2)) fmt.Println(Sqrt(-2))&#125;output:1.414213562373095 &lt;nil&gt;0 cannot Sqrt negtive number: -2 Readers io 包指定了 io.Reader 接口，它表示从数据流结尾读取。Go标准库包含了这个接口的许多实现，包括文件、网络连接、压缩、加密等。 io.Reader 接口有一个 Read方法： 1func (T) Read(b []byte)(n int, err error) Read 用数据填充指定的字节slice，返回填充的字节数和错误信息。在遇到数据流结尾时，返回io.EOF错误。下面的例子中创建了一个strings.Reader，每次以8字节的速度读取输出。 1234567891011121314151617181920212223242526package mainimport ( \"fmt\" \"io\" \"strings\")func main()&#123; r := strings.NewReader(\"Hello, Reader!\") b := make([]byte, 8) for&#123; n, err := r.Read(b) fmt.Printf(\"n = %v err = %v b = %v\\n\",n, err, b) fmt.Printf(\"b[:n] = %q\\n\", b[:n]) if err == io.EOF&#123; break &#125; &#125;&#125;output:n = 8 err = &lt;nil&gt; b = [72 101 108 108 111 44 32 82]b[:n] = \"Hello, R\"n = 6 err = &lt;nil&gt; b = [101 97 100 101 114 33 32 82]b[:n] = \"eader!\"n = 0 err = EOF b = [101 97 100 101 114 33 32 82]b[:n] = \"\" 练习：Reader 实现一个Reader类型，它不断生成ASCII字符‘A’的流。 1234567891011121314package mainimport \"golang.org/x/tour/reader\"type MyReader struct&#123;&#125;// TODO: Add a Read([]byte) (int, error) method to MyReader.func (r MyReader)Read(b []byte)(n int, err error)&#123; b[0] = 'A' return 1, nil&#125;func main() &#123; reader.Validate(MyReader&#123;&#125;)&#125;output:OK! 练习：rot13Reader 一个常见的模式是 io.Reader包裹另一个 io.Reader，然后通过某种形式修改数据流。例如 gzip.NewReader函数接受 io.Reader(压缩的数据流)，并且返回同样实现了 io.Reader的 *gzip.Reader（解压后的数据流）。 编写一个实现了 io.Reader的 rot13Reader，并从一个 io.Reader读取，利用rot13代换密码对数据流进行修改。 已经帮你构造了 rot13Reader类型。通过实现 Read方法使其匹配 io.Reader。 12345678910111213141516171819202122232425262728293031package mainimport ( \"io\" \"os\" \"strings\")type rot13Reader struct &#123; r io.Reader&#125;func (rot rot13Reader) Read(b []byte)(n int, err error)&#123; rot.r.Read(b) length := len(b) for i := 0 ; i&lt; length; i++ &#123; switch &#123; case b[i] &gt;= 'a' &amp;&amp; b[i] &lt; 'n': fallthrough case b[i] &gt;= 'A' &amp;&amp; b[i] &lt; 'N': b[i] = b[i] + 13 case b[i] &gt;= 'n' &amp;&amp; b[i] &lt;= 'z': fallthrough case b[i] &gt;= 'N' &amp;&amp; b[i] &lt;= 'Z': b[i] = b[i] - 13 &#125; &#125; return length, nil&#125;func main() &#123; s := strings.NewReader(\"Lbh penpxrq gur pbqr!\") r := rot13Reader&#123;s&#125; io.Copy(os.Stdout, &amp;r)&#125; Web服务器 包 http通过任何实现了 http.Handler 的值来响应HTTP请求： 1234package httptype Handler interface&#123; ServeHTTP(w ResponserWriter, r *Request)&#125; 在下面例子中，类型 Hello 实现了 http.Handler 。访问 http://localhost:4000/ 会看到来自程序的问候。需要注意的是，这个例子无法在基于web的指南用户界面运行。为了编写web服务器，可能需要安装Go。 1234567891011121314151617package mainimport ( \"fmt\" \"log\" \"net/http\")type Hello struct&#123;&#125;func (h Hello) ServeHTTP(w http.ResponserWirter, r *http.Request)&#123; fmt.Fprint(w, \"Hello!\")&#125;func main()&#123; var h Hello err := http.ListenAndServe(\"localhost:4000\",h) if err != nil&#123; log.Fatal(err) &#125;&#125; 练习：HTTP处理 实现下面的类型，并在其上定义ServeHTTP方法。在web服务器中注册它们来处理指定的路径。 1234567&gt; type String string&gt; type Struct struct&#123;&gt; Greeting string&gt; Punct string&gt; Who string&gt; &#125;&gt; 例如，可以使用如下方式注册处理方法： 123&gt; http.Handle(\"/string\", String(\"I'm a frayed knot.\"))&gt; http.Handle(\"/struct\", &amp;Struct(\"Hello\",\":\",\"Gophers!\"))&gt; 在启动你的http服务器后，你将能够访问： http://localhost:4000/string 和 http://localhost:4000/struct 1234567891011121314151617181920212223package mainimport ( \"log\" \"net/http\" \"fmt\")type String stringtype Struct struct&#123; Greeting string Punct string Who string&#125;func (s String)ServeHTTP(w http.ResponserWirter, r *http.Request)&#123; fmt.Fprint(w, s) &#125;func (s Sttruct)ServeHTTP(w http.ResponserWirter, r *http.Request)&#123; fmt.Fprint(w, s) &#125;func main()&#123; http.Handle(\"/string\", String(\"I'm a frayed knot.\")) http.Handle(\"/struct\", &amp;Struct(\"Hello\",\":\",\"Gophers!\")) log.Fatal(http.ListenAndServe(\"localhost:4000\", nil))&#125; 图片 Package image定义了 Image 接口： 123456package imagetype Image interface&#123; ColorModel() color.model Bounds() Rectangle At(x, y int) color.Color&#125; 注意， Bounds 方法的 Rectangle 返回值其实是一个 image.Rectangle，其定义在 image包中。 color.Color和 color.Model也是接口，但是通常因为直接使用预定义的实现image.RGBA和image.RGBAModel而被忽视了。这些接口和类型由image/color包定义。 1234567891011121314package mainimport ( \"fmt\" \"image\")func main()&#123; m := image.NewRGBA(image.Rect(0, 0, 100, 100)) fmt.Println(m.Bounds()) fmt.Println(m.At(0, 0).RGBA())&#125;output:(0,0)-(100,100)0 0 0 0 练习： 还记得之前编写的图片生成器吗？现在来另外编写一个，不过这次将会返回 image.Image 来代替 slice 的数据。 自定义的 Image 类型，要实现必要的方法，并且调用pic.ShowImage。 Bounds 应当返回一个 image.Rectangle，例如 image.Rect(0, 0, w, h)。 ColorModel 应当返回 color.RGBAModel。 At 应当返回一个颜色；在这个例子里，在最后一个图片生成器的值 v 匹配 color.RGBA{v, v, 255, 255}。 1234567891011121314151617181920212223package mainimport ( \"golang.org/x/tour/pic\" \"image/color\" \"image\")type Image struct&#123; Width int Height int&#125;func (i Image) Bounds() image.Rectangle&#123; return image.Rect(0, 0, i.Width, i.Height)&#125;func (i Image) ColorModel() color.Model&#123; return color.RGBAModel&#125;func (i Image) At(x, y int) color.Color&#123; return color.RGBA&#123;uint8(x), uint8(y), 255, 255&#125;&#125;func main() &#123; m := Image&#123;Width:100, Height:100&#125; pic.ShowImage(m)&#125; 并发goroutine goroutine是由Go运行时环境管理的轻量级线程。 1go f(x, y ,z) 开启一个新的goroutine执行 1f(x, y ,z) f, x, y 和 z是当前goroutine中定义的，但是在新的goroutine中运行f。goroutine在相同的地址空间中运行，因此访问共享内存必须进行同步。sync 提供了这种可能，不过在Go中并不经常用到，因为有其他的办法。 123456789101112131415161718192021222324252627package mainimport ( \"fmt\" \"time\")func say(s string)&#123; for i := 0 ; i &lt; 5 ; i++ &#123; time.Sleep(100 * time.Millisecond) fmt.Println(s) &#125;&#125;func main()&#123; go say(\"world\") say(\"hello\")&#125;output:worldhellohelloworldworldhellohelloworldworldhello channel是有类型的管道，可以用channel操作符 &lt;- 对其发送或者接收值。 12ch &lt;- v // 将v送人channel chv := &lt;-ch //从ch接收，并赋值给v 箭头就是数据流入的方法 和map和slice一样，channel在使用前必须创建： 1ch := make(chan int) 默认情况下，在另一端准备好之前，发送和接收都会阻塞。这使得goroutine可以在没有明确锁或竞态变量的情况下进行同步。 1234567891011121314151617181920package mainimport \"fmt\"func sum(a []int, c chan int)&#123; sum := 0 for _, v := range a &#123; sum += v &#125; c &lt;- sum //将和送入c&#125;func main()&#123; a := []int&#123;7, 2, 8, -9, 4, 0&#125; c := make(chan int) go sum(a[:len(a)/2],c) go sum(a[len(a)/2:],c) x, y := &lt;-c, &lt;-c //从c中获取 fmt.Println(x, y ,x+y)&#125;output:-5 17 12 channel是可以带缓冲的。为make提供第二个参数作为缓冲长度来初始化一个缓冲channel： 1ch := make(chan int, 100) 向带缓冲的channel发送数据的时候，只有在缓冲区满的时候才会阻塞。而当缓冲区为空的时候接收操作会阻塞。而当缓冲区为空时，接收操作会阻塞。 123456789101112package mainimport \"fmt\"func main()&#123; ch := make(chan int, 2) ch &lt;- 1 ch &lt;- 2 fmt.Println(&lt;-ch) fmt.Println(&lt;-ch)&#125;output:12 若修改上面这个例子，使得缓冲区被填满，则会报错： 12345678910111213141516package mainimport \"fmt\"func main()&#123; ch := make(chan int, 2) ch &lt;- 1 ch &lt;- 2 ch &lt;- 1 fmt.Println(&lt;-ch) fmt.Println(&lt;-ch)&#125;output:fatal error: all goroutines are asleep - deadlock!goroutine 1 [chan send]:main.main() /tmp/sandbox402344606/main.go:9 +0x100 发送者可以 close 一个channel来表示再没有值会被发送了。接收者可以通过赋值语句的第二个参数来测试channel是否被关闭：当没有值可以接收并且channel已经被关闭，那么经过 1v, ok := &lt;-ch 之后， ok 会被设置为false。 循环 for i := range c会不断从channel接收值，直到它被关闭。需要注意的是，只有发送者才能关闭channel，而不是接收者。 向一个已经关闭的channel发送数据会引起panic。channel与文件不同，通常情况下不需要关闭它们，只有在需要告诉接收者没有更过的数据的时候才有必要进行关闭，例如中断一个range。 1234567891011121314151617181920212223242526272829303132333435package mainimport \"fmt\"func fibonacci(n int, c chan int)&#123; x, y := 0, 1 for i := 0 ; i &lt; n ; i++ &#123; c -&lt; x x, y = y, x+y &#125; close(c)&#125;func main()&#123; c := make(chan int, 10) go fibonacci(cap(c),c) for i := range c&#123; fmt.Println(i) &#125; c &lt;- 2 // panic&#125;output:0112358132134panic: send on closed channelgoroutine 1 [running]main.main() /tmp/sandbox910931553/main.go:22 +0x160 select语句使得一个goroutine在多个通讯操作上等待。 select会阻塞，直到条件分支中的某个可以继续执行，这时会执行那个条件分支。当多个都准备好的时候，会随机选择一个。 1234567891011121314151617181920212223242526272829303132333435363738package mainimport \"fmt\"func fibonacci(c, quit chan int)&#123; x, y := 0, 1 for&#123; select&#123; case c &lt;- x: x, y = y, x+y case &lt;- quit: fmt.Println(\"quit\") return &#125; &#125;&#125;func main()&#123; c := make(chan int) quit := make(chan int) go func()&#123; for i := 0 ; i&lt; 10; i++&#123; fmt.Println(&lt;-c) &#125; quit &lt;- 0 &#125;() fibonacci(c, quit)&#125;output:0112358132134quit 当select 中的其他条件分支都没有准备好的时候，default分支会被执行。为了非阻塞的发送或者接收，可使用default分支： 123456select&#123; case i := &lt;-c: //使用i default: //从c读取会阻塞&#125; 1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( \"fmt\" \"time\")func main()&#123; tick := time.Tick(100 * time.Millisecond) boom := time.After(500 * time.Millisecond) for&#123; select&#123; case &lt;- tick: fmt.Println(\"tick.\") case &lt;- boom: fmt.Println(\"boom!\") return default: fmt.Println(\" .\") time.Sleep(50 * time.Millisecond) &#125; &#125;&#125;output: . .tick. . .tick. . .tick. . .tick. . .tick. 练习：等价二叉树 可以用多种不同的二叉树的叶子节点存储相同的数列值。用于检查两个二叉树村出了相同的序列的函数在多数语言中都是相当复杂的。这里将使用Go的并发和channel来编写一个简单的解法。这个例子使用了 tree 包，定义了类型： 123456&gt; type Tree struct&#123;&gt; Left *Tree&gt; Value int&gt; Right *Tree&gt; &#125;&gt; 实现Walk函数。 测试Walk函数。函数 tree.New(k) 构造了一个随机结构的二叉树，保存了值 k，2k，3k，…，10k。创建一个新的channel ch 并对其进行步进： 12&gt; go Walk(tree.New(1), ch)&gt; &gt; 然后从channel 中读取并打印10个值。应当是1，2，3 ，…，10。 用 Walk 实现 Same 函数来检测是否 t1 和 t2 存储了相同的值。 测试 Same 函数。 Same(tree.New(1), tree.New(1))应当返回true，而Same(tree.New(1), tree.New(２))应当返回false。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package mainimport ( \"golang.org/x/tour/tree\" \"fmt\")func Walk(t *tree.Tree, ch chan int)&#123; if t == nil&#123; return &#125; Walk(t.Left, ch) ch &lt;- t.Value Walk(t.Right, ch)&#125;func Same(t1, t2 *tree.Tree)bool&#123; ch1 := make(chan int) ch2 := make(chan int) go func()&#123; Walk(t1, ch1) ch1 &lt;- 0 &#125;() go func()&#123; Walk(t2, ch2) ch2 &lt;- 0 &#125;() for&#123; t1 := &lt;- ch1 t2 := &lt;- ch2 if t1 == 0 &amp;&amp; t2 == 0&#123; return true &#125; if t1 == t2 &#123; continue &#125;else&#123; return false &#125; &#125; return true&#125;func main()&#123; ch := make(chan int) go func()&#123; Walk(tree.New(1), ch) ch &lt;- 0 &#125;() for&#123; t := &lt;- ch if t == 0 &#123; break; &#125; fmt.Println(t) &#125; fmt.Println(Same(tree.New(2), tree.New(2)))&#125;output:12345678910true sync.Mutex 我们已经看到 channel 用在各个goroutine间进行通信是非常合适的了。但是如果我们不需要进行通信呢？比如说，如果我们只想保证在每个时刻，只有一个goroutine能访问一个共享的变量从而避免冲突？这里涉及的概念叫做互斥。通常使用互斥锁mutex来提供这个限制。Go标准库中提供了 sync.Mutex类型及其两个方法： 12LockUnlock 我们可以通过在代码前调用 Lock 方法，在代码后调用 Unlock方法来保证一段代码的互斥执行。我们也可以用 12345678910111213141516171819202122232425262728293031323334package mainimport( \"fmt\" \"sync\" \"time\")// SafeCounter的并发使用是安全的type SafeCounter struct&#123; v map[string]int mux sync.Mutex&#125;// Inc增加给定key的计数器的值func(c *SafeCounter) Inc(key string)&#123; c.mux.Lock() c.v[key]++ c.mux.Unlock()&#125;//Value返回给定key的计数器的当前值func (c *SafeCounter) Value(key string)int&#123; c.mux.Lock() defer c.mux.Unlock() return c.v[key]&#125;func main()&#123; c := SafeCounter&#123;v: make(map[string]iint)&#125; for i := 0 ; i &lt; 1000 ; i++&#123; go c.Inc(\"somekey\") &#125; time.Sleep(time.Second) fmt.Println(c.Value(\"somekey\"))&#125;output:1000 练习：Web爬虫 在这个练习中，将会用Go的并发特性来并行执行web爬虫。修改 Crawl函数来并行抓取URLs，并且保证不重复。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package mainimport ( \"fmt\" \"sync\")var crawled = make(map[string]bool)var crawledMutex sync.Mutextype fakeFetcher map[string]*fakeResulttype fakeResult struct &#123; body string urls []string&#125;type Fetcher interface&#123; //Fetch返回URL的body内容，并且将在这个页面上找到的URL放到一个slice中 Fetch(url string)(body string, urls []string, err error)&#125;func (f *fakeFetcher) Fetch(url string) (string, []string, error) &#123; if res, ok := (*f)[url]; ok &#123; return res.body, res.urls, nil &#125; return \"\", nil, fmt.Errorf(\"not found: %s\", url)&#125;//Crawl使用fetcher从某个url开始递归地爬取页面，直到达到最大深度func Crawl(url string, depth int, fetcher Fetcher, out chan string, end chan bool)&#123; //并行抓取URL //不重复抓取页面 if depth &lt;= 0&#123; return &#125; if _, ok := crawled[url];ok&#123; end &lt;- true return &#125; crawledMutex.Lock() crawled[url] = true crawledMutex.Unlock() body, urls, err := fetcher.Fetch(url) if err != nil&#123; fmt.Println(err) return &#125; out &lt;- fmt.Sprintf(\"found: %s %q\\n\", url, body) subEnd := make(chan bool) for _, u := range urls&#123; go Crawl(u, depth-1, fetcher, out, subEnd) &#125; for i := 0 ; i &lt; len(urls); i++&#123; &lt;- subEnd &#125; end &lt;- true&#125;func main()&#123; out := make(chan string) end := make(chan bool) go Crawl(\"http://golang.org/\",4,fetcher,out, end) for&#123; select&#123; case t := &lt;-out: fmt.Print(t) case &lt;- end: return &#125; &#125;&#125;","categories":[{"name":"Go","slug":"Go","permalink":"http://tankcat2.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://tankcat2.com/tags/Go/"},{"name":"MIT6.824","slug":"MIT6-824","permalink":"http://tankcat2.com/tags/MIT6-824/"}]},{"title":"再见我的暴力女王","slug":"evil","date":"2017-02-27T12:11:31.000Z","updated":"2017-05-05T11:27:22.000Z","comments":true,"path":"2017/02/27/evil/","link":"","permalink":"http://tankcat2.com/2017/02/27/evil/","excerpt":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了；","text":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了； 到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她； 没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。 今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。 最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"生化危机","slug":"生化危机","permalink":"http://tankcat2.com/tags/生化危机/"}]},{"title":"Kafka快速入门","slug":"kafka_quickstart","date":"2017-02-27T12:11:31.000Z","updated":"2017-07-20T00:48:16.000Z","comments":true,"path":"2017/02/27/kafka_quickstart/","link":"","permalink":"http://tankcat2.com/2017/02/27/kafka_quickstart/","excerpt":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf","text":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf 下载Kafka 我使用的是kafka_2.10-0.10.2.1版本 12tar -xvzf kafka_2.10-0.10.2.1cd kafka_2.10-0.10.2.1/config 接下来进行参数配置：server.properties 123456789vim server.properties...# 修改broker.id,全局唯一# 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔zookeeper.connect=192.168.115:2181# 设置话题的删除,默认值为falsedelete.topic.enable=true# 设置数据日志路径log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs 启动 Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的： 1zkServer.sh start 接着启动Kafka: 1bin/kafka-server-start.sh config/server.properties 创建topic 使用下面的命令创建名为single_node的topic，副本数为1，分区数为1，命令执行结束后，kafka-logs路径下就会生成一个single_node-0的文件夹。 1bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node 发布与消耗数据 执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。 1234bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_nodewxtzfi love u 执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据： 1234bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginningwxtzfi love u 以上均是单机版的Kafka配置与使用。","categories":[{"name":"Kafka文档","slug":"Kafka文档","permalink":"http://tankcat2.com/categories/Kafka文档/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://tankcat2.com/tags/kafka/"}]},{"title":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","slug":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","date":"2017-01-17T05:11:31.000Z","updated":"2017-05-05T11:28:40.000Z","comments":true,"path":"2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","link":"","permalink":"http://tankcat2.com/2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","excerpt":"并行连接处理的两种基本框架","text":"并行连接处理的两种基本框架 hash-based 基于哈希，如下图所示，分为四个步骤： partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集； distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R{ik}$ 和 $S{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$Rk=\\bigcup{i=1}^{n}R_{ik}$,$Sk=\\bigcup{i=1}^{n}S_{ik}$; build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表； prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果. duplication-based 基于副本，如下图所示，分为三个步骤： duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$Rk=\\bigcup{i=1}^{n}R_i=R$即为全集R； build构建，构建哈希表，与hash-based相似； prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似. PRPD连接算法 PRPD定义：partial redistribution &amp; partition duplication，即将hash-based和duplication-based相结合. 处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S{loc}$是倾斜数据子集，$S{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R{dup}$是与$S{loc}$连接键值相同的数据子集，$R{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S{redis}$进行连接. 存在的问题： global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布； broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加. 本文提出算法PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进. Semijoin-based 连接 semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行不完全连接查询，即查找到匹配的数据行就返回，不再继续查找. semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤： 类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线); 对各个节点中的数据集$S_i$在属性b上做投影操作得到$\\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中； 每个计算节点k收到数据集S的key 子集$\\pib(S{ik})$，和数据集R的子集$Rk=\\bigcup{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)； 各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果. 特点： 由于投影操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以解决数据倾斜； 第2和第3步骤，只传输key和能连接上的元组，因此减轻了网络传输代价. 对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量. Query-based 连接 根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下： 若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value； 返回的value和本地的S数据集做最后的实际连接操作，输出连接结果. 特点： 对于高选择性的连接处理，优势大，减轻网络通信负载； 对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证的序列以便最后的连接处理，因此可能降低处理速度. 折中综合：通过一个计数器来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based. 性能问题本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷. PRPQ连接算法 PRPQ定义：partial redistribution &amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤： R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)； Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)； Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$Rk=\\bigcup{i=1}^{n}R{ik}$建立哈希表，(1). 对接收到的集合$\\bigcup{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\\pib(h{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i； Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\\bigcup$第4部分的连接结果. 特点： 与query-based算法相比 当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性. 继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. 与PRPD算法相比 最主要的区别在于，使用query而不是duplication操作. PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价. 对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载. 算法实现每个节点上skew元组的提取是基于局部倾斜量化，因此引入一个阈值参数，即当一个key出现的次数超过该阈值，则视这个key为skewed. 下面先整理如何处理阈值参数，再整理PRPQ算法的具体实现. 局部数据倾斜有很多方法可实现局部数据倾斜的快速监测，比如采样，扫描等.但是这些与本文的思路无关，所以本文仅仅在每个节点中对key的出现次数进行计数，按照降序排列，并保存到文件中. 在每一次的参数测试中，每个节点预先读取出现次数超过t的key，写入一个ArrayList中，并视它们为skew key. PRPQ具体实现具体算法和前面的四个步骤一一对应，如下： 在每个原节点中，将所有的元组读取到一个ArrayList中，处理数据集R的元组. 首先初始化一个R_c，用于收集分组的元组，R_c的初始化大小为计算节点的数量.接着，各个线程读取ArrayList中的R集元组，根据连接key的哈希值对元组进行分组.最后，将分好组的元组分发到对应的计算节点中(算法中的here表示当前计算节点的id). 根据给定的阈值参数t，对数据集S进行划分，倾斜的key被读入一个hashset，并且所有对应的元组被存储到一个hashmap中，剩余的非倾斜元组存储到一个$S^{’}_c$中.接着对hashmap进行投影操作，将所有的unique keys保存到key_c中.最后将key_c和$S^{’}_c$按照key的哈希值分发到对应的计算节点上. 在计算节点中，对接收到的R集元组建立一个哈希表T’，对数据集$S’$元组进行遍历，并查找哈希表，若有匹配的key，则输出连接结果.同时遍历key集key_c，并查找哈希表，若不存在匹配的key，则返回值为null的value到对应的原节点，否则返回实际key对应的value. 倾斜元组的连接结果可以通过遍历查询返回的value集合，若value为null，则不存在能连接上的S集元组，否则输出最终连接结果. 实验对比数据集的选取：用作基准的数据集模仿决策支援系统下的连接操作.数据集R的cardinality为64M，数据集S的cardinality为1GB.由于数据仓储中数据一般以面向列的形式存储，所以实验中将数据格式设置为的键值对，其中key和value均是8字节整型. 工作负载的选取：设置数据集R和S之间存在外键的关系，保持R的主键的unique，而在S中为对应的外键增加skew.除此之外，若S是统一分布的，它们中的每一个以相同的概率匹配关系R中的元组.对于倾斜的元组，它们的unique key在节点间均匀分布，并且每一个均能与R匹配上.下表给出了数据集S的分布情况. S key distribution Partition Size Zipf skew=0,1,1.4 均匀evenly 512M Linear f(r)=46341-r,23170 排序范围sort-range 1GB,2GB Zipf分布中，skew=0表示统一分布，skew=1表示排名前十的key占据总量14%，skew=1.4表示排名前十的key占据总量68%.线性分布中，使用f(r)来描述key的分布情况，其中f(r)=46341-r表示频率最高的key出现46341次，频率第二的key出现46340次.使用该函数生成的数据集可以看作low-skewed的数据集.f(r)=23170表示所有的key都是均匀分布的，但是重复次数较高.f(r)对应的两个数据集均为1GB的大小，有46341个unique key. R和S在计算节点中的分布情况：R均匀分布在所有的节点上，而S使用均匀和排序范围分布.均与分布保证每个计算节点上skewed元组的数量相同；排序范围分布是先将所有的元组按照键的频率排序，然后等分成大小一样的块，再将块按照次序分配到每个计算节点上.因此每个节点上skewed元组的数量差距可能会比较大. 实验共从运行时间、网络通信、负载均衡、可扩展性四个方面来进行比较.这里只就运行时间稍作整理. 运行时间记录Hash-based算法、PRPD、PRPQ和query-based算法的运行时间，如下图所示.当S是均匀分布(第一组数据skew=0)，Hash、PRPD和PRPQ算法的性能相近，远远优于Query算法；当S是low skewed时，PRPD和PRPQ均比另外两种算法快；当S是high skewed时，Hash算法性能最差，而其余三种性能相近，则可得出结论，其余PRPD、PRPQ和Query可以较好地处理数据倾斜.随着skew程度的增加，Hash算法的执行时间增长剧烈，而Query算法呈现下降趋势.而PRPD和PRPQ算法呈现平稳的下降趋势. 上图展示是选择最佳频率阈值t的性能，原文中关于不同阈值的实验这里不再整理，基本情况是无论t值如何变化以及分区计划如何，PRPQ的运行时间是低于PRPD的.","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://tankcat2.com/categories/论文阅读/"}],"tags":[{"name":"data skew","slug":"data-skew","permalink":"http://tankcat2.com/tags/data-skew/"},{"name":"parallel join","slug":"parallel-join","permalink":"http://tankcat2.com/tags/parallel-join/"}]},{"title":"使用Storm遇到的问题以及解决方案","slug":"stormproblems","date":"2016-12-30T05:45:31.000Z","updated":"2017-01-17T12:32:46.000Z","comments":true,"path":"2016/12/30/stormproblems/","link":"","permalink":"http://tankcat2.com/2016/12/30/stormproblems/","excerpt":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错：","text":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错： 12345678910111213142016-12-30 12:41:17.269 b.s.event [ERROR] Error when processing eventjava.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: File '/home/admin/stormdata/data/supervisor/localstate/1480504905565' does not exist at backtype.storm.utils.LocalState.partialSnapshot(LocalState.java:118) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.utils.LocalState.get(LocalState.java:126) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.local_state$ls_local_assignments.invoke(local_state.clj:83) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:321) ~[storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.applyToHelper(AFn.java:154) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:626) ~[clojure-1.6.0.jar:?] at clojure.core$partial$fn__4228.doInvoke(core.clj:2468) ~[clojure-1.6.0.jar:?] at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.6.0.jar:?] at backtype.storm.event$event_manager$fn__7258.invoke(event.clj:40) [storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?] at java.lang.Thread.run(Thread.java:744) [?:1.7.0_45] 找不到’/home/admin/stormdata/data/supervisor/localstate/1480504905565’这个文件夹，网上找了下原因，给出的答案是stop the server without previously stop the supervisor，就是说可能是由于不正常关机造成状态不一致，具体原因不知，解决方案是删除stormdata/data/supervisor整个目录即可. 在集群环境下日志清理，自己写了一个脚本clear-log.sh，主要是删除apache-storm-XXX下的logs文件里的日志文件，如下： 1234567STORM_HOME=/home/admin/apache-storm-0.10.0 HOSTS_FILE=/home/admin/hosts.txtcat $HOSTS_FILE | while read line do ssh $line \"rm -rf $STORM_HOME/logs/*\" &lt; /dev/null doneecho \"remove log files...done\" 其中STORM_HOME是storm的安装路径，hosts.txt是集群中各个节点的地址，我自己的配置如下： 1234567891011121314admin@10.11.1.53admin@10.11.1.40admin@10.11.1.41admin@10.11.1.42admin@10.11.1.45admin@10.11.1.46admin@10.11.1.51admin@10.11.1.53admin@10.11.1.54admin@10.11.1.55admin@10.11.1.56admin@10.11.1.58admin@10.11.1.60admin@10.11.1.64 编辑完之后执行chmod +x clear-log.sh命令使得该文件获得可执行权限，再执行./clear-log.sh运行该脚本即可.","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"},{"name":"日志","slug":"日志","permalink":"http://tankcat2.com/tags/日志/"},{"name":"环境配置","slug":"环境配置","permalink":"http://tankcat2.com/tags/环境配置/"}]},{"title":"关于苏打绿的题库","slug":"knowledgebase","date":"2016-10-26T08:26:31.000Z","updated":"2016-10-26T10:53:28.000Z","comments":true,"path":"2016/10/26/knowledgebase/","link":"","permalink":"http://tankcat2.com/2016/10/26/knowledgebase/","excerpt":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴","text":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴 龚钰祺：键盘手+中提琴手,1980.12.16，台湾，国立台北艺术大学音乐研究所，中提琴、钢琴、电子琴 刘家凯：电子吉他手，1982.2.5，台湾台北，国立政治大学心理系、国立阳明大学脑科学研究所，吉他 何景扬：木吉他手，1982.4.4，台湾，国立政治大学公共行政研究所，吉他、乌克丽丽 2. 重要时间节点 2001年成立于校园，2003年确立6人阵容，2004年5月，苏打绿正式出道，发行第一张单曲《空气中的视听与幻觉》。 2005年，苏打绿发行了《SodaGreen》。 2006年，苏打绿发行专辑《小宇宙》。 2007年11月，苏打绿发行专辑《无与伦比的美丽》。 2008年5月，苏打绿发行专辑《陪我歌唱》。 韦瓦第计划：2009年5月8日，苏打绿发行了第五张专辑《春·日光》；2009年9月11日，苏打绿发行专辑《夏·狂热》；2013年9月18日，苏打绿发行专辑《秋：故事》，该专辑成为2013年度iTunes Store最受欢迎专辑；2015年9月23日，苏打绿发行专辑《冬·未了》。 2011年 2011年11月11日，苏打绿发行专辑《你在烦恼什么》。 2014年，苏打绿开始了十周年世界巡回演唱会。 3. 唱片公司 2004-2009，林浩哲音乐社； 2009-至今，环球唱片 4. “游乐园鱼丁糸”比赛题第一轮 单选题（以下各题四个选项中,只有一个选项正确）： EP《空气中的视听与幻觉》碟的颜色为：DA、白B、红C、蓝D、墨绿 迄今为止仍未引进内地的苏打绿正式专辑是：AA、无与伦比的美丽 B、小宇宙 C、同名专辑 D、陪我歌唱 青峰创作的第一首歌是：DA、空气中的视听与幻觉 B、降落练习存在孪生基因 C、后悔莫及 D、窥 03年海洋音乐祭时，谁迟到了？CA、家凯 B、阿福 C、小威D、青峰 家凯、林暐哲法国街头赛跑谁赢了？AA、家凯B、林暐哲C、都赢D、无法评判 图片题（见附件）：这张是苏打绿在大陆第一场演唱会（615北京场）的新闻图片，请问方框里面的人是谁？CA、茶水 B、lfxfox C、博博鱼 D、和尚 苏打绿第一任的团长是谁？AA.小威 B.阿福 C.馨仪 D.青峰 苏打绿第一张同名专辑共有几首歌？DA、2 B、3 C、10 D、11 苏打绿一共来过上海几次？BA、1 B、2 C、3 D、4 苏打绿的第三张单曲是：CA、空气中的视听与幻觉 B、飞鱼 C、Believe in music D、迟到千年 苏打绿成军时间：BA、2000 B、2001 C、2004 D、2005 “我想到你离开了以後，我们的城市好寂寞”选自苏打绿的哪首歌曲？BA、无与伦比的美丽 B、雨中的操场 C、相信 D、城市 单曲《飞鱼》中有几首歌？DA、1 B、2 C、3 D、4 苏打绿首次来内地时青峰裤子上的油漆是怎么来的：CA、裤子本身印有的 B、被团员泼的 C、自己粉刷房间墙壁弄的D、不知道 以下苏打绿未上过的台湾综艺节目是：BA、娱乐百分百 B、康熙来了 C、国光帮帮忙 D、大学生了没 第18届金曲奖最佳作曲人奖是因为哪首歌？BA、小宇宙 B、小情歌 C、频率 D、飞鱼 红包场中，谁是王子造型的？DA、青峰 B、家凯 C、阿龚 D、馨仪 小情歌是为了哪个艺人的唱片公司收歌而写？CA、刘若英 B、江美琪 C、徐若瑄 D、杨乃文 《爱人动物》是以下哪部电影的主题曲？BA、情非得已之生存之道 B、Juno C、囧男孩 D、海角七号 「无与伦比的美丽」小巨蛋演唱会是几月几号？CA、2007.11.1 B、2007.11.2 C、2007.11.3 D、2007.11.13 第二轮 单选题： 1~5 BDB(D)CD 6~10 CCCCB 11~15DCCBC 16~2 0ABADD 多选题： 1.BDEF 2.CD 3.BCDE 4.AC 5.ACD 6.BCD 7.ABC 8.ABC 9.ACD 10.DE 第三轮 模拟题1单选题： 王菀之的首张国语创作专辑中多少首歌是由青峰作词？CA、3 B、4 C、5 D、6 615北京演唱会开场歌曲是什么？AA、无与伦比的美丽 B、小宇宙 C、小情歌 D、白日出没的月球 1224广州演唱会最后一首歌曲是：DA、频率 B、小情歌 C、这天 D、陪我歌唱 苏打绿夺得了第几届海洋音乐祭的陪审团大奖？BA、4 B、5 C、6 D、7 家凯是因为想参加什么音乐节而加入苏打绿的？BA、春浪 B、春天呐喊 C、海洋音乐祭 D、简单生活 第一期空气中的视听与幻觉广播是06年几月几号？BA、0906 B、1007 C、1017 D、1104 苏打绿的第一首抒情歌是：AA、 频率 B、无言歌 C、背着你 D、小情歌 以下哪首是政大吉他社社歌？ CA、天天想你 B、我的未来不是梦 C、我呼吸我感觉我存在 D、和天一样高 除了小巨蛋演唱会之外，哪一个影像保存了苏打绿《是我的海》的live演出？DA、好友音乐会 B、周日狂热夜 C、摇滚风城音乐祭 D、 Taiwan Roc 在小巨蛋DVD中，青峰一共talking了多少次（不包括唱歌中以及预报下一首歌）？BA、4次 B、5次 C、6次 D、7次 《Air》里的Bass是谁弹的？DA、家凯 B、馨怡 C、阿福 D、阿龚 《搜包包》中谁全程站在家凯的面前？AA、tirtir B、小威 C、青峰 D、将将 《当代歌坛》第一次出现专写苏打绿的文章是哪一期？CA、342期B、370期C、363期D、389期 创作给别人的歌中，青峰最喜欢哪首？AA、女爵B、多希望你在C、穿墙人D、爱与奇异果 苏打绿中谁一直没有拿到高中毕业证书：DA、家凯 B、阿龚 C、阿福 D、小威 在豆瓣公开现身过的苏打绿团员是：CA、家凯 B、阿龚 C、阿福 D、小威 苏打绿现在的六人编制的第一次演出是在哪里？BA、春天呐喊 B、政大金旋奖 C、西门町 D、海洋音乐祭 多选题： 下面不属于919上海演唱会安可曲目的是：DEA、相信 B、这天 C、女爵 D、是我的海》 E、白日出没的月球 “搁浅”一词出现在以下哪些歌曲中：ABCEA、吵 B、迟到千年 C、蓝眼睛 D、无与伦比的美丽 E、漂浮 青峰曾为以下哪些艺人写歌？ACDEA、左光平 B、尚雯婕 C、张韶涵 D、刘若英 E、王菀之 填空（每题5分，共5题） “各站停靠”演唱会上，作为舞台背景的钟显示的时间是22:15__ 2010年青峰和小威一起庆生时，其他团员送他们的礼物是_小时候照片拼贴出来的相框__ 阿福在自由发挥的歌曲_欢迎光临__的MV中客串演出了一个角色 各站停靠台中场的小礼物是_阿福面纸__ 苏打绿出道时候的第一支广告是_蔡康永推荐的广播广告 模拟题2 单选（以下各题，只有一个正确选项。每题5分，共10题） 苏打绿二度踏上小巨蛋举办“日光狂热”演唱会前，媒体特地准备多样运动器材帮忙训练体力。结果，阿福在哪样器械上竟然输给了馨仪？CA.滚轮 B.握力棒 C.哑铃 D.铁饼 青峰对自己人生过程进行总结的歌是 DA.交响梦 B.融雪之前 C.相信 D.近未来 苏打绿、王菀之、方大同合作的903拉阔演唱会上，他们约定好各自代表的颜色，分别是 DA.黑、白、绿 B.黑、绿、白C.绿、黑、白 D.绿、白、黑 被媒体拍到跟青峰一起看电影，并且对镜头比中指的女友人是 AA.小兔 B.张悬 C.娃娃 D.阿纯 歌词本的字型让“吴青峰写到手心手背都是肉”的专辑是 CA. 苏打绿同名 B.春•日光 C.陪我歌唱 D.无与伦比的美丽 这是2010年5月苏打绿参加利物浦音乐节的返国记者会照片。请问青峰的这件衬衫，还在以下哪场公开表演中穿过？DA.成都热波音乐节 B.北京原创A8颁奖礼 C.西安草莓音乐节 D.西湖音乐节 在《日光》MV中，和大家一起采茶的团员是：AA.阿龚 B.小威 C.馨仪 D.家凯 来源于青峰日记的歌词是:CA.近未来 B.女爵 C.困在 D.早点回家 苏打绿一共参与过几次简单生活节:CA.1次 B.2次 C.3次 D.4次 阿福入伍前的饯别仪式上，第一个给阿福剪头发的团员是:AA.青峰 B.馨仪 C.小威 D.家凯 多选（以下各题有两个或两个以上的正确选项，多选、漏选、错选均不得分。每题5分，共5题） 在“最会睡”系列中，暐哲老师睡觉的地点有：ABCDEA.馨仪家 B.伦敦录音室 C.草地 D.公司地板 E.会议室沙发 The Wall“多希望你在”系列演出的歌单中包含：ABCEA.疼惜我的吻 B.吵 C.我在欧洲打电话给你 D.寂寞拥挤E.再见 “维瓦第计划”的歌词中，没有出现的意象是:BCA.萤火虫 B.羔羊 C.海豚 D.白鸽 E.落叶 以下歌曲中，苏打绿有公开表演多个语言版本的是ACEA.Oh Oh Oh Oh B.飞鱼 C.无眠 D.日光 E.小情歌 游乐园的官方周边包括BCDEA. .海报B.T恤 C.贴纸 D.徽章 E.年历 填空题 《小宇宙》这首歌标题的灵感来自陈黎的《小宇宙》_。 《小情歌》在被徐若瑄拒绝后，又被江美琪拒绝。 2005年9_月3_日发行首张同名专辑《苏打绿》及进行全省签唱会表演。 青峰的小学时期，每天早上跟着跳的早操音乐是 我的未来不是梦。 无美丽电台的封面除了苏打绿六个人外还有谁？ 将将 5.获奖情况 2016 第27届台湾金曲奖 最佳国语专辑奖 冬未了 （获奖） 2016 第27届台湾金曲奖 最佳乐团奖 （获奖） 2016 第27届台湾金曲奖 最佳编曲人奖 痛快的哀艳 （获奖） 2015 第五届阿比鹿音乐奖 最受欢迎唱片 冬未了 （获奖） 2014 第36届十大中文金曲 全年最高销量歌手 秋:故事 （获奖） 2013 第7届无线咪咕汇音乐盛典 年度最受欢迎组合 韦瓦第计划 （获奖） 2012 Hito流行音乐奖 hito乐团 （获奖） 2011 第1届全球流行音乐金榜 年度最佳乐团 （获奖） 2011 第1届全球流行音乐金榜 年度20大金曲 十年一刻 （获奖） 2010 第10届华语音乐传媒大奖 最佳乐队 春·日光、夏/狂热 （获奖） 2010 MusicRadio中国TOP排行榜 年度最受欢迎乐团 （获奖） 2010 MusicRadio中国TOP排行榜 年度最佳乐团 （获奖） 2010 新加坡金曲奖 最佳创作歌手 夏/狂热 （获奖） 2010 新加坡金曲奖 最佳乐团 （获奖） 2010 第21届金曲奖 最佳音乐录像带奖 日光 （获奖） 2010 第21届金曲奖 最佳编曲人 日光 （提名） 2010 第21届金曲奖 最佳乐团 春·日光 （提名） 2010 第21届金曲奖 最佳乐团 夏/狂热 （提名） 2009 第32届香港十大中文金曲颁奖典礼 全国最佳组合 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊组合/乐团 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊金曲20 狂热 （获奖） 2009 第4届A8原创中国音乐盛典 年度最佳原创乐团 （获奖） 2009 MUSIC RADIO中国TOP排行榜 港台年度最佳乐团 （获奖） 2009 全球华语歌曲排行榜 地区杰出歌手奖 春·日光 （获奖） 2009 全球华语歌曲排行榜 年度20大金曲 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳编曲人 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳乐团 春·日光 （获奖） 2009 新城国语力颁奖礼 国语力亚洲乐团 （获奖） 2009 新城国语力颁奖礼 国语力歌曲 日光 （获奖） 2009 新城国语力颁奖礼 国语力乐团 （获奖） 2009 新浪网络盛典 年度最佳乐团 （获奖） 2008 第8届华语音乐传媒大奖 年度国语专辑 无与伦比的美丽 （获奖） 2008 第8届华语音乐传媒大奖 最佳编曲人 白日出没的月球 （获奖） 2008 第8届华语音乐传媒大奖 最佳乐队 无与伦比的美丽 （获奖） 2008 香港新城国语颁奖礼 新城国语歌曲 陪我歌唱 （获奖） 2008 香港新城国语颁奖礼 国语乐团 （获奖） 2008 第9届CCTV/MTV音乐盛典 港台年度最佳组合 （获奖） 2008 新城劲爆颁奖礼 新城全国乐迷投选劲爆突破表现大奖 （获奖） 2008 新加坡金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2008 第19届金曲奖 最佳音乐录影带导演奖 左边 （提名） 2008 第19届金曲奖 最佳年度歌曲 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳编曲人 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2007 新加坡金曲奖 最佳乐团 小宇宙 （获奖） 2007 第44届金马奖 最佳原创电影歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳年度歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳国语专辑 小宇宙 （提名） 2007 第18届金曲奖 最佳乐团 小宇宙 （获奖） 2006 第17届金曲奖 最佳编曲人 Oh Oh Oh Oh （提名） 2004 MTV百万乐团挑战赛 网路最佳人气乐团 （获奖） 2004 第5届海洋音乐祭 评审团大赏 （获奖） 2002 政大第19届金旋奖 乐团组冠军 空气中的视听与幻觉 （获奖） 2001 政大第18届金旋奖 乐团组最佳人气奖 （获奖） 6. 老吴写给别人的歌那英我的幸福刚刚好 林忆莲 寂寞拥挤 张惠妹 掉了，你和我的时光 李玟 想你的夜 莫文蔚 看着，老掉牙 容祖儿 在时间面前 江蕙 你讲的话 刘若英 没道理 陶晶莹 翔 蔡依林 彩色相片，栅栏间隙偷窥你，迷幻 杨丞琳 带我走，少年维特的烦恼，下个转弯是你吗，被自己绑架，一小节休息 张韶涵 最近好吗，刺情，蓝眼睛 王心凌 从未到过的地方 范玮琪 坏了良心 范晓萱 开机关机 杨乃文 女爵 许茹芸 爱人动物，飞行时光，I will be with you，最难的是相遇，现在该怎么好，我留下的一个生活 蔡健雅 极光，费洛蒙 周笔畅 别忘了 谢安琪 再见 张悬 两者 徐佳莹 乐园 魏如萱 被雨伤透，困在，开机关机 尚雯婕 什么？什么！ 吉克隽逸 我唱故我在 袁泉 等 吴映洁 一直 刘容嘉 没有人爱 潘玮仪 不同 路嘉欣 穿墙人，当我继续唱 王菀之 学会，迷湖，冬梦，是爱，爱与奇异果 旅行团 Bye Bye VOX玩声乐团 让我做你的家，朱古力 TFBOYS 小精灵 谭咏麟 超越，糖衣陷阱，蓝侬梦，魔毯，算爱，未知 张信哲 柔软 陈奕迅 放弃治疗，这样的一个麻烦，谋情害命 林俊杰 独舞，爱的鼓励，裂缝中的阳光，不存在的情人 萧敬腾 以爱之名，多希望你在 杨宗纬 想对你说 萧煌奇 下个街角 信 你存在，我记得，给自己的信 左光平 心里有鬼","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}]},{"title":"遇见小公举","slug":"jielun","date":"2016-10-23T13:21:31.000Z","updated":"2016-10-23T13:29:02.000Z","comments":true,"path":"2016/10/23/jielun/","link":"","permalink":"http://tankcat2.com/2016/10/23/jielun/","excerpt":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。","text":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。我记不得听周杰伦的第一首歌是什么。小学六年级的时候我妈同事的女儿买了一个MP3,需要分外安电池的那种，里面有两首杰伦的歌，一首发如雪，一首夜曲。我借过来听，很喜欢这两首歌的旋律，后来搜到了歌词，就经常买花花绿绿的本子抄写歌词。到了初中，很多我很不喜欢的男生疯狂地迷恋周杰伦，可能因为女孩子成熟得早，特别反感他们自以为帅气的非主流风格（还有一个原因是很多喜欢周杰伦的人同时喜欢着许嵩）。本来因为杰伦不清楚的发音，我对他是无感的，但是因为一些烦人的粉丝，对歌手本人也没什么好感。后来到了大学，某个逗比室友对此和我惊人的相似。初中的时候我可能更多的是在追日漫，喜欢主题曲，还记得初二家里买电脑之前，星空卫视每天晚上6点放犬夜叉，我就拿着复读机录那首change the world。到了高一，学校是明令禁止使用电子产品的，我用充饭卡的钱偷偷买了一个mp3,列了一个歌单让前桌的男生回家帮我下点歌，哪知道他全给我下的周杰伦。很幸运的是，他给我下的都是一些慢节奏情歌，最长的电影，给我一首歌的时间，甜甜的，说好的幸福呢，彩虹，七里香之类的。但也没有因为这些歌而粉上杰伦，还是听歌不看人的状态。后来的后来，杰伦当了好声音的导师，看了一两期，发现他其实很个很可爱的人。再到现在，他的新专辑床边故事，那首前世情人，告白气球和now you see me，让我感觉，这就是从前那个酷酷的杰伦呀，那种我以前不屑的风格原来这么奇妙。于是一点没犹豫地在网易云上买了数字专辑。今天大老远从上海跑到合肥听周杰伦的演唱会，虽然位子很不好，看不到人也算了，屏幕也看不到；虽然室外的音响效果也有点让人失望，那些快歌基本听不清歌词；虽然排队很长，座位坐得很乱…但是那些我自己很熟悉的旋律响起的时候，所有的举动只剩下舞动荧光棒，跟着一起唱。让我印象很最深刻的是点歌环节，点到的第三个女孩子，杰伦问她是和谁一起来的，她说她一个人来的。当时杰伦愣了一下，然后安慰她说，全场的观众都是她的朋友，都陪着她听唱歌。当时我特别想去拥抱那个女生。一个人去看演唱会，身边都是情侣或者闺蜜团。你的注意力本该只放在爱豆身上，可无法避免的，有些场景，有些歌词就是会触动你内心那块最柔软的地方。这种经历我体验过。错过了可以有多一点杰伦的年少时光有些遗憾，但也很幸运，我开始路转粉了，未来的路，还可以相伴而行。💗💗💗ps:当然啦，如果能遇见一个喜欢人的一起去苏打绿的after summer，那么会更加幸运~~~","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"周杰伦","slug":"周杰伦","permalink":"http://tankcat2.com/tags/周杰伦/"}]},{"title":"My Favorite Band","slug":"sodagreen","date":"2016-10-17T12:02:31.000Z","updated":"2016-10-17T12:22:48.000Z","comments":true,"path":"2016/10/17/sodagreen/","link":"","permalink":"http://tankcat2.com/2016/10/17/sodagreen/","excerpt":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name.","text":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name.Pure, artistic, indie, free, soft and simple. All of these are my impression of Sodagreen. The band is well known for its lead vocalist and songwriter Wu Tsing-Fong, who is excellent for his poetic lyrics, unique performing style and wide vocal range. As a typical Virgo, he is a paranoia that is disproportionate to an idol. He never does what an idol should do. He is unwilling to please the fans and doesn’t like to participate in the announcement program. To be a qualified artist is very hard; to be an artist who can satisfy all the fans is harder. I still remember the live show in Spring Wave Music And Art Festival this year. Sodagreen was arranged to the final appearance and didn’t finish all the songs in that the organizer advanced the end of the show and turned off the microphone domineeringly. Wu reluctantly left in the dark, but insisted on singing the rest of the songs through Weibo. One of my favorite albums is “Summer/Fever”. Whenever I feel sad, I will listen to this album, which has inspiring power. It was released on September 11, 2009 and is their fifth full-length studio album. It is the second of the band’s Vivaldi Project, a planned series of four albums representing the four seasons respectively. The recording of this album took place in London and the songs were mostly written by the lead vocalist Wu. The album contains Britpop elements and lyrical references to the supernatural, Faust, Madame Butterfly, Don Quixote and the Greek god Dionysus. Among the songs of this album, I like “The Sound That Remains” best. In the lyrics, it draws an analogy between the sound of cicadas and the flood of public opinion, which narrows our horizon. I think the metaphor of the song is what Sodagreen has being teaching us: Don’t always mind about what other people think of you and just be free to pursue the self-value realization. The Sodagreen’s last round of road show “After Summer” before their temporarily overturn has launched. I wish I could grab a ticket for the live show in Shanghai!","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}]},{"title":"(二)构建，运行和REPL","slug":"repl","date":"2016-09-13T15:38:31.000Z","updated":"2017-05-06T13:06:18.000Z","comments":true,"path":"2016/09/13/repl/","link":"","permalink":"http://tankcat2.com/2016/09/13/repl/","excerpt":"原文地址：http://www.braveclojure.com/getting-started/在这一章节呢，你得花一小段时间，熟悉一个快速简单的方法来构建并运行Clojure程序。能在自己机子上跑通一个程序的感觉很棒有木有！到达这一里程碑会让你轻松地实践，会让你忍不住地和别人分享，让你偷偷对那些还在使用上个世纪的编程语言的同事幸灾乐祸hiahiahia…这真的能让你一直充满动力诶~","text":"原文地址：http://www.braveclojure.com/getting-started/在这一章节呢，你得花一小段时间，熟悉一个快速简单的方法来构建并运行Clojure程序。能在自己机子上跑通一个程序的感觉很棒有木有！到达这一里程碑会让你轻松地实践，会让你忍不住地和别人分享，让你偷偷对那些还在使用上个世纪的编程语言的同事幸灾乐祸hiahiahia…这真的能让你一直充满动力诶~除了这些呢，你还会学到如何在一个使用读-计算-输出 循环(Read-Eval-Print Loop)的Clojure进程里立即执行程序代码。使用这个呢，你能实时考查自己对这门语言的理解，并更加高效地学习。但首先呢，我要先简单扼要地介绍一下Clojure，接着会介绍一个Clojure的标准构建工具——Leiningen。最后呢，你会按照下面的步骤动手实现一番： 使用Leiningen创建一个新的Clojure工程； 对这个工程进行构建，从而创建一个可执行的JAR文件； 执行这个JAR文件； 在REPL中执行程序代码。 首先:Clojure是啥？在一座神话里的火山上，Rich Hickey用了一点Lisp(函数式编程)，再加了一小撮他自己史诗般的头发，于是乎，锻造出了炫酷又强大的Clojure！和大多数非Lisp的编程语言相比，这位Lisp的继承人能让你写的代码更富于表现力。它独特的函数式编程也会使得你的思维更加敏捷。除此之外，有一些众所周知的复杂领域，譬如并发编程，困扰开发者多年，而Clojure提供了很好的解决工具。当我们讨论Clojure时，牢记Clojure语言和Clojure编译器之间的区别很重要。Clojure语言是一种Lisp方言，重视函数性，它的语法和语义独立于任何实现。而编译器是一个可执行的JAR文件，clojure.jar，把使用Clojure语言编写的代码编译成JVM字节码。你会观察到，Clojure同时表示语言和编译器，如果你没有意识到二者的区别，这很可能会困扰到你。但你现在你明白了，就没事了。这个区别是很有必要的，因为，不像其他的编程语言，比如Ruby,Python,C,以及一些其他的语言，Clojure是一种主机模式的语言。Clojure的程序时运行在JVM中，依赖于JVM的核心特性，比如线程和垃圾回收机制。Clojure也支持JavaScript和微软公共语言运行库(CLR)，但本书只关注JVM的实现。后面我们会探讨Clojure和JVM之间的关联，但现在你需要理解下面这些关键的概念： JVM进程执行Java字节码； 通常，Java编译器编译Java源码，生成字节码； JAR文件是Java字节码的集合； Java程序通常发布为Jar文件的形式； Java程序clojure.jar读取Clojure源码，并生成字节码； Java字节码被已经运行clojure.jar的JVM进程执行。 Clojure还在不断地发展中。在写这本书之前，它最新的版本是1.7.0,开发变得越来越强大。如果你在很久之后才会阅读这本书，而Clojure也已经有了更新的版本，不用担心！这本书会覆盖Clojure的基础，这些基础不会随着版本的变化而变化。所以你不用让你的机器人管家把书归还到书店。现在呢，你已经知道了Clojure是什么，那么让我们创建一个真正的Clojure程序吧！ Leiningen目前，大多是Clojure程序员使用Leiningen来创建和管理他们的工程。在附录A，你可以找到关于Leiningen的一个完整描述，但是现在先让我们把注意力集中在如何使用它： 创建一个新的Clojure工程 运行一个Clojure工程 构建一个Clojure工程 使用REPL 在此之前，你得确保已经安装了Java 1.6或以后的版本。你可以在终端运行命令java -version来检查java的版本，从 http://www.oracle.com/technetwork/java/javase/downloads/index.html 可以下载到最新的JRE。接着，按照Leiningen主页http://leiningen.org/ 上的教程安装Leiningen。Windows用户可以使用Windows环境下的安装程序。当你安装好Leiningen时，它会自动下载Clojure的编译器，clojure.jar。 创建一个新的Clojure工程创建一个新的Clojure工程超级简单。一行简单的Leiningen命令就可以创建一个工程框架。接下来，你会做一些小练习，比如引入Clojure类库，但现在，这些教程将确保你可以执行你所编写的代码。继续，在终端输入下面的命令来创建你的第一个Clojure工程：1lein new app clojure-noob 这条命令会创建一个类似于下面的目录结构（如果有一些不一样的地方，也没事的）：123456789101112| .gitignore| doc| | intro.md➊ | project.clj| README.md➋ | resources| src| | clojure_noob➌ | | | core.clj➍ | test| | clojure_noob| | | core_test.clj 这个工程框架并不存在潜在特殊性，它仅仅是Leiningen使用的惯例。你将一直使用Leiningen来创建和运行Clojure应用程序，Leiningen期待你的应用程序也能有这个结构。在➊处的第一个文件是project.clj，它是Leiningen的配置文件。它能帮助Leiningen解决一些问题，比如“这个工程有哪些dependency？”,或者“当这个工程运行时，哪一个函数会首先被调用？”。总体来说，你会把你的源代码保存在目录src/中。第三行➌的文件src/clojure_noob/core.clj就是你写代码的地方。第四行的➍的test目录包含了一些测试案例，第二行➋的resource目录则保存一些类似图片的文件。 运行Clojure工程现在，我们来运行工程。在你最喜欢的编辑器中打开src/clojure_noob/core.clj,你会看到：1234567➊ (ns clojure-noob.core (:gen-class))➋ (defn -main &quot;I don&apos;t do a whole lot...yet.&quot; [&amp; args]➌ (println &quot;Hello, World!&quot;)) 第一行➊声明了命名空间，现在你不用纠结这是啥。第二行➋的函数-main就是程序的切入点。附录A会对这个有介绍。现在，换掉第三行➌的“Hello, world!”，改成“I&#39;m a little teapot!”。完整的一行就是：(print “I&#39;m a little teapot!”)。接着，从终端切换到clojure_noob目录，输入： 1lein run 你会看到“I&#39;m a little teapot!”的输出。恭喜你，小茶壶~你编写并执行了一个Clojure程序！通过这本书，你会学习更多关于程序里到底发生了什么，但是现在你所需要知道的是你创建了一个函数-main，当你执行lein run时这个函数就会被调用。 构建Clojure工程运行代码时执行lein run是个不错的选择，但是如果你要和那些没有安装Leiningen的人分享你的作品时该怎么办呢？为此，你可以创建一个独立的文件，这样任何安装了Java的人就都可以执行。执行下面的命令来创建这个文件：1lein uberjar 这行命令创建了文件target/uberjar/clojure-noob-0.1.0-SNAPSHOT-standalone.jar。你可以运行下面的命令来使用java运行该文件：1java -jar target/uberjar/clojure-noob-0.1.0-SNAPSHOT-standalone.jar 当当当当~一个刚出炉的，一流的Clojure 程序target/uberjar/clojure-noob-0.1.0-SNAPSHOT-standalone.jar诞生了！你可以将它部署运行在几乎任何一个平台上。现在，你已经掌握了所有的基本知识来创建，运行和发布一个基础的Clojure程序代码。在后续的章节中，你将深入学习，在运行命令时，Leiningen是做什么，从中你可以对Clojure与JVM之间的关联有一个完整的认识，理解如何运行产品代码。在学习第二章节，讨论Emacs的奇迹和荣耀之前，让我们回顾一些另一个重要的工具：REPL。 使用REPLREPL是一个运行代码的工具。它能够让你和一个运行的程序交互，并快速地实现自己的想法。它提供给你一个DOS风格的命令提示，你可以在里头输入自己的代码。然后它会读取你的输入，计算它，输出结果，接着重复以上操作，继续提供你一个命令提示。这种编码类型保证了其他程序语言所不具备的快速反馈周期。我强烈安利你多使用它，这样你就能够快速检查自己是不是真的理解所学的内容。除此之外，REPL开发也是学习Lisp的必不可少的体验，如果你不使用它，那么很可能你会失去很多东西。执行下面的语句，启动REPL：1lein repl 执行后的输出应该像下面一样：1234567891011121314nREPL server started on port 28925REPL-y 0.1.10Clojure 1.7.0 Exit: Control+D or (exit) or (quit)Commands: (user/help) Docs: (doc function-name-here) (find-doc &quot;part-of-name-here&quot;) Source: (source function-name-here) (user/sourcery function-name-here) Javadoc: (javadoc java-object-or-class-here)Examples from clojuredocs.org: [clojuredocs or cdoc] (user/clojuredocs name-here) (user/clojuredocs &quot;ns-here&quot; &quot;name-here&quot;)clojure-noob.core=&gt; 最后一行clojure-noob.core=&gt;告诉你，现在程序的命名空间是clojure-noob.core。后面你会学习什么是命名空间的，现在你只需要知道命名空间对应着src/clojure_noob/core.clj文件的名字。而且从上面可以看出，REPL的版本是Clojure 1.7.0，但正如前面提到过，只要一切工作正常，用哪个版本都可以的。这个DOS风格的命令行提示也暗示了，你的程序代码是加载到REPL中的，你可以执行那些已经定义过的函数。现在只定义了一个函数-main。执行它：123clojure-noob.core=&gt; (-main)I&apos;m a little teapot!nil 干得漂亮！你刚刚用REPL执行了一次函数调用！接着再试试其他的Clojure基本函数吧：123456clojure-noob.core=&gt; (+ 1 2 3 4)10clojure-noob.core=&gt; (* 1 2 3 4)24clojure-noob.core=&gt; (first [1 2 3 4])1 棒呆！你增加了一些数字，对一些数字进行乘法操作，并从一个向量中取出了第一个元素。这就是与奇特Lisp语法的第一次邂逅~所有的Lisp风格语言，包括Clojure，采取了前缀表示法，意味着表达式的第一个符号永远是操作符。如果你不理解这个，也不用担心，很快你就会学习所有的CLojure语法的。从概念上说，REPL和Secure Shell(SSH)很像。就像你使用SSH和一个远程服务器交互，你也能使用REPL和一个运行中的Clojure进程交互。这个特性是很强大的，因为你甚至可以将一个REPL内置到一个生产应用程序中，在运行过程中调整你的代码。从现在开始，你会一直使用REPL来帮助理解Clojure的语法和语义。还有一点要注意的是：这本书后面展示的代码不再会包含REPL的命令行，但你们也还是得自己动手敲敲代码呀！下面举个例子：1234(do (println &quot;no prompt here!&quot;) (+ 1 3)); =&gt; no prompt here!; =&gt; 4 当你看到类似上面的代码片段，以; =&gt;开头的行表示正在运行程序的输出结果。在上面这个例子中，no prompt here会被打印出来，程序的返回值是4。 Clojure编辑器此时此刻你应该明白，学习Clojure时无须过分讲究编辑器或者集成开发环境的问题。但是如果你想体验一番强大的编辑器，第二章介绍的Emacs将是很棒的选择，它可是在Clojure程序员中最受欢迎的编辑器。当然，你完全没必要使用它，但是Emacs提供了紧密集成的REPL，非常适合编写Lisp风格的程序代码。更重要的是，你认为哪个工具适合自己，就用哪个~ 如果Emacs不是你的菜，这里有一些适合Clojure开发的文本编辑器和IDE： 这个油管上的视频展示了如何安装Sublime Text 2：http://www.youtube.com/watch?v=wBl0rYXQdGg/ Vim为Clojure开发提供了很棒的工具，强烈推荐这篇文章：http://mybuddymichael.com/writings/writing-clojure-with-vim-in-2013.html Counterclockwise是一个很热门的Eclipse插件：https://github.com/laurentpetit/ccw/wiki/GoogleCodeHome 如果你使用Intellij呢，推荐试一试集成开发环境Cursive Clojure：https://cursiveclojure.com/ Nightcode是一个简易，免费的，使用Clojure编写的IDE：https://github.com/oakes/Nightcode/ 综上所述你真是我的骄傲，小茶壶~你已经运行了你第一个Clojure程序了！当然除了这个，你已经熟悉REPL，这样一个开发Clojure软件的最重要的工具之一，的使用了。这简直太不可思议了！这让我想起爱豆的那首“Long Live”里的一句歌词： 你如英雄一般扬起头在史册的那一页这是一个世纪的结束也是一个时代的开端 ——泰勒斯威夫特 万岁！","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"http://tankcat2.com/categories/文档翻译/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"leiningen","slug":"leiningen","permalink":"http://tankcat2.com/tags/leiningen/"},{"name":"repl","slug":"repl","permalink":"http://tankcat2.com/tags/repl/"}]},{"title":"Clojure API整理","slug":"functions","date":"2016-08-30T13:37:31.000Z","updated":"2017-05-06T13:03:58.000Z","comments":true,"path":"2016/08/30/functions/","link":"","permalink":"http://tankcat2.com/2016/08/30/functions/","excerpt":"clojure初学者，在读《clojure编程》的过程中经常遇到一些没有看到过的函数或者宏，于是翻阅clojure api文档了解功能定义与使用方式，在这里做个汇总。由于处于不断学习中，所以不会一下子整理完整。","text":"clojure初学者，在读《clojure编程》的过程中经常遇到一些没有看到过的函数或者宏，于是翻阅clojure api文档了解功能定义与使用方式，在这里做个汇总。由于处于不断学习中，所以不会一下子整理完整。 formacro/宏Usage/用法：(for seq-exprs body-expr)Source/源码：code List comprehension. Takes a vector of one or more binding-form/collection-expr pairs, each followed by zero or more modifiers, and yields a lazy sequence of evaluations of expr. Collections are iterated in a nested fashion, rightmost fastest, and nested coll-exprs can refer to bindings created in prior binding-forms. Supported modifiers are: :let [binding-form expr …], :while test, :when test. 定义：列表解析。作用于含有一个或者多个“绑定-形式”或者“集合-表达式”的向量vector，每一个向量元素遵循0个或者多个修饰器，返回符合修饰要求的表达式执行结果的惰性序列。以嵌套的方式迭代集合，嵌套的“集合-表达式”可以引用之前在“绑定-形式”中创建的绑定。支持的修饰器包括：:let[bing-form expr],:when test 和:while。 示例：123456789101112131415user=&gt;(for [x [0 1 2 3 4 5] :let [y (* x 3)] :when (even? y)] y)(0 6 12)user=&gt;(def digits (seq [1 2 3])) (for [x1 digits x2 digits] (* x1 x2))(1 2 3 2 4 6 3 6 9)user=&gt;(for [x (range 10) y (range 10) :when (&lt; x y) :when (&lt; (+ x y) 5)] [x y])([0 1] [0 2] [0 3] [0 4] [1 2] [1 3])user=&gt;(for [x (range 10) y (range 10) :while (&lt; x y)] [x y])() 注意最后两个示例，:when test计算结果为false时不会阻止循环提前结束，而:while test计算结果为false时循环会立即结束，所以最后一个例子返回为空序列。 filterfunction/函数Usage/用法：(filter pred),(filter pred coll)Source/源码：code Returns a lazy sequence of the items in coll for which (pred item) returns true. pred must be free of side-effects. Returns a transducer when no collection is provided. 定义：返回一个惰性序列，序列中包含集合coll中所有(pred elememt)执行结果为true的元素。pred必须是一个无副作用的函数。当coll没有提供时，返回一个transducer。 示例：123456789;过滤0-9中所有的奇数user=&gt;(filter even? (range 10))(0 2 4 6 8);过滤所有长度超过1的字符串user=&gt;(filter (fn [x] (= (count x) 1)) [&quot;a&quot; &quot;aa&quot; &quot;b&quot; &quot;n&quot; &quot;f&quot; &quot;lisp&quot; &quot;clojure&quot; &quot;q&quot; &quot;&quot;])(&quot;a&quot; &quot;b&quot; &quot;n&quot; &quot;f&quot; &quot;q&quot;) get-infunction/函数Usage/用法：(get-in m ks),(get-in m ks not-found)Source/源码：code Returns the value in a nested associative structure, where ks is a sequence of keys. Returns nil if the key is not present, or the not-found value if supplied. 定义：返回一个嵌套关联结构m中的值，其中ks是一个键的序列。如果键不存在，则返回nil，否则返回对应的值或者提供的默认值not-found。 示例：123456789101112131415161718192021222324252627282930313233;使用get-in返回一个嵌套map中指定的值user=&gt; (def m &#123;:username &quot;sally&quot; :profile &#123;:name &quot;Sally Clojurian&quot; :address &#123;:city &quot;Austin&quot; :state &quot;TX&quot;&#125;&#125;&#125;)#&apos;user/muser=&gt; (get-in m [:profile :name])&quot;Sally Clojurian&quot;user=&gt; (get-in m [:profile :address :zip-code])niluser=&gt; (get-in m [:profile :address :zip-code] &quot;no zip code!&quot;)&quot;no zip code!&quot;;使用get-in查找嵌套vector中指定下标对应的值user=&gt; (def v [[1 2 3] [4 5 6] [7 8 9]])#&apos;user/vuser=&gt; (get-in v [0 2])3user=&gt; (get-in v [2 1])8;混合类型的嵌套结构user=&gt; (def mv &#123;:username &quot;jimmy&quot; :pets [&#123;:name &quot;Rex&quot; :type :dog&#125; &#123;:name &quot;Sniffles&quot; :type :hamster&#125;]&#125;)#&apos;user/mvuser=&gt; (get-in mv [:pets 1 :type]):hamster","categories":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://tankcat2.com/categories/函数式编程/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"}]},{"title":"理解Clojure持久性Vector的实现（一）","slug":"vector","date":"2016-08-30T04:54:31.000Z","updated":"2016-09-19T11:52:42.000Z","comments":true,"path":"2016/08/30/vector/","link":"","permalink":"http://tankcat2.com/2016/08/30/vector/","excerpt":"原文链接：http://hypirion.com/musings/understanding-persistent-vector-pt-1","text":"原文链接：http://hypirion.com/musings/understanding-persistent-vector-pt-1你也许有或者没有听说过Clojure的持久性vector。受Phil Bagwell关于Idel Hash Trees这篇文章的影响，Clojure的发明者Rich Hickey创造出了这种数据结构，能够在\\(O(1)\\)的时间复杂度内实现追加(append)、更新(update)、查找(look up)和截取片段(subvec)操作。由于该vector是持久性的，每一次的修改操作都将创建一个新的vector，而不是修改原来旧的vector。那么，它们是如何工作的呢？我将尝试着通过一系列的博客来作以解释，每一篇博客将只关注一部分内容。这将是一次深入细节的解析，包括一些在实现部分上的不同的、怪异的内容。 今天，我们将学习一些基础知识，涵盖更新(update)、追加(append)和弹出(popping)操作。Clojure持久性vector的实现使用这些操作作为核心，同时也采用了一些加速优化，譬如transient和tail。我们将在后续博客中学习这些内容。 基本思想一般来说，易变的vector和ArrayList仅仅是数组，可按需增长或者减小。当有易变性(mutability)的需求时，它们可以工作得很好；但是如果有持久性(persistence)的需求时，这会导致严重的问题。当执行修改操作时，由于不得不总是需要拷贝整个数组，执行速度将会降低，并且占用大量内存空间。执行查询操作时，如果在不降低性能的前提下，尽可能地避免冗余存储，这将是非常棒的。而这正是Clojure持久性vector在平衡有序二叉树上实现的。基本思路就是实现一个类似二叉树的数据结构，唯一的不同就是，树的内部节点最多只有两个子节点，并且内部节点中不存储元素；叶子节点中至多存储两个元素。元素在叶子节点中是有序存储的，也就是说，第一个元素就是最左边叶子节点中存储的第一个元素，最后一个元素就是最右边叶子节点中存储的最后一个元素。眼下我们要求所有的叶子节点在同一深度上[参考1]。举个例子，如下图所示，该树存储整数0~8，其中0是第一个元素，8是最后一个元素。这个树对应的vector的大小是9。 假设这个vector是可变的，当在最后增加一个新的元素是，我们将把元素9插入到最右的叶子节点中，如下图所示。 但是这里存在一个问题，如果需要保证持久性就不能这么操作。而且如果我们想要更新一个元素，这么做显然行不通，因为必须拷贝整个结构，或者至少是部分。为了在保持持久性的前提下最小化拷贝代价，我们采用路径拷贝：拷贝从根节点到需要修改或者插入元素所在节点的路径上的所有内部节点，并在到达叶子节点时用新的元素值替代旧的值。下图展示了多次插入操作的结果。这里，包含7个元素的vector和包含10个元素的vector共享结构。 粉红色的节点是两个vector所共享的部分，而棕色和蓝色的节点是各自所属的。其他没有显示的vector可能也和这两个vector共享部分节点。 Update 更新最容易理解的“修改”操作应该是更新或者替换vector中的元素。接下来我们首先解释更新(update)操作是如何运行的。在Clojure中，更新对应着assoc和update-in/update函数。为了更新一个元素，我们需要往下对树进行遍历，直到到达该元素所在的叶子节点。当我们遍历时，对路径上的内部节点进行拷贝以保证持久性。当我们到达叶子节点时，对其进行拷贝，同时对拷贝的节点用新的元素值进行代替。最后，连带更新的路径，我们返回新的vector。举个例子，我们对一个存储0~8元素的vector执行assoc操作，如下所示：12(def brown [0 1 2 3 4 5 6 7 8])(def blue (assoc brown 5 &apos;beef)) 蓝色节点是被拷贝的路径，内部结构如下图所示： 此处假设我们已知晓如何定位到需要修改元素所在的叶子节点，这看起来非常容易。(这个系列博客的下一部分我们将介绍如何寻到一条路径到指定的索引) Appends 追加追加操作(在vector的最后插入)和更新操作差别不是很大，除了一些为了存储值不得不创建新节点的边缘情况，本质上，分为以下三种： 最右叶子节点中有空间存储一个新的元素； 根节点有空余空间，但是最右叶子节点不存在空闲空间； 当前的根节点没有足够的空间存储新的元素。 下面我们将一一讨论这三种情况，它们的解决方法不是很难掌握。 1. 类似Assoc不论何时，当最右的叶子节点中有足够的空间，执行追加操作就如同执行assoc函数：我们只拷贝路径上的节点，在最新创建的叶子节点上，我们将待插入的元素放置在最后一个位置中。举个例子，执行(conj [0 1 2 3 4] 5)，下图展示了过程和内部结构。蓝色节点是新创建的，棕色节点是旧节点。 就这样，没有什么神奇的内容，仅仅是拷贝路径节点和在叶子节点中插入元素。 2. 当需要时创建新的节点那么，当最右叶子节点中存储空间不足是应该怎么办？幸运的是，我们不会止步于一个错误位置的叶子节点：我们总是有办法寻找正确的路径到达叶子节点。相反，我们意识到，将达到的叶子节点并不存在(指针为空)。当一个节点不存在时，我们会创建一个并将其设置为原本“拷贝”的节点。 如上图所示，粉色节点是新建的节点，蓝色的是拷贝的路径节点。 3. 根节点溢出最后一种情况是根节点溢出。当根节点对应的这个树不存在空余空间时，这种情况会发生。如何解决这个情况也不难：我们创建一个新的根节点，然后将旧的根节点作为新根节点的第一个孩子节点。从这里，我们执行了节点创建操作，就像上一种情况中一样。下图所示的例子中，紫色节点是新的根节点，粉色的节点是拷贝的路径节点和创建的叶子节点。 解决这种情况是问题，但检测这种情况什么时候发生也至关重要。幸运的是，这也很容易。当它是一个二叉分支的vector时，当vector中元素个数是2的幂次方时，这种情况就会发生。一般来说，当一个n叉分支的vector存储的元素为n的幂次方个时，会发生根节点溢出。 Popping 弹出解决弹出操作(剔除最后一个元素)的方法同样也不难掌握。类似于追加，弹出操作也会遇到三种情况： 最右叶子节点包含不止一个元素； 最右叶子节点只存储一个元素(执行弹出操作后存储为空)； 执行弹出操作后，根节点只包含一个元素。 本质上来说，这只是前面追加部分3种情况的复原，同样不复杂。 1. Dissoc再者，我们面临的情况与更新一个vector类似：我们向下遍历到叶子节点的过程中拷贝路径节点，然后移除拷贝的叶子节点的最后一个元素。只要新的叶子节点中至少还包含一个元素时，我们就不需要做额外的操作。 记住，在一个vector上多次执行弹出操作不会得到同一个vector：它们是相等的，但是不共享根节点。举个例子：123(def brown [0 1 2 3 4 5])(def blue (pop brown))(def green (pop brown)) 执行上述代码得到的结果对应的内部结构如下图所示： 2. 移除空节点不论什么时候，当叶子节点只包含一个元素时，我们需要一些不同的处理。不惜一切代价，我们要避免在树中存在空节点。因此，一旦出现空间点，我们返回null而不是返回节点本身。父节点此时将包含一个空孩子指针，而不是一个指向空节点的孩子指针： 这里，棕色的vector是原始的vector，蓝色的是弹出操作之后生成的vector。不幸的是，仅仅删除叶子节点不是那么容易，你会发现，当我们返回一个空指针给一个节点，而这个节点原本只包含一个子节点，我们必须将子节点作为空指针返回：清空一个节点的结果将向上层传播。这里要正确处理需要一点技巧，但本质上它是通过向下查看新的孩子节点，如果是null，并且预计要放在索引为0的位置上，则返回null。如果用Clojure实现，它看起来是如下所示的递归函数：1234567891011121314(defn node-pop [idx depth cur-node] (let [sub-idx (calculate-subindex idx depth)] (if (leaf-node? depth) (if (== sub-idx 0) nil (copy-and-remove cur-node sub-idx)) ; not leaf node (let [child (node-pop idx (- depth 1) (child-at cur-node sub-idx))] (if (nil? child) (if (== sub-idx 0) nil (copy-and-remove cur-node sub-idx)) (copy-and-replace cur-node sub-idx child)))))) 实现这样的函数后，节点删除操作的所有情况就全部兼顾到了。举个例子，如下图所示，执行弹出操作后，vector删除了2个节点(蓝色的表示新vector):包含c的叶子节点和它的父节点。 3. 移除根节点到此为止我们已经覆盖到所有情况，除了最后这一种。在目前的实现中，如果对包含9个元素的vector执行弹出操作，会得到以下结果： 不错，我们得到了一个只有一个孩子节点的根节点。但这个根节点根本毫无用处，当我们查询或者添加元素时，我们总是会向下移动到孩子节点，并且追加元素时将会创建一个新的根节点。因此我们要做的就是消除这个根节点。这里所做的可能是这篇博客最简单的部分：在我们执行完弹出操作后，检查根节点是否只包含一个孩子节点(检查第二个孩子指针是否为空)。如果是，并且这个根节点不是叶子节点，则我们仅需要用这个孩子节点代替根节点即可。正如我们所期待，一个新的vector(蓝色的)，使用了原来vector的根节点的唯一子节点作为新的根节点： O(1)!=O(log n)有些人可能会疑惑，为什么这些操作的时间复杂度是\\(O(1)\\)。事实上，当一个节点有2个孩子时，负载度是\\(O(log_2{n})\\),与\\(O(1)\\)相差甚远。不过，没有人说过，每个节点不许只有两个孩子(常代指分支因子)。Clojure中的vector每个节点包括32个孩子节点，这样的结果是生成的树深度很小。事实上，如果vector中的元素个数少于10亿个，整个树的深度最多6层。你需要大概35亿个元素才能达到8层深度的树。这种时候，我认为内存消耗是更加严重的问题了。为了真正辨别出区别，这里我们给出一个四叉树的例子：这个四叉树含有14个元素，但只有2层的深度。如果你向上滚动鼠标查看前面包含13或者12个元素的树，你会发现它们已经达到4层的深度了，是这种情况的2倍。 深度极小的树带来的后果是，我们倾向于将Clojure vector的修改和查找操作看成是近似常量时间，尽管理论上它的时间复杂度是\\(O(log_{32} n)\\)。了解大O记法的人应该都知道这等价于\\(O(log n)\\)，但是从营销角度看，人们喜欢加上常量因子。 后续工作希望这篇博客可以帮助你们更好地理解Clojure持久性vector是如何工作的，以及在更新、添加和弹出操作背后的思想，同时非正式地阐述这些操作是如何做到高效的。关于追加和弹出的进一步优化是可以实现的，我将在写完更“关键”的部分后讨论这些工作。尤其是关于tail,transient和subvec是如何更有意义地工作的，在讨论稍许实现细节之前。 第二部分将探讨分支和如何查找元素的细节工作。 [1] 你可以认为这是对路由选择的简化，尽管由于JVM自适应优化，它保括了对JVM运行速度的影响。后面我们会更深入讨论这一点。","categories":[{"name":"文档翻译","slug":"文档翻译","permalink":"http://tankcat2.com/categories/文档翻译/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"vector","slug":"vector","permalink":"http://tankcat2.com/tags/vector/"}]},{"title":"利用map简单实现数据聚合计算","slug":"map","date":"2016-08-29T09:09:31.000Z","updated":"2017-05-06T13:05:52.000Z","comments":true,"path":"2016/08/29/map/","link":"","permalink":"http://tankcat2.com/2016/08/29/map/","excerpt":"map通常也被用来保存总结信息、索引信息或者对应关系，想象一下数据库中的索引和视图，map和它们类似。比如group-by函数可以用来很方便地根据一个key函数把一个集合分成多组：","text":"map通常也被用来保存总结信息、索引信息或者对应关系，想象一下数据库中的索引和视图，map和它们类似。比如group-by函数可以用来很方便地根据一个key函数把一个集合分成多组：12user=&gt;(group-by #(rem % 3) (range 10))&#123;0 [0 3 6 9] 1[1 4 7] 2[2 5 8]&#125; 这里我们看到，把相同key的值的元素组合到了同一个组里面去了。有的时候我们需要的不是把元素分组，而是计算出每组的聚合信息。可以利用group-by和reduce来定义一个reduce-by函数用来对任意种类的数据计算聚合数据。假设有一些订单数据，我们利用map来表示：12345678910user=&gt;(def orders [&#123;:product &quot;Clock&quot;, :customer &quot;Wile Coyote&quot;, :qty 6, :total 300&#125; &#123;:product &quot;Dynamite&quot;, :customer &quot;Wile Coyote&quot;, :qty 20, :total 5000&#125; &#123;:product &quot;Shotgun&quot;, :customer &quot;Elmer Fudd&quot;, :qty 2, :total 800&#125; &#123;:product &quot;Shells&quot;, :customer &quot;Elmer Fudd&quot;, :qty 4, :total 100&#125; &#123;:product &quot;Hole&quot;, :customer &quot;Wile Coyote&quot;, :qty 1, :total 1000&#125; &#123;:product &quot;Anvil&quot;, :customer &quot;Elmer Fudd&quot;, :qty 2, :total 300&#125; &#123;:product &quot;Anvil&quot;, :customer &quot;Wile Coyote&quot;, :qty 6, :total 900&#125; ])#&apos;user/orders 下面定义一个reduce-by函数，算出每个人的订单总金额：123456789user=&gt;(def reduce-by [key-fn f init coll] (reduce (fn [summaries x] (let [k (key-fn x)] (assoc summaries k (f (summaries k init) x)) ) ) ) &#123;&#125; coll )#&apos;user/orders 这里讲一下reduce函数的功能。 定义中给出的解释是，reduce的功能由以下情况定义： 没有给定val coll为空：以无参的方式调用f，调用所得的值为返回值； coll只有一个元素：不调用f，直接将这个元素作为返回值； coll不止一个元素：将coll的前两个元素应用到f，得到的结果再和coll的第三个元素一起应用到f，以此类推。 给定val coll为空：不调用f，直接返回val； coll不为空：将val和coll的第一个元素应用到f，得到的结果和coll的第二个元素一起应用到f，以此类推。 12user=&gt;(reduce-by :customer #(+ %1 (:total %2)) 0 orders)&#123;&quot;Elmer Fudd&quot; 1200, &quot;Wile Coyote&quot; 7200&#125;","categories":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://tankcat2.com/categories/函数式编程/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"map","slug":"map","permalink":"http://tankcat2.com/tags/map/"},{"name":"reduce","slug":"reduce","permalink":"http://tankcat2.com/tags/reduce/"}]},{"title":"关于clojure的some函数","slug":"some","date":"2016-08-29T04:33:31.000Z","updated":"2017-05-06T13:06:30.000Z","comments":true,"path":"2016/08/29/some/","link":"","permalink":"http://tankcat2.com/2016/08/29/some/","excerpt":"在读《clojure编程》，关于some函数有点疑惑，书中some函数的解释是在一个序列里面搜索第一个能够符合指定谓词的元素，给出的例子是：","text":"在读《clojure编程》，关于some函数有点疑惑，书中some函数的解释是在一个序列里面搜索第一个能够符合指定谓词的元素，给出的例子是：12345user=&gt;(some #&#123;1 3 7&#125; [0 2 4 5 6])nilluser=&gt;(some #&#123;1 3 7&#125; [0 2 3 4 5 6])3 但是，在执行下面一个例子时，返回的结果和我想象中的不一样：12user=&gt;(some #&#123;false 3 7&#125; [false])nill 按理说，false是存在于这个set集合中的，但是返回的是nil。后来翻看了clojure api中关于some函数的定义，才知道是怎么一回事。 some函数的参数是一个谓词函数和一个集合，返回集合中第一个符合谓词(pred x)的逻辑上为true的元素。而nil或者false在逻辑上均是false，不符合要求，所以最后的返回值是nil。 下面给出some的源码。1234(defn some [pred coll] (when (seq coll) (or (pred (first coll)) (recur pred (next coll)))))","categories":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://tankcat2.com/categories/函数式编程/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"some","slug":"some","permalink":"http://tankcat2.com/tags/some/"}]},{"title":"(一)Introduction","slug":"introduction","date":"2016-08-28T09:02:31.000Z","updated":"2017-05-06T13:04:40.000Z","comments":true,"path":"2016/08/28/introduction/","link":"","permalink":"http://tankcat2.com/2016/08/28/introduction/","excerpt":"在你内心深处，你一直都知道自己是注定要学习Clojure的。每次因为一个难以理解的类层次结构，你高举键盘，在痛苦中呐喊；每次因为一个诱导突变的海森堡Bug(heisenbug)，你躺在床上夜不能寐，泣不成声，妨碍到你身旁的爱人；每次遇到一个竞争状态，你揪扯着日渐稀少的头发…其实你已知晓必须有一个更好的办法来解决这些问题。 终于，现在你面前的这份教学材料给予了你一直渴望的编程语言。","text":"在你内心深处，你一直都知道自己是注定要学习Clojure的。每次因为一个难以理解的类层次结构，你高举键盘，在痛苦中呐喊；每次因为一个诱导突变的海森堡Bug(heisenbug)，你躺在床上夜不能寐，泣不成声，妨碍到你身旁的爱人；每次遇到一个竞争状态，你揪扯着日渐稀少的头发…其实你已知晓必须有一个更好的办法来解决这些问题。 终于，现在你面前的这份教学材料给予了你一直渴望的编程语言。 学习一门新的编程语言：历经四个迷宫的旅程为了充分使用Clojure，在学习一门新的语言时，你需要在以下四个迷宫中寻找自己的出路： 工具之林。一个友好的、高效的编程环境可以使你很容易实现自己的想法。你需要学习如何搭建自己的编程环境。 语言之山。当你前进时，你会收获Clojure的语法、语义和数据结构的知识，你会学习如何使用强有力的工具和宏，学习使用Clojure的并发结构来简化你的人生。 石器之穴。继续深入，你会学习如何构建、运行和发布自己的程序，并学会如何使用核心库。除此之外，你还会学习如何使Clojure与JMV交互。 思维之堡。在稀薄的空气中，你会知晓为什么要使用以及如何使用Lisp与函数式编程。在这个过程中，你将了解到贯穿Clojure整体的简易哲学，以及如何以这种方式解决问题。 毫无疑问，你是需要工作的。但是这本书会让你的工作变得令人兴奋，而不是令人筋疲力尽，因为这本书遵循以下三条原则： 遵循“甜品第一”的法则，提供开发工具和语言的细节，使得你可以立刻玩上真正的程序。 你可以没有JVM、函数式编程或者Lisp的基础，因为这本书涵盖了这些概念，所以当在编写和运行Clojure程序时，你对你所做的将感到自信。 这本书回避了现实生活的例子，代替以更加有趣的练习，例如袭警的霍比特人和跟踪闪闪发光的吸血鬼。 最后，你将能够使用Clojure，这个世上最振奋人心、最有趣的编程语言之一！ 这本书是如何组织的这本书分为三部分介绍Clojure，以便于在你们勇敢的探索过程中更好地引导你们这些勇敢的新clojurer们~ 第一部分：环境设置为了保持活力和高效地学习，你需要动手编写代码，创建可执行文件。这一部分中的章节带你走上快速使用工具的旅程，让你得以简单地编写程序。这样子，你就可以专心于学习Clojure，而不用花时间摆弄环境。 第一章：创建，运行和REPL。让一个真实的程序运行起来是种强有力的、令人振奋的事情。一旦你做到了，你可以畅通无阻地实现，可以真正地分享你的成果。在这个短小的章节里，你将花费一小部分时间快速熟悉创建和运行Clojure程序，你将学习使用REPL来运行程序，这将加强你的环形反馈，使得你的学习更叫高效。 第二章：如何使用Emacs，一个绝妙的编辑器。一个快速的环形反馈对学习来说是至关重要的。在这一章节，我将自底向上介绍Emacs来确保你们拥有一个高效的Emacs/Clojure工作流程。 第二部分：语言基础这部分的章节负责给你夯实基础，以便于后续的Clojure学习。你将开始学习Clojure的基础知识，包括语法、语义和数据结构等，这样的话，你可以做一些事情。接着，你将后退一步，回头仔细检查Clojure中使用最多的函数，学习如何使用函数式编程的思想解决问题。 第三章：Clojure速成班。从这儿开始，你将深入了解Clojure。此时，你可能会关闭网页窗口因为你会用尽力气大声喊叫“holy moley that’s spiffy！”，一直到看到这本书的目录才会停止。毫无疑问，你已经听说了Clojure完美的并发支持和其他奇妙的特性，但是Clojure最显著的特性是，它是一门Lisp的方言。你将会探索它的核心，即函数和数据。 第四章：深度核心函数。在这一章节，你将会学习一些Clojure的基础概念。这在你阅读函数文档的时候提供基础，在使用函数的时候得以理解。除此之外，你会接触到你将接触到的绝大多数函数的使用示例，这会为你编写自己的代码和阅读他人的工程打下夯实的基础。还记得我提到的“追踪闪闪发光的吸血鬼”的例子吗？你将会在这章节遇到它(除非你在业余时间已经完成了)。 第五章：函数式编程。这一章节，你会拥有关于函数和数据结构的具体经验，并在其中注入一个新的思维：函数式编程。你将在创建最火热的、横扫全国的游戏“Peg Thing”中展示你的知识。 第六章：组织你的项目工程：一个图书管理员的故事。这一章节解释了什么是命名空间以及如何组织你的代码。我不想泄露太多，只能告诉你它涉及到一个国际奶酪贼。 第七章：Clojure炼金术：读取，计算和宏。在这个章节中，我们将后退一步，学习Clojure是如何运行你的程序的。这将会告诉你真正需要理解的关于Clojure是如何工作的概念结构，以及为何Clojure与其他非Lisp语言存在差异。在这个结构中，我将介绍一种强有力的工具——宏。 第八章：编写宏。这一章节从一个基础的示例开始到逐渐提高复杂度，彻底仔细地介绍了如何编写宏。你将戴上一顶伪装的帽子，假装你正运行一个在线的药品商店，使用宏来合理化用户订单。 第三部分：高级主题这部分的章节覆盖了额外的有关Clojure的主题：并发、Java互操作和抽象。尽管你可以无需理解这些概念编写程序，但是它们在智力上对你会有所回报，并且赋予你难以想象的能量。正如人们所说，学习Clojure的理由之一是，学习Clojure会使你成为一个更优秀的程序员，使得以上提出的概念更加容易理解和被使用。 第九章：并发和并行编程的神圣艺术。这一章节里，你将学习并发和并行是什么，以及它们的重要性。当你编写并行程序，你将面临挑战，并学习如何使用Clojure的设计减轻。你将使用future、delay和promise来编写安全的并行程序。 第十章：Clojure的形而上：Atoms,Refs,Vars和Cuddle Zombies。这一章节将带你深入了解Clojure管理状态和简化并发编程的方法。你将学会使用atoms,refs,vars来管理状态，以及学习如何使用pmap执行无状态的并行计算。你将遇见cuddle zombies。 第十一章：使用core.async精通并发编程。在这一章节，你会思考这个世界上的万物都是热狗自动售货机。通过这个方式你将会学习如何建模一个系统，独立地运行程序，使用库core.async通过信道和用户交互。 第十二章：与JVM的互操作。这一章节类似一个通向Java的常用语手册和文化导入的桥梁，阐述了什么是JVM，它如何运行程序，如何编译程序。除此之外，还提供了常用的Java类和方法的简要阐述，解释了如何使用它们与Clojure交互。这一章节向你们展示了如何思考和理解java以便你可以将java的库文件使用到你的Clojure文件中。 第十三章：创建和拓展抽象：多重方法、协议和记录。在第四章中，你已经知道了Clojure的主要特性是抽象，这一章节介绍如何实现你自己的抽象。你将学习到多重方法、协和和记录的基础知识。 附录A：使用Leiningen开发。该附录部分列列举了使用Leiningen的优势，类似Maven，介绍了如何解决Java 库的版本号问题。 附录B：奇特的Clojure开发框架：Boot。Boot可替代Leiningen，在完成同样的功能基础上提供额外的优点，使得扩展和编写可组合的任务更加容易。这一章节阐述了Boot的基础概念，并引导你完成你的第一个任务。 代码这本书的源码是按照章节组织的，你可以从code获得下载资源。第一章描述了Clojure代码的不同运行方式，包括如何使用REPL。我建议你们使用REPL来运行书中绝大多数的示例，尤其是第三章和第八章。这可以帮助你习惯编写和理解Lisp，同时也能让你保留正在学习的知识。但是，对于那些内容较长的代码示例，最好是把代码写到文件中，然后运行。 启程吧!勇敢的读者，准备好了吗？你准备好迎接你真正的命运了吗？抓好机会，你将要踏上一个终生的旅程！","categories":[{"name":"Clojure for the brave and true","slug":"Clojure-for-the-brave-and-true","permalink":"http://tankcat2.com/categories/Clojure-for-the-brave-and-true/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"文档翻译","slug":"文档翻译","permalink":"http://tankcat2.com/tags/文档翻译/"}]},{"title":"利用高阶函数构建简易日志系统","slug":"blog","date":"2016-08-27T12:11:31.000Z","updated":"2017-05-06T13:03:52.000Z","comments":true,"path":"2016/08/27/blog/","link":"","permalink":"http://tankcat2.com/2016/08/27/blog/","excerpt":"日志对于任意大小的系统都是一个必备的组件，对于开发人员来说，是定位、分析软件故障时的重要依据；对于运维人员来说，是了解运行状态、系统状态的重要途径；对于业务需求来说，是获取统计业务相关数据的重要来源。对于日志系统进行配置通常也因为种种原因很烦人并且复杂。这里用高阶函数来实现一个日志系统。","text":"日志对于任意大小的系统都是一个必备的组件，对于开发人员来说，是定位、分析软件故障时的重要依据；对于运维人员来说，是了解运行状态、系统状态的重要途径；对于业务需求来说，是获取统计业务相关数据的重要来源。对于日志系统进行配置通常也因为种种原因很烦人并且复杂。这里用高阶函数来实现一个日志系统。 0 高阶函数高阶函数(high-order functions，hof)是采取其他函数作为参数的函数。hof是一个重要的功能编程技术，在clojure中广泛使用。一旦你习惯把一个函数当做一个普通的值，那么会发现编写一个把函数作为返回值或者接受函数作为参数的高阶函数是很自然的事情。下面是一个hof的例子：返回某个给定数字与它的参数的和 1234567user=&gt;(defn adder [n] (fn [x] (+ n x)))#&apos;user/adder((adder 5) 18)23 第2行函数体部分定义了一个匿名函数，即函数adder的返回值是一个函数。 第6行调用adder，运行过程中n赋值为5，x赋值为18，结果输出23。 1 打印到标准输出我们都为使用System.out.println、puts或者print来打印日志感到羞愧——虽然有时候它们很方便，但是太原始了。为了改进这一点，从最简单的实现开始，实现一个高阶函数，返回值是一个可以把信息打印到任何作为参数传给高阶函数的writer的函数。 123456user=&gt;(defn print-logger [writer] #(binding [*out* writer] (println %) ))#&apos;user/print-logger 第1行，高阶函数print-logger接受一个参数，这个参数可以是任何实现了java.io.Writer接口的类的一个实例，这个接口是写数据到某个输出设备的最基本的接口。 第2行，使用函数字面量#()定义一个匿名函数，这个函数将标准输出out绑定到参数writer上。 第3行，在执行print-logger时，会将其参数(待写入的数据)传给占位符%，并用println写入到标准输出。 让我们看看它到底是如何工作的：12user=&gt;((print-logger *out*) &quot;hello&quot;)#&apos;user/hello print-logger总是返回一个接受单个参数的函数，这里我们传入一个字符串hello来调用它。 2 打印到内存这里做个改动，不将数据直接输出到标准输出，而是写入到一个内存buffer。这里我们使用java.io.StringWriter，不是输出到一个设备，而是直接保存到内存中的一块区域。 12345678910111213user=&gt;(def writer (java.io.StringWriter.))#&apos;user/writeruser=&gt;(def retained-logger (print-logger writer))#&apos;user/retained-loggeruser=&gt;(retained-logger &quot;hello&quot;)nilluser=&gt;(str writer)&quot;hello\\n&quot; 第1行，我们创建并保持StringWriter类的一个引用，创建引用是为了在后面调用print-logger之后检查数据是否真的被打印到内存中。 第9行，调用日志函数。它不会打印任何内容到标准输出，因为数据被打印到我们提供的内存对象中了。 3 打印到文件接下来，我们希望日志框架由能力把数据打印到一个文件中。上面定义的高阶函数print-logger允许我们很容易做到这一点：对于任意给定的writer,它都会返回一个函数，而这个返回的函数就会把消息打印到我们提供的writer。所以只要只要得到一个输出设备是我们指定文件的writer的引用，就搞定了。 123456789101112user=&gt;(defn file-logger [file] #(with-open [f (clojure.java.io/writer file :append true)] ((print-logger f) %) ))#&apos;user/file-loggeruser=&gt;((file-logger &quot;messages.log&quot;) &quot;hello&quot;)nilluser=&gt;% more messages.loghello 第1行，高阶函数file-logger接受单个参数file，需要记录的数据会被写入到这个文件里。由于clojure.java.io/writer的语义，这个file参数可以是具体的文件路径，也可以是java.io.File、java.net.URL或者java.net.URI类的一个对象。 第2行，由file-logger创建的函数字面量返回一个指向file的writer，这个file以追加的模式打开，并且把这个writer在局部作用域命名为f。 第3行，用创建的writer来调用print-logger，这会创建一个日志函数。要记住的是，函数本身就是值，所以我们不需要在上面先定义一个指向函数的引用，然后再调用它。 第11行，输出测试结果。当然，还可以定义其他的日志高阶函数，比如定义一个打印到数据库的，打印到消息队列的，打印到其他存储介质的，这样我们可以根据需要来选择使用。 4 打印到多个输出最后，如果想把一条日志打印到多个地方呢？这个逻辑不应该是日志函数考虑的。要实现这个，需要另一个不同的高阶函数，这个函数本身不做写日志的事情，而只是调用其他日志函数来写日志： 1234567891011121314151617user=&gt;(defn multi-logger [&amp; logger-fns] #(doseq [f logger-fns] (f %) ))#&apos;user/multi-loggeruser=&gt;(def log (multi-logger (print-logger *out*) (file-logger &quot;message.log&quot;) ))#&apos;user/loguser=&gt;(log &quot;hello again&quot;)hellonill 第1行，multi-logger接受任意多个日志函数。 第2行，利用函数字面量返回的函数会遍历调用每一个函数打印日志。 现在我们能把日志记录同时打印到多个地方去了。","categories":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://tankcat2.com/categories/函数式编程/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"high-order function","slug":"high-order-function","permalink":"http://tankcat2.com/tags/high-order-function/"}]},{"title":"关于clojure中的partial函数","slug":"partial","date":"2016-08-26T09:17:31.000Z","updated":"2017-05-06T13:06:04.000Z","comments":true,"path":"2016/08/26/partial/","link":"","permalink":"http://tankcat2.com/2016/08/26/partial/","excerpt":"这两天在读《clojure编程》，关于第二章的partial函数的讲解比较困惑，对书上的例子理解不是很透彻。于是谷歌，在ClojureDocs论坛中找到了相关的解释和示例，再结合书上的定义，才get到了里头的门路。这里稍作整理。","text":"这两天在读《clojure编程》，关于第二章的partial函数的讲解比较困惑，对书上的例子理解不是很透彻。于是谷歌，在ClojureDocs论坛中找到了相关的解释和示例，再结合书上的定义，才get到了里头的门路。这里稍作整理。 (partial f) (partial f arg1) (partial f arg1 arg2) (partial f arg1 arg2 arg3) (partial f arg1 arg2 arg3 &amp; more)Takes a function f and fewer than the normal arguments to f, and returns a fn that takes a variable number of additional args. When called, the returned function calls f with args + addition args. 这是partial函数的定义，意思是预先为某个函数(f)加上个别参数，调用partial函数返回的也是一个函数(g)，等到这个函数(g)真正被调用的时候把剩下的参数补上即可。 下面举几个例子。1234567891011121314151617user=&gt; (def hundred-times (partial * 100))#&apos;user/hundred-timesuser=&gt; (hundred-times 5)500user=&gt; (hundred-times 4 5 6)12000user=&gt; (defn fun-full [x y] (+ x y))#&apos;user/fun-fulluser=&gt; (def fun-half (partial fun-full 2))#&apos;user/fun-halfuser=&gt; (fun-half 3)5 回头看《clojure编程》中的解释偏函数应用则是你把函数的一部分参数传给一个函数，这样创建一个新函数，这个函数需要的参数就是你没有传给那个函数的那些剩余参数，就比较容易理解了。","categories":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://tankcat2.com/categories/函数式编程/"}],"tags":[{"name":"clojure","slug":"clojure","permalink":"http://tankcat2.com/tags/clojure/"},{"name":"partial","slug":"partial","permalink":"http://tankcat2.com/tags/partial/"}]},{"title":"Streaming Similarity Self-Join","slug":"sssj","date":"2016-08-23T05:45:31.000Z","updated":"2016-09-10T10:47:10.000Z","comments":true,"path":"2016/08/23/sssj/","link":"","permalink":"http://tankcat2.com/2016/08/23/sssj/","excerpt":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。","text":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。 Introduction 介绍在数据库和数据挖掘领域，相关性自连接处理被广泛研究，其应用场景较为广泛，包括剽窃检测、查询优化、协同过滤、重复网页的检测与去除等。若使用暴力解法，其复杂度是\\(O(n^2)\\)。在现实应用中，数据项常以高维稀疏向量的形式呈现，那么相似度的计算即为余弦相似值的计算。为了方便处理，可以将向量归一化，则进一步将问题转换为两个向量的点积。目前解决相似度自连接的算法主要依赖于基于倒排索引的删减技术以及一些数值界限。计算相似度自连接，不仅适用于静态的数据集，在数据流领域也同样适用。这里举例两个现实应用，在数据流环境下进行相似度自连接处理。 趋势监测。社交平台，譬如Twitter，趋势监测算法识别频率陡增的主题标签。更细粒度的趋势监测算法也会识别微博集合。这些集合中的数据项出现的频率增加，并且同时出现某些相同的标签。对于该趋势监测算法，在数据流环境下找出相似的微博十分重要。 近似重复项过滤。同样，在社交平台，譬如Twitter，当某个事件发生时，用户们可能会接收到与该事件相关的近似重复的多条微博。这些微博通常连续地出现。因此，将这些近似重复的微博进行过滤或者分组有利于提高用户体验。 但问题是，关于数据流环境下的相似度自连接处理的研究并不多。这是由于缺乏无限内存：不能将先到来数据项随意删除，因为该数据项可能与未来到的数据项相似。本文引入一个时间因子来解决内存问题。我们设定，只有在到达的时间间隔在指定范围内，两个数据项才有可能相似。我们定义时间依赖的相似性：基于内容的点积与时间因子的乘积，该时间因子会随着时间间隔的增加呈现指数级减小。由于时间因子的存在，当某些数据项的到达时间超出一定范围，我们可以将这些数据项删除。下图表达了这个idea。 如上图所示，标记有时间戳的文档以数据流的形式源源不断到达。时间轴上方的文档包含相似的内容，标记为红色。在所有两两相似的文档对中，我们只关心到达时间相近的文档对。因此，图中所有的4-选-2的文档对里，只有两对文档被选中(用蓝色箭头标记)。与已有的相似性自连接处理算法类似，本文的算法也依赖于索引技术。已有的算法使用不同类型的索引过滤，以减少通过索引返回的潜在配对项数量。按照这种语义，我们定义时间过滤来，与时间依赖的相似性进行关联，以便将旧的索引项从索引列表中删除。本文提出了两种算法框架来解决数据流的相似性自连接处理，均依赖于时间过滤。MB框架使用现成的索引技术，在运行过程中以流水线的方式创建两个索引，随着时间的推移丢弃旧索引。STR框架对现有的索引技术进行调整，将时间过滤的因素内嵌到其中。此外，本文结合已有的索引技术AP和L2AP，设计了一种可处理流式数据的索引算法L2,它存在以下四点优势： 有效减少潜在相似数据项的配对数量； 不需要收集数据流的统计信息； 使用轻量级的索引维持； 当数据项变“旧”时，可以迅速丢弃。 Related Work 相关工作关于数据流上的相似性自连接，已有的研究很少。与之相关的主要就是相似性自连接，这个课题的研究相对广泛，由Chaudhuri等人首次在文献A primitive operator for similarity joins in data cleaning中提出，此后涌现出大量相关算法研究，与本文最为相关的是由谷歌的Bayardo提出的AP算法(Scaling up all pairs similarity search)以及Anastasiu与Karpis提出的L2AP算法(L2AP: Fast Cosine Similarity Search With Prefix \\(L_2\\) Norm Bounds)。有关这两个算法，下文会进行阐述。 Problem Statement 问题陈述我们定义数据项是m维的行向量，向量中的元素是实数值。我们定义\\(sim(x,y)\\)为计算向量\\(x\\)和向量\\(y\\)相似性的函数，并设定所有的向量均被归一化为单位向量，即\\(||x||_2=||x||=\\sqrt{\\sum_{j=1}^{m}x_j^2}=1\\)。则数据项的相似度计算可以简化为两个向量的点积，如下公式所示。$$sim(x,y)=dot(x,y)=xy^T=\\sum_{j=1}^{m}=x_j\\cdot y_j$$其中，\\(x_j\\)是向量\\(x\\)的第\\(j\\)个元素。在现实应用中，维数m通常较高，并且向量较为稀疏(向量中数值为0的元素相当多)。因此通常使用\\((j,x[j])\\)的集合来表示一个向量\\(x\\),并有\\(x[j]&gt;0,j=1…m\\)。在非数据流环境下，我们指定一个向量数据集\\(D=\\{x_1,…,x_n\\}\\)，同L2AP论文一致，我们使用\\(x^{’}=x^{’}_p = &lt; x_1,…,x_p,0,…,0 &gt; \\)来表示向量\\(x\\)的前缀，并使用\\(vm_x\\)来表示其中向量\\(x\\)所有元素的最大值，使用\\(\\sum_{x}=\\sum_{j}x_j\\)表示向量\\(x\\)所有元素之和，使用\\(|x|\\)来表示向量\\(x\\)的大小或者非零元素的个数（注意与向量的长度\\(||x||\\)之间的区别），使用\\(m_j\\)表示集合\\(D\\)中所有向量第\\(j\\)个元素的最大值，所有的\\(m_j\\)组合成向量\\(m\\)。在标准的all-pairs 相似查询问题(相似性自连接)中，给定一个向量集合和一个相似度阈值\\(\\theta\\),目标是找出所有的向量对\\((x,y)\\)满足\\(sim(x,y)\\geq\\theta\\)。在数据流环境下，每一个数据项被标记有其到达时间\\(t(x)\\)，则数据流可以表示为\\(S = &lt; …,(x_i,t(x_i)),(x_{i+1},t(x_{j+1})),… &gt; \\)。因此，我们在定义两个向量的相似度时不仅要考虑点积，还要考虑它们到达的时间之差\\(\\triangle t_{xy}=|t(x)-t(y)|\\)。则给定两个标有时间戳的向量\\(x\\)和\\(y\\),则它们的时间依赖相似度为$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}$$ 其中\\(\\lambda\\)是一个时间衰减参数。当\\(\\triangle t_{xy}=0\\)或者\\(\\lambda=0\\)时，时间依赖相似性回归到标准的相似性计算；当\\(\\triangle t_{xy}\\)趋于无穷大时，相似度为0。综上，我们可以给出SSSJ的问题定义，如下所示。 给定具有时间戳的向量流\\(S\\)，相似性阈值\\(\\theta\\)以及时间衰减因子\\(\\lambda\\),输出所有的向量对\\(x,y)\\)，满足\\(sim(x,y)\\geq\\theta\\)。 此外，根据向量的第二范式可知，\\(dot(x,y)\\leq 1\\),则有$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}\\leq e^{-\\lambda|t(x)-t(y)|}$$ 因此，\\(\\triangle t_{xy}\\geq \\lambda ^{-1}log \\theta ^{-1}\\)表明\\(sim_{\\triangle t}(x,y)&lt;\\theta\\)，意味着对于给定的向量，不可能与在$$\\Gamma=\\lambda ^{-1}log \\theta ^{-1}$$ 个时间单位之前到达的向量相似。相应地，我们可以将“比\\(\\Gamma\\)旧”的向量删除，并称\\(\\Gamma\\)为时间期限。 Overview of the Approach 方法的高层次概述本文提出了两个算法框架，MB-IDX和STR-IDX，其中IDX是在静态数据集上解决all-pairs相似查询问题的索引技术。为了使算法框架的阐述更加清晰，我们首先回顾一下该索引技术的概述。所有的索引技术均是基于倒排索引设计的，为m个列表的集合\\(I = \\{I_1,I_2,…,I_m\\}\\)，每一个列表\\(I_j\\)由序列对\\((\\iota (x),x_j)\\)组成，其中\\(x_j\\)是向量\\(x\\)中第\\(j\\)个元素，且\\(x_j \\neq 0\\)；\\(\\iota (x)\\)是指向向量\\(x\\)的引用。所有的索引技术均是在检索相似对的同时逐个创建索引的。具体来讲，初始化时定义一个空索引，迭代地处理数据集\\(D\\)中的向量。对于每一个最新处理的向量\\(x\\)，我们从索引中检索已经存在的且与\\(x\\)相似的向量\\(y\\)，输出相似对\\((x,y)\\)。接着，将向量\\(x\\)中某些不为0的元素插入到索引中。创建索引与检索相似对的过程可以归纳为以下三个步骤： index construction(IC)：向索引列表\\(I\\)中增加新的向量； candidate generation(CG)：使用索引列表来生成潜在的相似配对项； candidate verification(CV)：计算所有潜在配对项之间的相似度，输出超过阈值的数据对。 针对这三项步骤，本文提出了三个原语操作，分为是： \\((I,P) \\leftarrow IndConstr-IDX(D,\\theta)\\)：给定向量数据集\\(D\\)，相似度阈值\\(\\theta\\)，IndConstr-IDX返回结果集\\(P=\\{(x,y)\\}\\)；同时，IndConstr-IDX创建索引列表\\(I\\)。 \\(C \\leftarrow CandGen-IDX(I,x,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)和相似度阈值\\(\\theta\\)，CandGen-IDX返回与向量\\(x\\)潜在配对的向量集合\\(C=\\{y\\}\\)。 \\(P \\leftarrow CandVer-IDX(I,x,C,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)，潜在向量集合\\(C\\)和相似度阈值\\(\\theta\\)，CandVer-IDX返回符合要求的相似度配对项集合\\(P=\\{(x,y)\\}\\)。 MB-IDX与STR-IDX均依赖于IDX索引，并通过增加时间过滤因子使得该索引适用于数据流环境。这两个算法框架的区别在于时间过滤因子是如何在索引中调整与设置的。MB-IDX将IDX视为黑盒，使用时间过滤因子来创建IDX的独立实例对象，并在其失效时进行丢弃；相反地，STR-IDX直接应用时间过滤因子，适时地调整索引。关于STR-IDX的处理过程会在下文进行阐述，这里先介绍MB-IDX的处理过程。MB-IDX在时间间隔\\(\\Gamma\\)内运行，在第\\(k\\)个时间间隔里，从数据流中读取向量，并将它们缓存在\\(W\\)中。在这个时间间隔的末尾，调用IndConstr-IDX来检索\\(W\\)中所有的相似对，并对\\(W\\)创建索引列表\\(I\\)。在第\\(k+1\\)个时间间隔内，重置缓存\\(W\\)，重新从数据流中读取向量。此时MB-IDX针对最新读取的向量\\(x\\)查询索引列表\\(I\\)，找出前一个时间间隔到达的并与\\(x\\)相似的向量。相似对的计算通过调用CandGen-IDX和CandVer-IDX实现。在第\\(k+1\\)个时间间隔的末尾，重置索引列表\\(I\\)，并在该时间间隔到达的所有向量之间查询出相似对。下图给出MB-IDX的伪代码。 Filtering Framework 过滤框架对于每一种索引机制，我们描述它的三个基本原语(IC,CG,CV)，并讨论如何对其进行调整以适用于数据流(见下文的STR-IDX算法)。为了使文章自成一体，接下来的每一种索引，我们先展示其在静态数据集上的操作过程，然后再阐述在MB的使用和STR框架中的调整。 Inverted Index 倒排索引最简单的是不包含“优化删减索引项”的倒排索引。在所有的索引机制中，最直观的是如果两个向量相似，则它们必须至少有一个共同的坐标。因此两个相似的向量可在某些索引列表\\(I_j\\)中存在。在IC操作中，对于每一个新读取的向量\\(x\\)，需要将所有的坐标元素插入到索引中。在CG操作中，我们只用索引列表\\(I\\)检索与向量\\(x\\)相似的潜在向量。特别地，这些潜在向量\\(y\\)所在的索引列表中必定存在向量\\(x\\)的非0坐标元素。在CV操作中，输出相似度大于阈值的。 MB framework(MB-INV)三个操作IndConstr-INV、CandGen-INV和CandVer-INV直接按照上述的过程实现，这里就不细节讨论。 STR framework(STR-INV)关于INV索引的阐述是用于解决静态数据集的相似自连接问题的，这里我们考虑流式数据的相关性自连接。我们在将向量\\(x\\)的坐标元素按照向量的到达顺序插入到索引列表中。则索引列表\\(I_j\\)按照时间戳\\(t(x)\\)对索引项\\((\\iota (x),x_j)\\)进行排序。在列表中就时间维持顺序较为容易。则我们也按照相同的顺序处理数据，每次操作时只需要将向量的坐标元素插入到列表的尾部即可。 All-pairs Indexing Scheme all-pairs索引机制Bayard在INV的基础上作出了改进，提出了AP算法，减少了索引列表的大小：无需对向量\\(x\\)中的所有坐标元素建立索引，只需要保证索引列表中存在至少一个向量\\(y\\)的坐标元素与\\(x\\)的相同。类似于INV，AP在处理一个向量时逐步建立索引列表。如以下算法InConstr-AP所示，处理向量\\(x\\)时，按照事先预先指定的顺序扫描。这里维护了一个pscore变量，用于表示向量\\(x\\)的前缀与数据集中任意一个变量相似度的上界。AP使用向量\\(m\\)，即数据集的所有维的最大坐标元素值组成的向量，来计算这个上界。只要pscore小于阈值\\(\\theta\\)，则目前扫描到的向量\\(x\\)坐标值可以不加入索引列表，并且不用担心造成相似配对的遗失。一旦pscore超过阈值，则将向量\\(x\\)的剩余坐标值全部添加进索引中，而前缀\\(x^{’}\\)则添加进未加索引集合R中。 潜在配对项的生成算法如以下CandGen-AP所示，为与向量\\(x\\)相似的向量\\(y\\)的大小（非0元素个数）指定一个下界\\(sz_1\\)。如果向量\\(y\\)的大小小于这个下界，则该向量不可能与向量\\(x\\)相似，如算法第7行所示。此外，还维护了一个变量\\(rs_1\\),用于指定向量\\(x\\)和向量\\(y\\)相似度的上界。当该上界小于阈值时，停止将新的向量加入潜在配对项映射表C中，如算法第8行所示。映射表C存储了潜在配对的向量以及已扫描的部分点积值。 最后，判别最终符合要求的相似配对项操作如算法CandVer-AP所示，使用已计算的部分相似度(C中存储的值)与没有加入索引的前缀计算得到最终的相似度。 由于数据流形式的AP算法，包括MB和STR，性能均较低，所以这里我们不深入讨论。 L2-based Indexing Scheme L2AP索引机制L2AP是由Anastasiu和Karypis两人提出的、目前最优的解决all-pairs相似性自连接的算法。该算法在AP的基础上使用了更为严密的\\(l_2-bound\\)界限，不仅可以减少索引列表的大小，还可以减少潜在配对项的数量和fully-computed similarity的数量。L2AP主要借助柯西不等式，得到\\(dot(x,y) \\leq ||x|| \\cdot ||y||\\)。这个不等式同样适用于向量的前缀。我们假设向量已归一化，即\\(||y||=1\\),则有$$dot(x^{’},y) \\leq ||x^{’}|| \\cdot ||y|| \\leq ||x^{’}||$$ 另外，当对向量\\(x\\)建立索引时，L2AP对pscore的值进行存储，以\\(\\iota (x)\\)为键将pscore保存在映射表Q中，见算法第15行；同时，L2AP也存储了向量\\(x\\)前缀的长度，如算法第16行所示的索引列表中的三元数据项。Q和\\(||x_j||\\)这两个额外信息均存储在倒排表中，在算法CendVer-L2AP中用于减少潜在的配对向量数量。 在算法CendVer-L2AP中，给定一个向量\\(x\\),我们从后往前扫描它的元素，并累计它后缀的相似值。我们定义一个变量remsore，用于维护向量\\(x\\)的剩余部分的相似值。remscore结合AP中的\\(rs_1\\)和使用存储在倒排表中的\\(||y_j^{’}||\\)的\\(rs_2\\),表示当前处理向量的前缀与倒排索引中的任意一个向量之间相似度的上界。关于L2AP算法的更多细节可以参考原论文。 Improved L2-Based Indexing Scheme 改进的L2索引机制本文在L2AP的基础上进行调整，提出了适用于数据流环境的L2算法。L2AP结合了许多不同的界限：从算法AP中继承的界限(比如IC中的\\(b_1\\)和CG中的\\(rs_1\\))，新提出的基于\\(l_2\\)范式的界限(比如IC中的\\(b_2\\)和CG中的\\(rs_2\\))。我们可以观察到基于\\(l_2\\)范式的界限比AP中的界限更为有效，在大多情况下更为严密，此结论可在Anastasiu和Karypis的论文实验部分得到验证。除此之外，AP算法在索引中使用到了静态数据的统计信息，而L2AP算法只依赖于当前被建立索引的向量。这表明在使用\\(l_2\\)范式的界限时，不需要维持向量\\(m(t)\\)，因此也不需要重新建立索引。因此L2算法只使用\\(l_2\\)范式的界限，丢弃AP中的界限。 为了阐述在数据流环境下的运行，我们需要引入额外的符号定义。首先此处的输入数据是以向量的数据流\\(S\\)，其次最重要的变动是向量\\(m\\)，它的坐标元素将随着时间的推移发生改变。以下三个操作是STR框架下的L2算法使用，主回路是IndConstr-L2-STR算法。","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://tankcat2.com/categories/论文阅读/"}],"tags":[{"name":"similarity search","slug":"similarity-search","permalink":"http://tankcat2.com/tags/similarity-search/"},{"name":"stream processing","slug":"stream-processing","permalink":"http://tankcat2.com/tags/stream-processing/"},{"name":"data mining","slug":"data-mining","permalink":"http://tankcat2.com/tags/data-mining/"}]},{"title":"Leiningen安装","slug":"lein","date":"2016-07-18T05:09:31.000Z","updated":"2016-09-16T03:38:40.000Z","comments":true,"path":"2016/07/18/lein/","link":"","permalink":"http://tankcat2.com/2016/07/18/lein/","excerpt":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下：","text":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下： Make sure you have a Java JDK version 6 or later. Download the lein script from the stable branch of this project. Place it on your $PATH. (~/bin is a good choice if it is on your path.) Set it to be executable. (chmod 755 ~/bin/lein) Run it. 这段安装教程是摘录自Leiningen，翻译如下： 安装Java JDK，确保其版本是6.0以及6.0之后的； 下载lein脚本 在/bin目录下新建一个名为lein的文件，将上述脚本拷贝进去并保存； 运行命令以下命令使得lein为可执行文件: 1$ chmod 755 ~/bin/lein 运行lein，下载leiningen-xxx-standalone.jar(xxx为版本) 1$ lein 由于网络连接的问题，下载过程中可能出现错误，比如：1It&apos;s also possible that you&apos;re behind a firewall and haven&apos;t set HTTP_PROXY and HTTPS_PROXY. 解决方案：删除掉~/.lein目录后，执行export HTTP_CLIENT=&quot;wget --no-check-certificate -O&quot;，然后重新执行lein即可。","categories":[{"name":"环境部署","slug":"环境部署","permalink":"http://tankcat2.com/categories/环境部署/"}],"tags":[{"name":"leiningen","slug":"leiningen","permalink":"http://tankcat2.com/tags/leiningen/"}]},{"title":"Storm组件和拓扑结构","slug":"storm component","date":"2016-07-16T02:06:31.000Z","updated":"2016-09-16T03:38:18.000Z","comments":true,"path":"2016/07/16/storm component/","link":"","permalink":"http://tankcat2.com/2016/07/16/storm component/","excerpt":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。","text":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。 Storm总体结构 Storm的术语比较多，本文涉及的有Spout，Bolt，Stream，Tuple，Stream Grouping，Topology。下面简单介绍一下。 Tuple：包含了一个或者多个键值对的列表。默认情况下，元组tuple中的域可以是整型(integer)等基本类型对象。也可以通过定义可序列化(实现Serializable接口)的对象来实现自定义的元组类型。 Stream：Storm中最核心的概念，在分布式环境下并行创建、处理的元祖Tuple序列。在声明数据流的时候要给定一个有效的id，若不显示指定，则系统默认会给数据流定义一个名为”default”的id。 Spout：消息生产者，是数据流的数据来源，充当一个采集器的角色，连接到外部的数据源，并将数据转化为一个个tuple。 Bolt：消息处理者，封装了数据处理逻辑，将一个或者多个数据流作为输入，对数据实施运算后，选择性地输出一个或者多个数据流。几种典型的功能有数据过滤、连接、聚合和数据库读写等。 Stream Grouping：定义了消息分发策略，即定义了一个数据流中的tuple如何分发给topology中的不同bolt的任务实例。 Topology：用于封装一个实时计算应用程序的逻辑，类似于Hadoop中的Job。由Stream Grouping连接起来的Spout和Bolt的有向无环图，处理的是源源不断的消息流。 Storm编程基础 Storm的源码共分为三层，分为如下： Storm最上层的所有接口均是用Java定义的。 上层绝大多数接口的逻辑是用Clojure实现的。 最底层的数据结构是用Thrift定义的。Thrift是Apache下面的跨语言框架，它可以基于Thrift定义文件产生不同语言的代码，有关详细内容可以参考thrift。 本文主要就上层Java接口介绍Storm的拓扑和组件结构。 拓扑Topology 首先，从最外层开始，举一个如何创建topology的例子。代码中的对象下面会一一介绍。123456789TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"spout1\", new Spout1(), 1);builder.setSpout(\"spout2\", new Spout2(), 5);builder.setBolt(\"bolt\", new Bolt1(), 3) .directGrouping(\"spout1\", \"stream1\") .shuffleGrouping(\"spout2\");Config conf = new Config();conf.setDebug(true);StormSubmitter.submitTopology(\"my-topology\", conf, builder.createTopology()); TopologyBuilder第一行代码定义了一个TopologyBuilder对象。TopologyBuilder是一个工具类，用于构造topology。Topology最底层实际上是Thrift的一个数据结构，分为一下2个部分： StormTopology定义了Topology的组成，包括Spout和Bolt，每个Spout或者Bolt都有全局唯一的id。目前为止，StateSpoutSpec还没有在设计代码中用到。 12345struct StormTopology&#123; 1: required map&lt;string, SpoutSpec&gt; spouts; 2: required map&lt;string, Bolt&gt; bolts; 3: required map&lt;string, StateSpoutSpec&gt; state_spouts;&#125; TopologySummary定义了用户提交的Topology的基本情况，例如该topology分布在几个工作进程上，使用了多少个线程，有多少个任务实例。这些数据主要供Nimbus使用，以返回UI请求的数据。 123456789struct TopologySummary&#123; 1: required string id; 2: required string name; 3: required i32 num_tasks; 4: required i32 num_executors; 5: required i32 num_workers; 6: required i32 uptime_secs; 7: required string status;&#125; 由于Topology在Thrift中过于描述化的特性不便于直接使用，所以TopologyBuilder进行了上层的封装，提供了更加方便的构建方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public class TopologyBuilder &#123; private Map&lt;String, IRichBolt&gt; _bolts = new HashMap&lt;&gt;(); private Map&lt;String, IRichSpout&gt; _spouts = new HashMap&lt;&gt;(); private Map&lt;String, ComponentCommon&gt; _commons = new HashMap&lt;&gt;(); private Map&lt;String, StateSpoutSpec&gt; _stateSpouts = new HashMap&lt;&gt;(); ... public StormTopology createTopology() &#123; Map&lt;String, Bolt&gt; boltSpecs = new HashMap&lt;&gt;(); Map&lt;String, SpoutSpec&gt; spoutSpecs = new HashMap&lt;&gt;(); maybeAddCheckpointSpout(); for(String boltId: _bolts.keySet()) &#123; IRichBolt bolt = _bolts.get(boltId); bolt = maybeAddCheckpointTupleForwarder(bolt); ComponentCommon common = getComponentCommon(boltId, bolt); boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common)); &#125; for(String spoutId: _spouts.keySet()) &#123; IRichSpout spout = _spouts.get(spoutId); ComponentCommon common = getComponentCommon(spoutId, spout); spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common)); &#125; StormTopology stormTopology = new StormTopology(spoutSpecs, boltSpecs,new HashMap&lt;String, StateSpoutSpec&gt;()); stormTopology.set_worker_hooks(_workerHooks); return stormTopology; &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt, Number parallelism_hint)&#123; validateUnusedId(id); initCommon(id, bolt, parallelism_hint); _bolts.put(id, bolt); return new BoltGetter(id); &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125; ... public SpoutDeclarer setSpout(String id, IRichSpout spout, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, spout, parallelism_hint); _spouts.put(id, spout); return new SpoutGetter(id); &#125; public SpoutDeclarer setSpout(String id, IRichSpout spout)&#123; return setSpout(id, spout, null); &#125; ... private void validateUnusedId(String id) &#123; if(_bolts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Bolt has already been declared for id \" + id); &#125; if(_spouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Spout has already been declared for id \" + id); &#125; if(_stateSpouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"State spout has already been declared for id \" + id); &#125; &#125; private ComponentCommon getComponentCommon(String id, IComponent component) &#123; ComponentCommon ret = new ComponentCommon(_commons.get(id)); OutputFieldsGetter getter = new OutputFieldsGetter(); component.declareOutputFields(getter); ret.set_streams(getter.getFieldsDeclaration()); return ret; &#125; private void initCommon(String id, IComponent component, Number parallelism) throws IllegalArgumentException &#123; ComponentCommon common = new ComponentCommon(); common.set_inputs(new HashMap&lt;GlobalStreamId, Grouping&gt;()); if(parallelism!=null) &#123; int dop = parallelism.intValue(); if(dop &lt; 1) &#123; throw new IllegalArgumentException(\"Parallelism must be positive.\"); &#125; common.set_parallelism_hint(dop); &#125; Map conf = component.getComponentConfiguration(); if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf)); _commons.put(id, common); &#125; ... 成员变量_bolts包含了所有的Bolt对象，_spouts包含了所有的spout对象，它们的类型分别是IRichBolt和IRichSpout类型，关于组件的接口下文会慢慢介绍。 spout的thrift底层结构为SpoutSpec，包含了两个成员，一个是实现消息生产者具体逻辑的spout_object对象，另一个是用来描述其输入输出的common对象，具体定义如下： 1234struct SpoutSpec&#123; 1: required ComponentObject spout_object; 2: required ComponentCommon common;&#125; bolt的thrift底层结构为Bolt，结构与SpoutSpec是一致的，这里不再阐述。 _commons包含了所有的Bolt和Spout对象，它在Thrift中的数据结构是ComponentCommon: 123456struct ComponentCommon&#123; 1: required map&lt;GlobalStreamId, Grouping&gt; inputs; 2: required map&lt;string, StreamInfo&gt; streams; 3: optimal i32 parallelism_hint; 4: optimal string json_conf;&#125; ComponentCommon是用来表示Topology的基础组件对象,inputs表示该组件将从哪些GlobalStreamId以何种方式接收数据，其中GlobalStreamId是某个组件上定义的一条流，数据结构如下： 1234struct GlobalStreamId&#123; 1: required string componentId; 2: required string streamId;&#125; GlobalStreamId有两个域，componentId表示该流属于哪一个组件，streamId是流的标识。不同组件之间可以使用相同的streamId。分组方式Grouping决定了组件所发送的消息将以何种方式发送到接收端,Grouping被定义为union类型，即表示节点之间只能采取一种分组方式，其结构定义如下： 12345678910union Grouping&#123; 1: list&lt;string&gt; fields //若为空，则采取global grouping的分组方式; 2: NullStruct shuffle; //随机分组 3: NullStruct all; //全分组，即广播 4: NullStruct none; //无分组，全部发送到同一个任务实例上 5: NullStruct direct; //直接分组，直接发送到指定的任务实例上 6: JavaObject custom_object; 7: binary custom_serialized; 8: NullStruct local_or_shuffle;&#125; ComponentCommon中的streams指定了该组件需要输出的流，它给定了streamId以及StreamInfo。StreamInfo中定义了输出流的字段名列表以及该流的消息分组是否是直接分组方式，其结构定义如下： 1234struct StreamInfo&#123; 1: required list&lt;string&gt; output_fields; 2: required bool direct;&#125; ComponentCommon中的parallelism_hint表示组件的并行度，即有多少个线程，这些线程可分布在不同的进程空间或者机器中，默认值是１。 ComponentCommon中的json_conf保存了与该组件相关的一些设置。 第29~46行定义了setBolt()方法以及其重载。该方法主要功能是定义topology中的bolt对象，并指定其并行度。该方法在运行的过程中首先会通过方法validateUnusedId()检测输入的组件ID是否是唯一的，其次调用initCommon()方法为该组件构建一个ComponentCommon对象，并且只初始化其并行度和配置信息，配置信息被序列化成JSON的形式。setBolt()最后会返回一个BoltGetter对象，将利用其为bolt对象添加输入流信息。关于这个对象后文再具体介绍。 第50~59行定义了setSpout()方法以及其重载。该方法类似于setBolt()方法，也会产生ComponentCommon对象。 第63~73行定义了validateUnusedId()方法，用于检测输入的组件Id是否是唯一的，若不是则抛出异常。 第75~81行定义了getComponentCommon()方法，该方法是在createTopology()创建拓扑的时候被调用的，设置了组件的输出流信息。 第83~96行定义了initCommon()方法，主要是对ComponentCommon对象进行初始化，设置并行度和配置信息。 最后一个也是最为重要的，第8~27行定义了createTopology()方法，根据输入的Bolt和Spout对象创建Topology对象。从第16行和第21行可以看出，在创建拓扑的过程中，Bolt和Spout均为对象序列化后得到的字节数组。 组件ComponentsBolt接口再回头看创建topology的简单示例，第4行设置bolt的方法中构建了一个类Bolt1的对象实例。该类是一个自定义的Bolt类，可以是实现了Storm定义的Bolt接口，主要有IBolt，IRichBolt，IBasicBolt和IBatchBolt，它们之间的关系如下图所示： IComponent接口IComponent是通用的组件接口，所有的Bolt和Spout都会实现这个接口，其代码如下：1234public interface IComponent extends Serializable&#123; void declareOutputFields(OutputFieldsDeclarer declarer); Map&lt;String, Object&gt; getComponentConfiguration();&#125; 其中，declareOutputFields()的参数为OutputFieldsDeclarer接口，定义了拓扑中每个组件的输出字段声明，每个组件都需要它来指定输出到哪些流、声明输出的字段列表以及指出输出流是否是直接流，代码如下：123456public interface OutputFieldsDeclarer&#123; public void delcare(Fields fields); public void delcare(boolean direct, Fields fields); public void delcareStream(String streamId,Fields fields); public void declareStream(String streamId,boolean direct,Fields fields);&#125; 其中，第5~6行没有明确指定输出流的id，默认使用的是storm的default流。所有的方法中都有一个Fields对象参数，该类用于存储消息的字段名列表，比如一条学生个人信息的scheme为(“stu_name”,”stu_number”,”stu_age”,”stu_sex”)，其所需参数是字段名集合。对于同一条消息，在构建Fields对象时会为其所有的字段建立索引。它的代码定义如下：1234567891011121314151617181920212223242526public class Fields implements Iterable&lt;String&gt;,Serializable&#123; private List&lt;String&gt; _fields; private Map&lt;String,Integer&gt; _index = new HashMap&lt;String,Integer&gt;(); public Fields(String... fields)&#123; this(Arrays.asList(fields)); &#125; public Fields(List&lt;String&gt; fields)&#123; _fields= new ArrayList&lt;String&gt;(fields.size()); for(String field:fields)&#123; if(_fields.contains(field))&#123; throw new IllegalArgumentException(String.format(\"duplicate field '%s'\", field)); &#125; _fields.add(field); &#125; index(); &#125; private void index()&#123; for(int i=0;i&lt;_fields.size();i++)&#123; _index.put(_fields.get(i),i); &#125; &#125; ...&#125; Fields实现了Iterable&lt;String&gt;接口，表明Fields可以遍历存储的字段名列表；也实现了Serializable接口，表明Fields可以被序列化。第2~3行定义了一个保存所有字段名的列表以及一个保存了从字段名到它在字段名列表中位置的映射表。第5~7行的构造函数接收一个可变参数fields，将其转换为列表好调用第9~19行定义的构造函数。第9~19行定义的构造函数首先会检查传入的字段名列表中是否存在重复的字段名，并保存该字段名列表，最后调用index()方法为该字段名列表建立索引。 IBolt接口Bolt是Storm中的基础运行单位，当接收到一条数据时可以不立刻对其进行处理，可以先保存后处理。其生命周期如下： 创建提交Topology时创建IBolt实例并进行序列化操作(见createTopology())； 将序列化的Bolt组件发送给集群中的主节点； 主节点启动工作进程，并在进程中反序列化Bolt组件； 在开始执行任务之前，先调用Bolt的prepare()回调方法进行初始化，然后再具体处理接收的数据。 IBolt的具体代码如下：12345public interface IBolt extends Serializable&#123; void prepare(Map stormConf, TopologyContext context, OutputCollector collector); void execute(Tuple input); void cleanup();&#125; 在实现Bolt的过程中，用户可以编写其构造函数，然而构造函数并不会被实际调用，因为在提交Topology时，系统会调用Topology的构造函数，并将产生的对象序列化成字节数组。每一个节点上的Bolt都是通过反序列化的方式得到的，这可能导致某些成员没有被正确的初始化，因此用户应该将复杂对象的初始化放在prepare()回调方法中。第三个参数是一个OutputCollector类对象，它是Bolt的输出收集器，Bolt处理好的消息都是通过输出收集器发送出去的，不同类型的输出收集器也不同，这里先讲一下IRichBolt的输出收集器，它实现了IOutputCollector接口，是一个代理类。IOutputCollector接口如下：。emit()方法用来向外发送数据，它的返回值是该消息所发送目标的TaskId集合，其输入参数分布是消息将被输出的流Id，输出的消息标记(通常代表该条消息从哪些消息产生的)以及要输出的消息。emitDirect()与emit()类似，主要区别在于它发送的消息只有指定的任务实例才能接收。这个方法要求streamId对应的流必须被定义为直接流。如果下游节点没有接收到该消息，那么此类消息其实并没有真正发送。fail()和ack()用来表示消息是否被真正处理。OutputCollector是IOutputCollector的默认实现类，它实际上是一个代理，包含一个真正工作的IOutputCollector实例，这个对象是在Clojure中定义的。OutputCollector中提供了许多重载方法供用户使用，具体定义可参照OutputCollector。 123456public interface IOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void ack(Tuple input); void fail(Tuple input);&#125; 对象在被销毁时，将调用cleanup()回调方法，但是Storm并不保证该方法一定被执行，只有当在本地模式下运行时杀死topology该方法才保证一定能被执行。 execute()方法实现对输入消息的处理。其参数是一个Tuple实例。Tuple是Storm中的主要数据结构，在发送接收消息的过程中，每一条消息实际上都是一个Tuple对象。其接口代码如下：第5~14行的方法用于获取由参数i指定的字段位置的值，如果用户知道该字段对应的类型，就可以调用对应类型的获取方法获取字段的值。第15~24行与之类似，不过这里的方法是根据字段名获取相应的值。123456789101112131415161718192021222324252627282930313233public interface ITuple &#123; public int size(); public int fieldIndex(String field); public boolean contains(String field); public Object getValue(int i); public String getString(int i); public Integer getInteger(int i); public Long getLong(int i); public Boolean getBoolean(int i); public Short getShort(int i); public Byte getByte(int i); public Double getDouble(int i); public Float getFloat(int i); public byte[] getBinary(int i); public Object getValueByField(String field); public String getStringByField(String field); public Integer getIntegerByField(String field); public Long getLongByField(String field); public Boolean getBooleanByField(String field); public Short getShortByField(String field); public Byte getByteByField(String field); public Double getDoubleByField(String field); public Float getFloatByField(String field); public byte[] getBinaryByField(String field); public Fields getFields(); public List&lt;Object&gt; select(Fields selector); public List&lt;Object&gt; getValues(); public GlobalStreamId getSourceGlobalStreamId(); public String getSourceComponent(); public int getSourceTask(); public String getSourceStreamId(); public MessageId getMessageId();&#125; IRichBolt接口IRichBolt同时实现了IComponent和IBolt接口，其含义是一个具有Bolt功能的组件。在实际使用中，IRichBolt是实现Topology组件的主要接口，其定义如下：123public interface IRichBolt extends IBolt,IComponent&#123; &#125; IBasicBolt接口IBasicBolt接口的定义与IBolt基本一致，具体实现要求也与IBolt相同，主要区别为一下两点： 它的输出收集器使用的是BasicOutputCollector，并且该参数被放在了execute方法中而不是prepare中； 它实现了IComponent接口，表明它可以用来定义Topology组件。首先我们来看一下BasicOutputCollector,它是Storm提供的IBasicOutputCollector接口的默认实现。我们看一下该接口的定义。12345public interface IBasicOutputCollector&#123; List&lt;Integer&gt; emit(String streamId,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,List&lt;Object&gt; tuple); void reportError(Throwable t);&#125; 对比IOutputCollector可以看出两者的区别： IBasicOutputCollector中没有ack和fail方法； IBasicOutputCollector的emit和emitDirect方法中没有anchor参数。这样设计的原因是如果使用了IBasicBolt，Storm框架会自动帮用户进行Ack、Fail和Anchor操作，用户自己不需要关心这一点。所以为了确保这种机制能正常运行，避免用户在使用时出错，Storm提供了简化版的IBasicOutputCollector。BasicOutputCollector收集器实际上是OutputCollector的封装类，其中包含了一个OutputCollector类型的成员变量，实际上所有的消息最终都将由这个OutputCollector进行处理。 再回到IBasicBolt，之所以设计这个接口上文已有解释，Storm框架本身帮用户处理了所发出消息的Ack、Fail和Anchor操作，是由执行器BasicBoltExecutor实现的。该类实现了IRichBolt接口，同时还包含了一个IBasicBolt成员变量用于调用的转发，它是基于装饰模式的，定义如下：12345678910111213141516171819202122232425262728293031323334public class BasicBoltExecutor implements IRichBolt &#123; public static final Logger LOG = LoggerFactory.getLogger(BasicBoltExecutor.class); private IBasicBolt _bolt; private transient BasicOutputCollector _collector; public BasicBoltExecutor(IBasicBolt bolt) &#123; _bolt = bolt; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; _bolt.declareOutputFields(declarer); &#125; public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; _bolt.prepare(stormConf, context); _collector = new BasicOutputCollector(collector); &#125; public void execute(Tuple input) &#123; _collector.setContext(input); try &#123; _bolt.execute(input, _collector); _collector.getOutputter().ack(input); &#125; catch(FailedException e) &#123; if(e instanceof ReportedFailedException) &#123; _collector.reportError(e); &#125; _collector.getOutputter().fail(input); &#125; &#125; public void cleanup() &#123; _bolt.cleanup(); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return _bolt.getComponentConfiguration(); &#125;&#125; 第3行定义了成员变量_bolt，实现了IBasicBolt接口；第17行设置执行器运行的上下文，它表示经execute方法发送出去的消息都是由输出消息产生的，即输出消息都将标记为输入消息所衍生出来的消息，这是使用IBasicBolt消息跟踪的重要环节。第20行对输入的消息进行Ack操作。这一步意味着基于当前输入消息的处理以及衍生消息的发送已经完成，此时可以对该消息进行Ack操作了。用户实现了IBasicBolt接口的Bolt对象之后，在构建Topology时，Storm会调用TopologyBuilder的setBolt方法设置该Bolt对象。setBolt方法会用BasicBoltExecutor封装用户的实现类，这是Storm自动帮用户完成的，而且它还会调用可接收IRichBolt参数的重载方法完成Bolt对象的设置。这也解释了BasicBoltExecutor需要实现IRichBolt接口的原因。 IBatchBolt接口区别与IBasicBolt接口，IBatchBolt主要用于Storm中的批处理。Storm的事务Topology以及Trident主要是基于IBatchBolt的。相比前面的IBolt、IRichBolt、IBasicBolt，IBatchBolt中多了一个finishBatch()方法，它在一个批处理结束时被调用。此外，IBatchBolt还去除了cleanup()方法，其接口定义如下：12345public interface IBatchBolt&lt;T&gt; extends Serializable,IComponent&#123; void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, T id); void execute(Tuple input); void finishBatch();&#125; prepare()方法用来初始化一个Batch，最后一个参数是通用类型T，它可以用作该Batch的唯一标识。在目前的Storm实现中，每个事务都会对应一个Batch，而每个Batch的数据都会由一个新创建的IBatchBolt对象进行处理，当一个Batch被成功处理后，该Batch对应的IBatchBolt对象将被销毁，因此用户不能通过IBatchBolt对象自身存储需要在多个Batch之间进行共享的数据。第二个参数是IBatchBolt的输出收集器BatchOutputCollector,其代码定义如下：123456789public abstract class BatchOutputCollector&#123; public List&lt;Integer&gt; emit(List&lt;Object&gt; tuple); public List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple); public void emitDirect(int taskId, List&lt;Object&gt; tuple)&#123; emitDirect(taskId, Utils.DEFAULT_STREAM_ID,tuple); &#125; public abstract void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple); public abstract void reportError(Throwable error);&#125; 同IBasicOutputCollector类似，不需要自己去处理Ack、Fail和Anchor这3项操作。Storm提供了BatchOutputCollector的默认实现类BatchOutputCollectorImpl,该类是一个代理类，内部封装了OutputCollector变量，所有的方法都通过调用OutputCollector方法来实现。 finishBatch()方法仅当这批消息被完全处理之后才会被调用。 与IBasicBolt类似，使用IBatchBolt也不需要关心何时该对收到的信息进行Ack等操作，Storm框架内部通过BatchBoltExecutor自动帮我们实现了这些功能。BatchBoltExecutor也实现了IRichBolt接口，它会为每个Batch创建与之对应的BatchBolt对象。同时还实现了FinishedCallBack和TimeoutCallback接口。BatchBoltExecutor的代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class BatchBoltExecutor implements IRichBolt, FinishedCallback,TimeoutCallback&#123; public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class); byte[] _boltSet; Map&lt;Object, IBatchBolt&gt; _openTransactions; Map _conf; TopologyContext _context; BatchOutputCollector _collector; public BatchBoltExecutor(IBatchBolt bolt)&#123; _boltSet= Utils.serialize(bolt); &#125; public void prepare(Map conf, TopologyContext context, OutputCollector collector)&#123; _conf=conf; _context=context; _collector= new BatchOutputCollectorImpl(collector); _openTransactions= new HashMap&lt;Object, IBatchBolt&gt;(); &#125; public void execute(Tuple input)&#123; Object id = input.getValue(0); IBatchBolt bolt = getBatchBolt(id); try&#123; bolt.execute(input); _collector.ack(input); &#125;catch(FailedException e)&#123; LOG.error(\"Failed to process tuple in batch\",e); _collector.fail(input); &#125; &#125; public void cleanup()&#123;&#125; public void finishedId(Object id)&#123; IBatchBolt bolt = getBatchBolt(id); _openTransactions.remove(id); bolt.finishBatch(); &#125; public void timeoutId(Object id)&#123; _openTransactions.remove(id); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; newTransactionalBolt().declareOutputFields(declarer); &#125; public Map&lt;String, Object&gt; getComponentConfiguration()&#123; return new newTransactionalBolt().getComponentConfiguration(); &#125; private IBatchBolt getBatchBolt(Object id)&#123; IBatchBolt bolt = _openTransactions.get(id); if(bolt == null)&#123; bolt = new newTransactionalBolt(); bolt.prepare(_conf,_context,_collector,id); _openTransactions.put(id,bolt); &#125; return bolt; &#125; private IBatchBolt newTransactionalBolt()&#123; return (IBatchBolt)Utils.deserialize(_boltSet); &#125;&#125; 第3行是内含的BatchBolt对象的序列化字节数组。第17~27行实现了execute()方法，它规定输入消息的第一列用于标识Batch的id。第29~33行实现了FinishedCallback接口，这里调用finishBatch()方法清理BatchBolt对象。可以看出，BatchBolt对象在不同的Batch之间是不重复使用的。第34~36行实现了TimeoutCallback接口,仅仅将缓存的BatchBolt删除，这对于清理不再使用的BatchBolt对象是很关键的。第52~54行通过反序列化生成一个IBatchBolt对象。Storm通过反序列化对象的方式来弥补不断创建IBatchBolt对象所带来的负担。 Spout接口Storm中与spout相关的接口主要是ISpout和IRichSpout，下图描述了它们之间的关系： ISpout接口接口ISpout定义了Spout应该实现的功能集合:123456789public interface ISpout extends Serializable &#123; void open(Map conf, TopologyContext context, SpoutOutputCollector collector); void close(); void activate(); void deactivate(); void nextTuple(); void ack(Object msgId); void fail(Object msgId);&#125; 关于各个函数的功能，源代码中的注释部分已经给出了详细的描述，这里不再赘述。其中，nextTuple()由于和ack()、nextTuple()是在一个线程被调用的，如果nextTuple阻塞的话，其他方法也将被阻塞。因此，该方法必须是非阻塞的，任何Spout都将使用nextTuple来发送信息。 ISpout的fail和ack方法仅仅给出了发送消息时所对应的MessageId,而没有具体给出消息内容，表明如果要实现消息重传的话，用户需要自己来维护哪些已经发送的消息。 当Spout被设置为活跃或者不活跃时，会分别调用activate()和deactivate()方法将状态通知给用户代码。这样当Spout处于非活跃的状态时，nextTuple不会被调用。 open()方法的第3个参数是Spout的输出收集器SpoutOutputCollector，Storm只定义了一个Spout的输出收集器接口ISpoutOutputCollector，SpoutOutputCollector是它的默认实现类。首先看一下ISpoutOutputCollector的代码定义：emit()方法用来向外发送数据，它的返回值是该消息所有发送目标的任务实例集合。emitDirect()方法的输入列表与emit()类似，主要区别在于使用前者时，只有由参数taskId所指定的任务实例才能接收到这条消息。SpoutOutputCollector类实际上是一个代理类，本身也封装了一个ISpoutOutputCollector对象，所有的操作实际上都是通过该对象来实现的。除此之外，它还提供了一些重载方法。12345public interface ISpoutOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple, Object messageId); void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple, Object messageId); long getPendingCount();&#125; IRich接口IRichSpout需要同时实现IComponent和ISpout接口，因此它是一个具有Spout功能的组件，其定义如下：123public interface IRichSpout extends ISpout,IComponent&#123; &#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"http://tankcat2.com/categories/源码分析/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}]}]}