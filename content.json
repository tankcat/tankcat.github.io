{"meta":{"title":"Tankcat","subtitle":"I'll try anything once...","description":"Big Data, Apache Storm, Stream Processing, Join Processing","author":"Tankcat","url":"http://tankcat2.com"},"pages":[{"title":"","date":"2017-11-26T05:33:11.636Z","updated":"2016-07-10T17:29:36.000Z","comments":true,"path":"404.html","permalink":"http://tankcat2.com/404.html","excerpt":"","text":"404"},{"title":"","date":"2017-11-26T05:33:11.640Z","updated":"2017-03-16T11:25:50.000Z","comments":true,"path":"tool.js","permalink":"http://tankcat2.com/tool.js","excerpt":"","text":"\"use strict\"; var fs = require(\"fs\"); var path = \"photos/\"; fs.readdir(path, function (err, files) { if (err) { return; } var arr = []; (function iterator(index) { if (index == files.length) { fs.writeFile(\"photos/data.json\", JSON.stringify(arr, null, \"\\t\")); console.log('get img success!'); return; } fs.stat(path + files[index], function (err, stats) { if (err) { return; } if (stats.isFile()) { arr.push(files[index]); } iterator(index + 1); }) }(0)); });"},{"title":"About","date":"2017-11-26T05:33:11.645Z","updated":"2016-10-12T15:28:16.000Z","comments":true,"path":"about/index.html","permalink":"http://tankcat2.com/about/index.html","excerpt":"","text":""},{"title":"Archive","date":"2016-07-10T08:14:45.000Z","updated":"2016-07-17T06:27:18.000Z","comments":true,"path":"archive/index.html","permalink":"http://tankcat2.com/archive/index.html","excerpt":"","text":""},{"title":"Photos","date":"2016-09-16T07:07:04.000Z","updated":"2017-05-07T10:29:36.000Z","comments":true,"path":"favorite/index.html","permalink":"http://tankcat2.com/favorite/index.html","excerpt":"","text":"劝君莫惜金缕衣，劝君惜取少年时。花开堪折直须折，莫待无花空折枝。——杜秋娘《金缕衣》 ​​​​ .hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}逛街的两个人 南京市人民政府 行走在平江路 龙之梦的一家饰品店 回家 一天天长大的小叶子 胖嘟嘟的阿拉斯加 姐妹花 苏州图书馆 同德兴的拉面 星巴克的桃桃红茶 夜晚十一点的红房子 阿里家的面 上海城市规划馆 和府捞面 南京大屠杀纪念馆 沙面撸猫 广州的一家青旅 很高兴遇见你 太古汇的索尼体验店 Godiva的双色甜筒 丽娃河畔的樱花 顾村公园赏樱 不知名的紫色小花 五舍楼后的白月季 河畔芦苇 小黄花 河西食堂前的桃李 丽娃桥 依旧耀眼的落日 滴水湖的路标 家里的后院 十八舍前一抹红 云雾缭绕 南师西区操场 修剪的树枝 蝙蝠侠大战超人 缺月挂疏桐 敬文图书馆 中北的蓝天(一) 中北的蓝天(二) 广玉兰 南京97路公交 大行宫地铁站 南师晒太阳的老黄 顺和祥的汤包 爷爷的梅干菜扣肉 左庭右院的牛肉饼 南师西区食堂的炸酱汤面 $('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });"},{"title":"Categories","date":"2017-11-26T05:33:11.655Z","updated":"2016-07-17T06:27:32.000Z","comments":true,"path":"categories/index.html","permalink":"http://tankcat2.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2017-11-26T05:33:11.672Z","updated":"2017-03-16T11:25:54.000Z","comments":true,"path":"photos/data.json","permalink":"http://tankcat2.com/photos/data.json","excerpt":"","text":"[\"14098405995146380.jpg\",\"227563740595514503.jpg\",\"340809002755226955.jpg\",\"775149960737431740.jpg\",\"IMG_20151027_202853.jpg\",\"IMG_20151105_184339.jpg\",\"IMG_20151105_202323.jpg\",\"spring.jpg\"]"},{"title":"Photos","date":"2017-03-16T08:14:45.000Z","updated":"2017-03-16T13:22:48.000Z","comments":true,"path":"photos/index.html","permalink":"http://tankcat2.com/photos/index.html","excerpt":"","text":""},{"title":"Search","date":"2017-11-26T05:33:11.689Z","updated":"2016-07-17T06:27:46.000Z","comments":true,"path":"search/index.html","permalink":"http://tankcat2.com/search/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-07-10T12:26:33.000Z","updated":"2016-07-17T06:27:58.000Z","comments":true,"path":"tags/index.html","permalink":"http://tankcat2.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"十一月的第四周","slug":"diary1126","date":"2017-11-26T10:29:45.000Z","updated":"2017-11-26T10:44:10.061Z","comments":true,"path":"2017/11/26/diary1126/","link":"","permalink":"http://tankcat2.com/2017/11/26/diary1126/","excerpt":"工作日的survey这个月进行survey的第四周，工作日的五天细读了两篇微软的流处理系统paper，一篇Naiad和一篇Falkirk Wheel；一篇很经典的Dynamo，十年前就发表的这篇今年喜获SIGOPS名人堂奖，重点关注了它的高可用是如何实现的；一篇综述，其中一个作者和Naiad、Falkirk Wheel联系密切，这篇综述关注的点很基础。 算下来从开始做survey，看过的文章也有二十多篇了，关于容错，心中确实有了一点感觉，但觉得还不够，有的时候回头想想看过的文章，竟然不能一句话讲出它做了什么，看过就忘成了很大的问题。男票告诉我，这是因为我没有理解透彻。所以，现在就有点迷茫，到底什么文章该精读，精读到什么程度是够的，要想真正理解一篇文章的proposal，耗时不短，这当中该怎么做权衡呢？ 还有一点需要反省，就是效率问题，一周只看了四篇文章，很大一部分时间还是被我浪费了，经常看文章看到一半就去找人聊天、逛豆瓣微博，没有完整、高效率的学习时间，这应该也是导致文章理解不透彻的原因之一吧。 不过，有进步的是，开始定制周计划了，也切实进行了，希望在次基础上提高效率并继续保持。 周六周日的吃吃玩玩周六中午出门，骑单车去武康路的一家星巴克臻选店买工业风的杯子，哪里知道好不容易找到这家店却被告之没有存货，感觉委屈的不行。直接返程肯定是不行的，不然就白出来了，于是就在武昌路和湖南路附近逛逛，后来又去一家网红面店吃了一碗辣肉面。然而，十个网红店九个是垃圾，这家面店很荣幸也是垃圾，38块一碗的面还不如河西食堂3块一碗的阳春面。返程的时候选择步行，正好记一记路线。从武康路-兴国路-华山路-江苏路-愚园路-长宁路-凯旋路-万航渡路-光复西路-枣阳路，短短五公里串联了这么多道路，有些路很小资、适合拍照，有些路就普普通通、大众化。走到长宁路上的兆丰广场，看到有Bose专柜就想顺便试试音质，试玩就心动了，比我的大法轻多了。走之前顺便又去nitori买了一口雪平锅，想着宿舍厨房里有电磁炉，以后可以熬奶茶、煮泡面吃，可是买的时候没看清楚，今天早上发现这锅只能用煤气加热。晚上懒得回实验室了，和男票商量了一下要讨论微信小程序的需求问题，于是又背着包出门，选择在枣阳路的这家星巴克讨论。环境还是不错的，有无线，周六晚上人很少，买了一杯太妃榛果，店员还给了一些蛋糕试吃。讨论完肚子又饿了，就去蔡师傅汤包店点了一碗小馄饨和一块素鸡，美味又划算。最后，回寝室洗漱。哦对了，昨天是老爸生日，舅舅一家去家里，燕子姐姐又给爸爸买了礼物。听老妈说，爸爸这次很感动，不仅燕子姐姐送了礼物，我又送了手机，好像老爸都哭了。 周日赖了床，早上8点半才醒，准备用雪平锅煮奶茶的时候发现电磁炉不能加热，没办法只能换成电炖锅来做了。牛奶是在盒马鲜生上买的明治鲜奶，加了两包立顿红茶包，这次冰糖又放多了，不过口感还是很醇厚的，虽然比较甜，和外面比起来还是健康很多的。给实验室的小伙伴带了一点，评价都不错。中午去了实验室，忙着把Hexo个人博客需要的环境在新笔记本上重新搭建了一下，遇到了不少问题，但是都解决了。搞完已经三点了，在大众点评上选择了一家中江路那边的日料店，骑了单车过去，由于不是饭点，店里除了我没其他顾客，点了三文鱼刺身、炸土豆饼、煎饺和炸鸡肉串，很快就消灭了~ 最后这一周过得还是挺充实了，周一早上体重还下百了，激动得我发了一条朋友圈。可是下百就一天，接着几天就无所顾忌地吃，周一把剩余的牛油果继续做了奶昔喝，周二吃了冒菜，周三又跑去吃了煲宫，周五还吃了麻辣香锅和芝麻糊小圆子，周六周日前面已经说了。减肥我还是会继续的，只是不会再像一开始那么严格了，该吃吃还是要吃，该锻炼也还是会锻炼，健康的前提下保持身材。技能上， 就这样吧，明天又是新的一周，下周五轮到我讲survey，还有四天半的时间好好准备，加油吧二筒子~","text":"工作日的survey这个月进行survey的第四周，工作日的五天细读了两篇微软的流处理系统paper，一篇Naiad和一篇Falkirk Wheel；一篇很经典的Dynamo，十年前就发表的这篇今年喜获SIGOPS名人堂奖，重点关注了它的高可用是如何实现的；一篇综述，其中一个作者和Naiad、Falkirk Wheel联系密切，这篇综述关注的点很基础。 算下来从开始做survey，看过的文章也有二十多篇了，关于容错，心中确实有了一点感觉，但觉得还不够，有的时候回头想想看过的文章，竟然不能一句话讲出它做了什么，看过就忘成了很大的问题。男票告诉我，这是因为我没有理解透彻。所以，现在就有点迷茫，到底什么文章该精读，精读到什么程度是够的，要想真正理解一篇文章的proposal，耗时不短，这当中该怎么做权衡呢？ 还有一点需要反省，就是效率问题，一周只看了四篇文章，很大一部分时间还是被我浪费了，经常看文章看到一半就去找人聊天、逛豆瓣微博，没有完整、高效率的学习时间，这应该也是导致文章理解不透彻的原因之一吧。 不过，有进步的是，开始定制周计划了，也切实进行了，希望在次基础上提高效率并继续保持。 周六周日的吃吃玩玩周六中午出门，骑单车去武康路的一家星巴克臻选店买工业风的杯子，哪里知道好不容易找到这家店却被告之没有存货，感觉委屈的不行。直接返程肯定是不行的，不然就白出来了，于是就在武昌路和湖南路附近逛逛，后来又去一家网红面店吃了一碗辣肉面。然而，十个网红店九个是垃圾，这家面店很荣幸也是垃圾，38块一碗的面还不如河西食堂3块一碗的阳春面。返程的时候选择步行，正好记一记路线。从武康路-兴国路-华山路-江苏路-愚园路-长宁路-凯旋路-万航渡路-光复西路-枣阳路，短短五公里串联了这么多道路，有些路很小资、适合拍照，有些路就普普通通、大众化。走到长宁路上的兆丰广场，看到有Bose专柜就想顺便试试音质，试玩就心动了，比我的大法轻多了。走之前顺便又去nitori买了一口雪平锅，想着宿舍厨房里有电磁炉，以后可以熬奶茶、煮泡面吃，可是买的时候没看清楚，今天早上发现这锅只能用煤气加热。晚上懒得回实验室了，和男票商量了一下要讨论微信小程序的需求问题，于是又背着包出门，选择在枣阳路的这家星巴克讨论。环境还是不错的，有无线，周六晚上人很少，买了一杯太妃榛果，店员还给了一些蛋糕试吃。讨论完肚子又饿了，就去蔡师傅汤包店点了一碗小馄饨和一块素鸡，美味又划算。最后，回寝室洗漱。哦对了，昨天是老爸生日，舅舅一家去家里，燕子姐姐又给爸爸买了礼物。听老妈说，爸爸这次很感动，不仅燕子姐姐送了礼物，我又送了手机，好像老爸都哭了。 周日赖了床，早上8点半才醒，准备用雪平锅煮奶茶的时候发现电磁炉不能加热，没办法只能换成电炖锅来做了。牛奶是在盒马鲜生上买的明治鲜奶，加了两包立顿红茶包，这次冰糖又放多了，不过口感还是很醇厚的，虽然比较甜，和外面比起来还是健康很多的。给实验室的小伙伴带了一点，评价都不错。中午去了实验室，忙着把Hexo个人博客需要的环境在新笔记本上重新搭建了一下，遇到了不少问题，但是都解决了。搞完已经三点了，在大众点评上选择了一家中江路那边的日料店，骑了单车过去，由于不是饭点，店里除了我没其他顾客，点了三文鱼刺身、炸土豆饼、煎饺和炸鸡肉串，很快就消灭了~ 最后这一周过得还是挺充实了，周一早上体重还下百了，激动得我发了一条朋友圈。可是下百就一天，接着几天就无所顾忌地吃，周一把剩余的牛油果继续做了奶昔喝，周二吃了冒菜，周三又跑去吃了煲宫，周五还吃了麻辣香锅和芝麻糊小圆子，周六周日前面已经说了。减肥我还是会继续的，只是不会再像一开始那么严格了，该吃吃还是要吃，该锻炼也还是会锻炼，健康的前提下保持身材。技能上， 就这样吧，明天又是新的一周，下周五轮到我讲survey，还有四天半的时间好好准备，加油吧二筒子~","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"周记","slug":"周记","permalink":"http://tankcat2.com/tags/周记/"}]},{"title":"Martiderm安瓶使用感","slug":"Martiderm","date":"2017-11-24T01:28:31.000Z","updated":"2017-11-26T06:16:30.824Z","comments":true,"path":"2017/11/24/Martiderm/","link":"","permalink":"http://tankcat2.com/2017/11/24/Martiderm/","excerpt":"这又是一记安利~ 先交代一下本人的肤质吧： 初中开始长青春痘 高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印 前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干 目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长 总结一句话，就是混油痘肌。 半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，臻活和平衡系列各五只，我的目的很明确，祛痘印+提亮肤色。 臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用伊索的绿茶水和无油保湿精华打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。 可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~","text":"这又是一记安利~ 先交代一下本人的肤质吧： 初中开始长青春痘 高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印 前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干 目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长 总结一句话，就是混油痘肌。 半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，臻活和平衡系列各五只，我的目的很明确，祛痘印+提亮肤色。 臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用伊索的绿茶水和无油保湿精华打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。 可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"护肤","slug":"护肤","permalink":"http://tankcat2.com/tags/护肤/"},{"name":"安瓶","slug":"安瓶","permalink":"http://tankcat2.com/tags/安瓶/"},{"name":"Martiderm","slug":"Martiderm","permalink":"http://tankcat2.com/tags/Martiderm/"}]},{"title":"视频导出音频小技能","slug":"ffmped","date":"2017-11-22T02:44:51.000Z","updated":"2017-11-26T06:17:18.404Z","comments":true,"path":"2017/11/22/ffmped/","link":"","permalink":"http://tankcat2.com/2017/11/22/ffmped/","excerpt":"早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。 视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——http://audio-extractor.net/cn/，需要先上传视频，再点击转换，最后再下载音频。 视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！http://ffmpeg.org/ 这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~ 我是把MP4转换成MP3，在别人的博客里找到了下面的命令： 1ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3 以上，谢谢阅读。","text":"早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。 视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——http://audio-extractor.net/cn/，需要先上传视频，再点击转换，最后再下载音频。 视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！http://ffmpeg.org/ 这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~ 我是把MP4转换成MP3，在别人的博客里找到了下面的命令： 1ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3 以上，谢谢阅读。","categories":[{"name":"技能树","slug":"技能树","permalink":"http://tankcat2.com/categories/技能树/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://tankcat2.com/tags/ffmpeg/"},{"name":"音频","slug":"音频","permalink":"http://tankcat2.com/tags/音频/"},{"name":"视频","slug":"视频","permalink":"http://tankcat2.com/tags/视频/"}]},{"title":"鼓楼半日记","slug":"gulou","date":"2017-08-15T12:11:31.000Z","updated":"2017-08-21T14:25:40.000Z","comments":true,"path":"2017/08/15/gulou/","link":"","permalink":"http://tankcat2.com/2017/08/15/gulou/","excerpt":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","text":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[]},{"title":"Tragic Ending or Peace Ending ?","slug":"my chester","date":"2017-07-20T16:00:00.000Z","updated":"2017-07-21T01:44:52.000Z","comments":true,"path":"2017/07/21/my chester/","link":"","permalink":"http://tankcat2.com/2017/07/21/my chester/","excerpt":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","text":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"Linkin Park","slug":"Linkin-Park","permalink":"http://tankcat2.com/tags/Linkin-Park/"},{"name":"Chester Bennington","slug":"Chester-Bennington","permalink":"http://tankcat2.com/tags/Chester-Bennington/"}]},{"title":"Storm UI详解","slug":"storm_ui","date":"2017-05-22T08:32:31.000Z","updated":"2017-07-20T00:45:32.000Z","comments":true,"path":"2017/05/22/storm_ui/","link":"","permalink":"http://tankcat2.com/2017/05/22/storm_ui/","excerpt":"","text":"","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"storm","slug":"storm","permalink":"http://tankcat2.com/tags/storm/"},{"name":"storm ui","slug":"storm-ui","permalink":"http://tankcat2.com/tags/storm-ui/"}]},{"title":"Storm Kafka之KafkaSpout","slug":"KafkaSpout","date":"2017-05-18T12:11:31.000Z","updated":"2017-07-20T00:46:14.000Z","comments":true,"path":"2017/05/18/KafkaSpout/","link":"","permalink":"http://tankcat2.com/2017/05/18/KafkaSpout/","excerpt":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。","text":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。 StaticHosts。这是另一个选择，不过broker$\\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建GlobalPartitionInformation类的实例，如下： 12345678Broker brokerForPartition0 = new Broker(\"localhost\");//localhost:9092,端口号默认为9092Broker brokerForPartition1 = new Broker(\"localhost\",9092);//localhost:9092,显示地指定端口号Broker brokerForPartition2 = new Broker(\"localhost:9092\");GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();partitionInfo.addPartition(0, brokerFroPartition0);// partition0 到 brokerForPartition0的映射partitionInfo.addPartition(1, brokerFroPartition1);partitionInfo.addPartition(2, brokerFroPartition2);StaticHosts hosts = new StaticHosts(partitionInfo); KafkaConfig创建KafkaSpout需要的另一个参数是KafaConfig，通过调用以下两个函数进行对象创建： 12public KafkaConfig(BrokerHosts host,String topic)public KafkaConfig(BrokerHosts host,String topic,String clientId) 其中，host可以为BrokerHost的任何一种实现，topic是一个topic的名称，clientId是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。 目前，KafkaConfig有两种扩展形式，SpoutcConfig提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。zkRoot用于存储consumer的offset，id用于唯一标识当前的spout。 1public SpoutConfig(BrokerHosts hosts,String topic,String zkRoot,String id) 除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout： 1234567891011//将当前的offset保存到Zookeeper的频率public long stateUpdateIntervals = 2000;//用于失效消息的重试策略public String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。public long retryInitialDetails = 0;public double retryDelayMultiplier = 1.0;//连续重试之间的最大延时public long retryDelayMaxMs = 60 * 1000;//当retryLimit低于0时，不停地重新发送失效的消息public int retryLimit = -1; KafkaSpout只接收一个SpoutConfig的实例作为参数。 下面给出一个实例： 首先创建一个名为couple的topic，如下： 1bin/kafka-topics.sh --create --zookeeper localhost:3030 --partitions 4 --replication-factor 1 --topic couple 写一个简单的Producer，将文件string_data.txt的记录发送到couple中，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class kafkaFileProducer&#123; private final String topic_name; private final String file_name; private final KafkaProducer&lt;String,String&gt; producer; private Boolean isAsync; public kafkaFileProducer(String topic_name,String file_name,Boolean isAsync)&#123; this.file_name=file_name; this.topic_name=topic_name; Properties properties=new Properties(); properties.put(\"bootstrap.servers\", \"localhost:9092\"); properties.put(\"client.id\",\"CoupleProducer\"); properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); producer=new KafkaProducer&lt;String, String&gt;(properties); this.isAsync=isAsync; &#125; public void sendMessage(String key,String value)&#123; long start_time=System.currentTimeMillis(); if(isAsync)&#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key),new CoupleCallBack(start_time,key,value)); &#125;else&#123; try &#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key,value)).get(); System.out.println(\"Sent message : ( \"+key+\" , \"+value+\" )\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args)&#123; String file_name=\"DataSource/Data/string_data.txt\"; String topic_name=\"couple\"; kafkaFileProducer producer=new kafkaFileProducer(topic_name,file_name,false); int lineCount=0; FileInputStream fis=null; BufferedReader br=null; try &#123; fis=new FileInputStream(file_name); br=new BufferedReader(new InputStreamReader(fis)); String line=null; while((line=br.readLine())!=null)&#123; lineCount++; producer.sendMessage(lineCount+\"\",line); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; class CoupleCallBack implements Callback&#123; private long start_time; private String key; private String message; public CoupleCallBack(long start_time, String key, String message) &#123; this.start_time = start_time; this.key = key; this.message = message; &#125; /* A callback method The user can implement to provide asynchronous handling of request completion. The method will be called when the record sent to the server has been acknowledged. */ @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; long elapsed_time=System.currentTimeMillis()-start_time; if(recordMetadata!=null)&#123; System.out.println(\"Message( \"+key+\" , \"+ message+\" ) sent to partition(\"+recordMetadata.partition()+\" ) , offset(\" +recordMetadata.offset()+\" ) in \"+elapsed_time+\" ms\"); &#125;else&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 编写一个简单的Storm Topology，利用KafkaSpout读取couple中的数据(一条条的句子)，并分割成一个个的单词，统计单词个数，如下： SplitSentenceBolt 1234567891011121314151617public class SplitSentenceBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String sentence=tuple.getStringByField(\"msg\"); System.out.println(tuple.getSourceTask()+\":\"+sentence); String[] words=sentence.split(\" \"); for(String word:words)&#123; basicOutputCollector.emit(new Values(word)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\")); &#125;&#125; WordCountBolt 123456789101112131415161718192021222324public class WordCountBolt extends BaseBasicBolt&#123; private Map&lt;String,Long&gt; counts=null; @Override public void prepare(Map conf,TopologyContext context)&#123; this.counts=new ConcurrentHashMap&lt;&gt;(); super.prepare(conf,context); &#125; public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=this.counts.get(word); if(count==null) count=0L; count++; this.counts.put(word,count); basicOutputCollector.emit(new Values(word,count)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\",\"count\")); &#125;&#125; ReportBolt 1234567891011121314public class ReportBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=tuple.getLongByField(\"count\"); String reportMsg=\"&#123; word : \"+word+\" , count : \"+count+\" &#125;\"; basicOutputCollector.emit(new Values(reportMsg)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"message\")); &#125;&#125; KafkaWordCountTopology 123456789101112131415161718192021222324252627282930313233343536373839public class WordCountKafkaTopology &#123; private static final String KAFKA_SPOUT_ID=\"kafka-spout\"; private static final String SPLIT_BOLT_ID=\"split-bolt\"; private static final String WORD_COUNT_BOLT_ID=\"word-count-bolt\"; private static final String REPORT_BOLT_ID=\"report-bolt\"; private static final String CONSUME_TOPIC=\"couple\"; private static final String PRODUCT_TOPIC=\"test\"; private static final String ZK_ROOT=\"/couple\"; private static final String ZK_ID=\"wordcount\"; private static final String TOPOLOGY_NAME=\"word-count-topology\"; public static void main(String[] args)&#123; BrokerHosts brokerHosts=new ZkHosts(\"192.168.1.118:3030\"); SpoutConfig spoutConfig=new SpoutConfig(brokerHosts,CONSUME_TOPIC,ZK_ROOT,ZK_ID); spoutConfig.scheme = new SchemeAsMultiScheme(new MessageScheme()); TopologyBuilder builder=new TopologyBuilder(); builder.setSpout(KAFKA_SPOUT_ID,new KafkaSpout(spoutConfig),3);//需要注意的是，spout的并行度不能超过topic的partition个数！ builder.setBolt(SPLIT_BOLT_ID,new SplitSentenceBolt(),1).shuffleGrouping(KAFKA_SPOUT_ID); builder.setBolt(WORD_COUNT_BOLT_ID,new WordCountBolt()).fieldsGrouping(SPLIT_BOLT_ID,new Fields(\"word\")); builder.setBolt(REPORT_BOLT_ID,new ReportBolt()).shuffleGrouping(WORD_COUNT_BOLT_ID); //builder.setBolt(KAFKA_BOLT_ID,new KafkaBolt&lt;String,Long&gt;()).shuffleGrouping(REPORT_BOLT_ID); Config config=new Config(); Map&lt;String,String&gt; map=new HashMap&lt;&gt;(); //map.put(\"metadata.broker.list\", \"localhost:9092\"); map.put(\"bootstrap.servers\", \"localhost:9092\"); map.put(\"serializer.class\",\"kafka.serializer.StringEncoder\"); config.put(\"kafka.broker.properties\",map); config.setNumWorkers(3); LocalCluster cluster=new LocalCluster(); cluster.submitTopology(TOPOLOGY_NAME,config,builder.createTopology()); Utils.sleep(10000); cluster.killTopology(TOPOLOGY_NAME); cluster.shutdown(); &#125;&#125; ​","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://tankcat2.com/tags/kafka/"},{"name":"storm","slug":"storm","permalink":"http://tankcat2.com/tags/storm/"}]},{"title":"列存储中常见压缩技术","slug":"compression","date":"2017-05-11T05:40:00.000Z","updated":"2017-07-20T00:46:44.000Z","comments":true,"path":"2017/05/11/compression/","link":"","permalink":"http://tankcat2.com/2017/05/11/compression/","excerpt":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种.","text":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种. 行程编码 Run-length Encoding行程编码的核心思想是将有序列中的相同元素转化成一个三元组&lt;属性值，该值第一次出现的位置，出现的次数&gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势. 位向量 Bit-Vector位向量的核心思想是，将一个列中相同的值转为一个二元组&lt;属性值，在列中出现的位置的位图&gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩. 词典编码 Dictionary Encoding词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文Processing a trillion cells per mouse click上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"压缩","slug":"压缩","permalink":"http://tankcat2.com/tags/压缩/"},{"name":"行程编码","slug":"行程编码","permalink":"http://tankcat2.com/tags/行程编码/"},{"name":"词典编码","slug":"词典编码","permalink":"http://tankcat2.com/tags/词典编码/"},{"name":"位向量","slug":"位向量","permalink":"http://tankcat2.com/tags/位向量/"}]},{"title":"“视图”漫游","slug":"view","date":"2017-05-10T06:17:31.000Z","updated":"2017-07-20T00:47:04.000Z","comments":true,"path":"2017/05/10/view/","link":"","permalink":"http://tankcat2.com/2017/05/10/view/","excerpt":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select.","text":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select. 从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查. 那视图有优点呢？换句话说，为什么要使用视图呢？归纳起来有四点： 简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系. 限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等. 支持列的动态计算.基本表一般都不支持这个功能，比如有一张orders订单表，包含产品数量produce_num与单价produce_price_each两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列total_price(product_num*produce_price_each)就可以直接查询出订单的总价了. 兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用. 既然说视图也是一种SQL语句，那么它和查询的区别是什么呢？简单来说，有三点： 存储上，视图存储是数据库设计的一部分，而查询则不是； 更新限制上，因为视图来自于基本表，所以可间接地对基本表进行更新，但是存在诸多限制，比如不能在使用了group by语句的视图中插入值. 排序结果上，通过查询，可以对一个基本表进行排序，而视图不可以. 此外，经常对视图的增删改查还是会转换为对基本表的增删改查，会不会降低操作的效率呢？其实未必，尤其是对于多表关联，视图创建后数据库内部会作出相应处理，建立对应的查询路径，反而有利于查询的效率，这就涉及到物化视图的知识了. 维基百科里解释道，物化视图(Materialized View)，也叫做快照，是包含了查询结果的数据库对象，可能是一个远程数据的本地副本，或者是一张表或连接结果的行或者列的子集，等.创建物化视图的过程有时候也被称作是物化，一种缓存查询结果的形式，类似于函数式编程中将函数值进行缓存，有时也称作是“预计算”，用来提高查询的性能与效率. 在关系型数据库中，如果涉及到对基本视图的查找或修改，都会转化为与之对应的基本表的查找或修改.而物化视图采取不同的方法.查询的结果被缓存在一个实体表中，而不是一个视图里，实际存储着数据的，这个实体表会随着基本表的更新而更新. 这种方法利用额外的存储代价和允许部分数据过期的代价，使得查询时的数据访问更加高效.在数据仓库中，物化视图经常使用，尤其是代价较大的频繁基本表查询操作. 和基本视图还有一点不同的是，在物化视图中，可以在任何一列上建立索引，相反，基本视图通常只能在与基本表相关的列上建立索引.","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"视图","slug":"视图","permalink":"http://tankcat2.com/tags/视图/"},{"name":"物化视图","slug":"物化视图","permalink":"http://tankcat2.com/tags/物化视图/"}]},{"title":"近日爱读诗词","slug":"poet","date":"2017-05-09T04:44:31.000Z","updated":"2017-05-09T08:13:28.000Z","comments":true,"path":"2017/05/09/poet/","link":"","permalink":"http://tankcat2.com/2017/05/09/poet/","excerpt":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景","text":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景初夏即事石梁茅屋有弯碕，流水溅溅度两陂。晴日暖风生麦气，绿阴幽草胜花时。ps:读这首诗的那天正好是立夏苔白日不到处，青春恰自来。苔花如米小，也学牡丹开。ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"诗词","slug":"诗词","permalink":"http://tankcat2.com/tags/诗词/"}]},{"title":"聚类索引与非聚类索引","slug":"index","date":"2017-05-06T13:08:00.000Z","updated":"2017-07-20T00:47:26.000Z","comments":true,"path":"2017/05/06/index/","link":"","permalink":"http://tankcat2.com/2017/05/06/index/","excerpt":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能.","text":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能. 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示. 非聚集索引非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示. 两者的区别实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别. 我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。我们把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。 如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”。 通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。 两种索引的应用场合 动作描述 聚集索引 非聚集索引 经常对列进行分组排序 √ √ 返回某个范围内的数据 √ × 一个或者极少不同的值 × × 小数目的不同值 √ × 大数目的不同值 × √ 频繁更新的列 × √ 频繁更新索引列 × √ 外键列 √ √ 主键列 √ √","categories":[{"name":"数据库","slug":"数据库","permalink":"http://tankcat2.com/categories/数据库/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://tankcat2.com/tags/索引/"}]},{"title":"<刀锋>观后感","slug":"daofeng","date":"2017-04-20T12:11:31.000Z","updated":"2017-05-05T12:09:24.000Z","comments":true,"path":"2017/04/20/daofeng/","link":"","permalink":"http://tankcat2.com/2017/04/20/daofeng/","excerpt":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.”","text":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.” “因为人不论男男女女，都不仅仅是他们自身；他们也是自己出生的乡土，学步的农场或城市公寓，儿时玩的游戏，私下听来的山海经，吃的饭食，上的学校，关心的运动，吟哦的诗章，和信仰的上帝。这一切东西把他们造成现在这样，而这些东西都不是道听途说就可以了解的，你非得和那些人生活过。要了解这些，你就得是这些。 ” 无论中英文，都是一流的文字，解释了各种文化之间的冲突，以及冲突误解的永恒性。 很少有外国作品上让我读得这么舒服，这完全归功于周旭良老师的翻译功底，整本书翻译地非常好，读起来如沐春风。书写得很平淡，但每个角色都很有意思，拉里最为迷人。很奇怪，刚开始看的时候我脑子里对拉里的想象，居然是血战钢锯岭里的戴斯蒙特，这里也仅是人物形象。拉里一直追寻的答案，等同于追求终极真理，而这个问题最终都会归结到理性与精神的绝对满足。真的很难以想象，拉里这样的人，现实中又有多少，他们的生活又是怎样的？这种绝对的内心的泰然平和，我生生世世估计也做不到吧。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://tankcat2.com/categories/读书笔记/"}],"tags":[{"name":"传记","slug":"传记","permalink":"http://tankcat2.com/tags/传记/"}]},{"title":"<简明美国史>观后感","slug":"historyUSA","date":"2017-04-16T12:11:31.000Z","updated":"2017-05-05T12:08:22.000Z","comments":true,"path":"2017/04/16/historyUSA/","link":"","permalink":"http://tankcat2.com/2017/04/16/historyUSA/","excerpt":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。","text":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。 有人说陈勤老师的这本美国史写得实在是太简太浅显，但是对我这种历史水平只停留在高中课本上的”史盲“来说，基本上是够了，从脉络上了解美国自1620年《五月花号公约》至2016年奥巴马最后的执政之间所发生的重大历史事件，这其中涵盖了美国从英属殖民地开始，到1776年《独立宣言》宣告独立，到1860年林肯领导南北战争，再到一战、二战、冷战，以及至今美国发生的种种。读完全书的第一感受是，陈勤老师应当是亲美派的，书中给我描述的美国是一个有趣、鲜活、有人味的美国，虽然对历史变革中发生的流血事件只是轻描淡写地带过，但还是能些许体会到”世界何尝不简单，历史从来不温柔“这一面。读完一遍脑海中对美国的历史线还是稍有混乱，有时间自己再整理整理。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://tankcat2.com/categories/读书笔记/"}],"tags":[{"name":"历史","slug":"历史","permalink":"http://tankcat2.com/tags/历史/"}]},{"title":"再见我的暴力女王","slug":"evil","date":"2017-02-27T12:11:31.000Z","updated":"2017-11-26T06:16:11.104Z","comments":true,"path":"2017/02/27/evil/","link":"","permalink":"http://tankcat2.com/2017/02/27/evil/","excerpt":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了； 到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她； 没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。 今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。 最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。","text":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了； 到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她； 没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。 今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。 最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"生化危机","slug":"生化危机","permalink":"http://tankcat2.com/tags/生化危机/"}]},{"title":"Kafka快速入门","slug":"kafka_quickstart","date":"2017-02-27T12:11:31.000Z","updated":"2017-07-20T00:48:16.000Z","comments":true,"path":"2017/02/27/kafka_quickstart/","link":"","permalink":"http://tankcat2.com/2017/02/27/kafka_quickstart/","excerpt":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf","text":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf 下载Kafka 我使用的是kafka_2.10-0.10.2.1版本 12tar -xvzf kafka_2.10-0.10.2.1cd kafka_2.10-0.10.2.1/config 接下来进行参数配置：server.properties 123456789vim server.properties...# 修改broker.id,全局唯一# 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔zookeeper.connect=192.168.115:2181# 设置话题的删除,默认值为falsedelete.topic.enable=true# 设置数据日志路径log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs 启动 Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的： 1zkServer.sh start 接着启动Kafka: 1bin/kafka-server-start.sh config/server.properties 创建topic 使用下面的命令创建名为single_node的topic，副本数为1，分区数为1，命令执行结束后，kafka-logs路径下就会生成一个single_node-0的文件夹。 1bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node 发布与消耗数据 执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。 1234bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_nodewxtzfi love u 执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据： 1234bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginningwxtzfi love u 以上均是单机版的Kafka配置与使用。","categories":[{"name":"Kafka文档","slug":"Kafka文档","permalink":"http://tankcat2.com/categories/Kafka文档/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://tankcat2.com/tags/kafka/"}]},{"title":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","slug":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","date":"2017-01-17T05:11:31.000Z","updated":"2017-05-05T11:28:40.000Z","comments":true,"path":"2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","link":"","permalink":"http://tankcat2.com/2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","excerpt":"并行连接处理的两种基本框架","text":"并行连接处理的两种基本框架 hash-based 基于哈希，如下图所示，分为四个步骤： partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集； distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R{ik}$ 和 $S{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$Rk=\\bigcup{i=1}^{n}R_{ik}$,$Sk=\\bigcup{i=1}^{n}S_{ik}$; build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表； prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果. duplication-based 基于副本，如下图所示，分为三个步骤： duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$Rk=\\bigcup{i=1}^{n}R_i=R$即为全集R； build构建，构建哈希表，与hash-based相似； prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似. PRPD连接算法 PRPD定义：partial redistribution &amp; partition duplication，即将hash-based和duplication-based相结合. 处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S{loc}$是倾斜数据子集，$S{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R{dup}$是与$S{loc}$连接键值相同的数据子集，$R{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S{redis}$进行连接. 存在的问题： global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布； broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加. 本文提出算法PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进. Semijoin-based 连接 semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行不完全连接查询，即查找到匹配的数据行就返回，不再继续查找. semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤： 类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线); 对各个节点中的数据集$S_i$在属性b上做投影操作得到$\\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中； 每个计算节点k收到数据集S的key 子集$\\pib(S{ik})$，和数据集R的子集$Rk=\\bigcup{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)； 各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果. 特点： 由于投影操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以解决数据倾斜； 第2和第3步骤，只传输key和能连接上的元组，因此减轻了网络传输代价. 对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量. Query-based 连接 根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下： 若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value； 返回的value和本地的S数据集做最后的实际连接操作，输出连接结果. 特点： 对于高选择性的连接处理，优势大，减轻网络通信负载； 对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证的序列以便最后的连接处理，因此可能降低处理速度. 折中综合：通过一个计数器来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based. 性能问题本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷. PRPQ连接算法 PRPQ定义：partial redistribution &amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤： R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)； Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)； Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$Rk=\\bigcup{i=1}^{n}R{ik}$建立哈希表，(1). 对接收到的集合$\\bigcup{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\\pib(h{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i； Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\\bigcup$第4部分的连接结果. 特点： 与query-based算法相比 当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性. 继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. 与PRPD算法相比 最主要的区别在于，使用query而不是duplication操作. PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价. 对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载. 算法实现每个节点上skew元组的提取是基于局部倾斜量化，因此引入一个阈值参数，即当一个key出现的次数超过该阈值，则视这个key为skewed. 下面先整理如何处理阈值参数，再整理PRPQ算法的具体实现. 局部数据倾斜有很多方法可实现局部数据倾斜的快速监测，比如采样，扫描等.但是这些与本文的思路无关，所以本文仅仅在每个节点中对key的出现次数进行计数，按照降序排列，并保存到文件中. 在每一次的参数测试中，每个节点预先读取出现次数超过t的key，写入一个ArrayList中，并视它们为skew key. PRPQ具体实现具体算法和前面的四个步骤一一对应，如下： 在每个原节点中，将所有的元组读取到一个ArrayList中，处理数据集R的元组. 首先初始化一个R_c，用于收集分组的元组，R_c的初始化大小为计算节点的数量.接着，各个线程读取ArrayList中的R集元组，根据连接key的哈希值对元组进行分组.最后，将分好组的元组分发到对应的计算节点中(算法中的here表示当前计算节点的id). 根据给定的阈值参数t，对数据集S进行划分，倾斜的key被读入一个hashset，并且所有对应的元组被存储到一个hashmap中，剩余的非倾斜元组存储到一个$S^{’}_c$中.接着对hashmap进行投影操作，将所有的unique keys保存到key_c中.最后将key_c和$S^{’}_c$按照key的哈希值分发到对应的计算节点上. 在计算节点中，对接收到的R集元组建立一个哈希表T’，对数据集$S’$元组进行遍历，并查找哈希表，若有匹配的key，则输出连接结果.同时遍历key集key_c，并查找哈希表，若不存在匹配的key，则返回值为null的value到对应的原节点，否则返回实际key对应的value. 倾斜元组的连接结果可以通过遍历查询返回的value集合，若value为null，则不存在能连接上的S集元组，否则输出最终连接结果. 实验对比数据集的选取：用作基准的数据集模仿决策支援系统下的连接操作.数据集R的cardinality为64M，数据集S的cardinality为1GB.由于数据仓储中数据一般以面向列的形式存储，所以实验中将数据格式设置为的键值对，其中key和value均是8字节整型. 工作负载的选取：设置数据集R和S之间存在外键的关系，保持R的主键的unique，而在S中为对应的外键增加skew.除此之外，若S是统一分布的，它们中的每一个以相同的概率匹配关系R中的元组.对于倾斜的元组，它们的unique key在节点间均匀分布，并且每一个均能与R匹配上.下表给出了数据集S的分布情况. S key distribution Partition Size Zipf skew=0,1,1.4 均匀evenly 512M Linear f(r)=46341-r,23170 排序范围sort-range 1GB,2GB Zipf分布中，skew=0表示统一分布，skew=1表示排名前十的key占据总量14%，skew=1.4表示排名前十的key占据总量68%.线性分布中，使用f(r)来描述key的分布情况，其中f(r)=46341-r表示频率最高的key出现46341次，频率第二的key出现46340次.使用该函数生成的数据集可以看作low-skewed的数据集.f(r)=23170表示所有的key都是均匀分布的，但是重复次数较高.f(r)对应的两个数据集均为1GB的大小，有46341个unique key. R和S在计算节点中的分布情况：R均匀分布在所有的节点上，而S使用均匀和排序范围分布.均与分布保证每个计算节点上skewed元组的数量相同；排序范围分布是先将所有的元组按照键的频率排序，然后等分成大小一样的块，再将块按照次序分配到每个计算节点上.因此每个节点上skewed元组的数量差距可能会比较大. 实验共从运行时间、网络通信、负载均衡、可扩展性四个方面来进行比较.这里只就运行时间稍作整理. 运行时间记录Hash-based算法、PRPD、PRPQ和query-based算法的运行时间，如下图所示.当S是均匀分布(第一组数据skew=0)，Hash、PRPD和PRPQ算法的性能相近，远远优于Query算法；当S是low skewed时，PRPD和PRPQ均比另外两种算法快；当S是high skewed时，Hash算法性能最差，而其余三种性能相近，则可得出结论，其余PRPD、PRPQ和Query可以较好地处理数据倾斜.随着skew程度的增加，Hash算法的执行时间增长剧烈，而Query算法呈现下降趋势.而PRPD和PRPQ算法呈现平稳的下降趋势. 上图展示是选择最佳频率阈值t的性能，原文中关于不同阈值的实验这里不再整理，基本情况是无论t值如何变化以及分区计划如何，PRPQ的运行时间是低于PRPD的.","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://tankcat2.com/categories/论文阅读/"}],"tags":[{"name":"data skew","slug":"data-skew","permalink":"http://tankcat2.com/tags/data-skew/"},{"name":"parallel join","slug":"parallel-join","permalink":"http://tankcat2.com/tags/parallel-join/"}]},{"title":"使用Storm遇到的问题以及解决方案","slug":"stormproblems","date":"2016-12-30T05:45:31.000Z","updated":"2017-01-17T12:32:46.000Z","comments":true,"path":"2016/12/30/stormproblems/","link":"","permalink":"http://tankcat2.com/2016/12/30/stormproblems/","excerpt":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错：","text":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错： 12345678910111213142016-12-30 12:41:17.269 b.s.event [ERROR] Error when processing eventjava.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: File '/home/admin/stormdata/data/supervisor/localstate/1480504905565' does not exist at backtype.storm.utils.LocalState.partialSnapshot(LocalState.java:118) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.utils.LocalState.get(LocalState.java:126) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.local_state$ls_local_assignments.invoke(local_state.clj:83) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:321) ~[storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.applyToHelper(AFn.java:154) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:626) ~[clojure-1.6.0.jar:?] at clojure.core$partial$fn__4228.doInvoke(core.clj:2468) ~[clojure-1.6.0.jar:?] at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.6.0.jar:?] at backtype.storm.event$event_manager$fn__7258.invoke(event.clj:40) [storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?] at java.lang.Thread.run(Thread.java:744) [?:1.7.0_45] 找不到’/home/admin/stormdata/data/supervisor/localstate/1480504905565’这个文件夹，网上找了下原因，给出的答案是stop the server without previously stop the supervisor，就是说可能是由于不正常关机造成状态不一致，具体原因不知，解决方案是删除stormdata/data/supervisor整个目录即可. 在集群环境下日志清理，自己写了一个脚本clear-log.sh，主要是删除apache-storm-XXX下的logs文件里的日志文件，如下： 1234567STORM_HOME=/home/admin/apache-storm-0.10.0 HOSTS_FILE=/home/admin/hosts.txtcat $HOSTS_FILE | while read line do ssh $line \"rm -rf $STORM_HOME/logs/*\" &lt; /dev/null doneecho \"remove log files...done\" 其中STORM_HOME是storm的安装路径，hosts.txt是集群中各个节点的地址，我自己的配置如下： 1234567891011121314admin@10.11.1.53admin@10.11.1.40admin@10.11.1.41admin@10.11.1.42admin@10.11.1.45admin@10.11.1.46admin@10.11.1.51admin@10.11.1.53admin@10.11.1.54admin@10.11.1.55admin@10.11.1.56admin@10.11.1.58admin@10.11.1.60admin@10.11.1.64 编辑完之后执行chmod +x clear-log.sh命令使得该文件获得可执行权限，再执行./clear-log.sh运行该脚本即可.","categories":[{"name":"Storm学习之路","slug":"Storm学习之路","permalink":"http://tankcat2.com/categories/Storm学习之路/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"},{"name":"日志","slug":"日志","permalink":"http://tankcat2.com/tags/日志/"},{"name":"环境配置","slug":"环境配置","permalink":"http://tankcat2.com/tags/环境配置/"}]},{"title":"关于苏打绿的题库","slug":"knowledgebase","date":"2016-10-26T08:26:31.000Z","updated":"2016-10-26T10:53:28.000Z","comments":true,"path":"2016/10/26/knowledgebase/","link":"","permalink":"http://tankcat2.com/2016/10/26/knowledgebase/","excerpt":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴","text":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴 龚钰祺：键盘手+中提琴手,1980.12.16，台湾，国立台北艺术大学音乐研究所，中提琴、钢琴、电子琴 刘家凯：电子吉他手，1982.2.5，台湾台北，国立政治大学心理系、国立阳明大学脑科学研究所，吉他 何景扬：木吉他手，1982.4.4，台湾，国立政治大学公共行政研究所，吉他、乌克丽丽 2. 重要时间节点 2001年成立于校园，2003年确立6人阵容，2004年5月，苏打绿正式出道，发行第一张单曲《空气中的视听与幻觉》。 2005年，苏打绿发行了《SodaGreen》。 2006年，苏打绿发行专辑《小宇宙》。 2007年11月，苏打绿发行专辑《无与伦比的美丽》。 2008年5月，苏打绿发行专辑《陪我歌唱》。 韦瓦第计划：2009年5月8日，苏打绿发行了第五张专辑《春·日光》；2009年9月11日，苏打绿发行专辑《夏·狂热》；2013年9月18日，苏打绿发行专辑《秋：故事》，该专辑成为2013年度iTunes Store最受欢迎专辑；2015年9月23日，苏打绿发行专辑《冬·未了》。 2011年 2011年11月11日，苏打绿发行专辑《你在烦恼什么》。 2014年，苏打绿开始了十周年世界巡回演唱会。 3. 唱片公司 2004-2009，林浩哲音乐社； 2009-至今，环球唱片 4. “游乐园鱼丁糸”比赛题第一轮 单选题（以下各题四个选项中,只有一个选项正确）： EP《空气中的视听与幻觉》碟的颜色为：DA、白B、红C、蓝D、墨绿 迄今为止仍未引进内地的苏打绿正式专辑是：AA、无与伦比的美丽 B、小宇宙 C、同名专辑 D、陪我歌唱 青峰创作的第一首歌是：DA、空气中的视听与幻觉 B、降落练习存在孪生基因 C、后悔莫及 D、窥 03年海洋音乐祭时，谁迟到了？CA、家凯 B、阿福 C、小威D、青峰 家凯、林暐哲法国街头赛跑谁赢了？AA、家凯B、林暐哲C、都赢D、无法评判 图片题（见附件）：这张是苏打绿在大陆第一场演唱会（615北京场）的新闻图片，请问方框里面的人是谁？CA、茶水 B、lfxfox C、博博鱼 D、和尚 苏打绿第一任的团长是谁？AA.小威 B.阿福 C.馨仪 D.青峰 苏打绿第一张同名专辑共有几首歌？DA、2 B、3 C、10 D、11 苏打绿一共来过上海几次？BA、1 B、2 C、3 D、4 苏打绿的第三张单曲是：CA、空气中的视听与幻觉 B、飞鱼 C、Believe in music D、迟到千年 苏打绿成军时间：BA、2000 B、2001 C、2004 D、2005 “我想到你离开了以後，我们的城市好寂寞”选自苏打绿的哪首歌曲？BA、无与伦比的美丽 B、雨中的操场 C、相信 D、城市 单曲《飞鱼》中有几首歌？DA、1 B、2 C、3 D、4 苏打绿首次来内地时青峰裤子上的油漆是怎么来的：CA、裤子本身印有的 B、被团员泼的 C、自己粉刷房间墙壁弄的D、不知道 以下苏打绿未上过的台湾综艺节目是：BA、娱乐百分百 B、康熙来了 C、国光帮帮忙 D、大学生了没 第18届金曲奖最佳作曲人奖是因为哪首歌？BA、小宇宙 B、小情歌 C、频率 D、飞鱼 红包场中，谁是王子造型的？DA、青峰 B、家凯 C、阿龚 D、馨仪 小情歌是为了哪个艺人的唱片公司收歌而写？CA、刘若英 B、江美琪 C、徐若瑄 D、杨乃文 《爱人动物》是以下哪部电影的主题曲？BA、情非得已之生存之道 B、Juno C、囧男孩 D、海角七号 「无与伦比的美丽」小巨蛋演唱会是几月几号？CA、2007.11.1 B、2007.11.2 C、2007.11.3 D、2007.11.13 第二轮 单选题： 1~5 BDB(D)CD 6~10 CCCCB 11~15DCCBC 16~2 0ABADD 多选题： 1.BDEF 2.CD 3.BCDE 4.AC 5.ACD 6.BCD 7.ABC 8.ABC 9.ACD 10.DE 第三轮 模拟题1单选题： 王菀之的首张国语创作专辑中多少首歌是由青峰作词？CA、3 B、4 C、5 D、6 615北京演唱会开场歌曲是什么？AA、无与伦比的美丽 B、小宇宙 C、小情歌 D、白日出没的月球 1224广州演唱会最后一首歌曲是：DA、频率 B、小情歌 C、这天 D、陪我歌唱 苏打绿夺得了第几届海洋音乐祭的陪审团大奖？BA、4 B、5 C、6 D、7 家凯是因为想参加什么音乐节而加入苏打绿的？BA、春浪 B、春天呐喊 C、海洋音乐祭 D、简单生活 第一期空气中的视听与幻觉广播是06年几月几号？BA、0906 B、1007 C、1017 D、1104 苏打绿的第一首抒情歌是：AA、 频率 B、无言歌 C、背着你 D、小情歌 以下哪首是政大吉他社社歌？ CA、天天想你 B、我的未来不是梦 C、我呼吸我感觉我存在 D、和天一样高 除了小巨蛋演唱会之外，哪一个影像保存了苏打绿《是我的海》的live演出？DA、好友音乐会 B、周日狂热夜 C、摇滚风城音乐祭 D、 Taiwan Roc 在小巨蛋DVD中，青峰一共talking了多少次（不包括唱歌中以及预报下一首歌）？BA、4次 B、5次 C、6次 D、7次 《Air》里的Bass是谁弹的？DA、家凯 B、馨怡 C、阿福 D、阿龚 《搜包包》中谁全程站在家凯的面前？AA、tirtir B、小威 C、青峰 D、将将 《当代歌坛》第一次出现专写苏打绿的文章是哪一期？CA、342期B、370期C、363期D、389期 创作给别人的歌中，青峰最喜欢哪首？AA、女爵B、多希望你在C、穿墙人D、爱与奇异果 苏打绿中谁一直没有拿到高中毕业证书：DA、家凯 B、阿龚 C、阿福 D、小威 在豆瓣公开现身过的苏打绿团员是：CA、家凯 B、阿龚 C、阿福 D、小威 苏打绿现在的六人编制的第一次演出是在哪里？BA、春天呐喊 B、政大金旋奖 C、西门町 D、海洋音乐祭 多选题： 下面不属于919上海演唱会安可曲目的是：DEA、相信 B、这天 C、女爵 D、是我的海》 E、白日出没的月球 “搁浅”一词出现在以下哪些歌曲中：ABCEA、吵 B、迟到千年 C、蓝眼睛 D、无与伦比的美丽 E、漂浮 青峰曾为以下哪些艺人写歌？ACDEA、左光平 B、尚雯婕 C、张韶涵 D、刘若英 E、王菀之 填空（每题5分，共5题） “各站停靠”演唱会上，作为舞台背景的钟显示的时间是22:15__ 2010年青峰和小威一起庆生时，其他团员送他们的礼物是_小时候照片拼贴出来的相框__ 阿福在自由发挥的歌曲_欢迎光临__的MV中客串演出了一个角色 各站停靠台中场的小礼物是_阿福面纸__ 苏打绿出道时候的第一支广告是_蔡康永推荐的广播广告 模拟题2 单选（以下各题，只有一个正确选项。每题5分，共10题） 苏打绿二度踏上小巨蛋举办“日光狂热”演唱会前，媒体特地准备多样运动器材帮忙训练体力。结果，阿福在哪样器械上竟然输给了馨仪？CA.滚轮 B.握力棒 C.哑铃 D.铁饼 青峰对自己人生过程进行总结的歌是 DA.交响梦 B.融雪之前 C.相信 D.近未来 苏打绿、王菀之、方大同合作的903拉阔演唱会上，他们约定好各自代表的颜色，分别是 DA.黑、白、绿 B.黑、绿、白C.绿、黑、白 D.绿、白、黑 被媒体拍到跟青峰一起看电影，并且对镜头比中指的女友人是 AA.小兔 B.张悬 C.娃娃 D.阿纯 歌词本的字型让“吴青峰写到手心手背都是肉”的专辑是 CA. 苏打绿同名 B.春•日光 C.陪我歌唱 D.无与伦比的美丽 这是2010年5月苏打绿参加利物浦音乐节的返国记者会照片。请问青峰的这件衬衫，还在以下哪场公开表演中穿过？DA.成都热波音乐节 B.北京原创A8颁奖礼 C.西安草莓音乐节 D.西湖音乐节 在《日光》MV中，和大家一起采茶的团员是：AA.阿龚 B.小威 C.馨仪 D.家凯 来源于青峰日记的歌词是:CA.近未来 B.女爵 C.困在 D.早点回家 苏打绿一共参与过几次简单生活节:CA.1次 B.2次 C.3次 D.4次 阿福入伍前的饯别仪式上，第一个给阿福剪头发的团员是:AA.青峰 B.馨仪 C.小威 D.家凯 多选（以下各题有两个或两个以上的正确选项，多选、漏选、错选均不得分。每题5分，共5题） 在“最会睡”系列中，暐哲老师睡觉的地点有：ABCDEA.馨仪家 B.伦敦录音室 C.草地 D.公司地板 E.会议室沙发 The Wall“多希望你在”系列演出的歌单中包含：ABCEA.疼惜我的吻 B.吵 C.我在欧洲打电话给你 D.寂寞拥挤E.再见 “维瓦第计划”的歌词中，没有出现的意象是:BCA.萤火虫 B.羔羊 C.海豚 D.白鸽 E.落叶 以下歌曲中，苏打绿有公开表演多个语言版本的是ACEA.Oh Oh Oh Oh B.飞鱼 C.无眠 D.日光 E.小情歌 游乐园的官方周边包括BCDEA. .海报B.T恤 C.贴纸 D.徽章 E.年历 填空题 《小宇宙》这首歌标题的灵感来自陈黎的《小宇宙》_。 《小情歌》在被徐若瑄拒绝后，又被江美琪拒绝。 2005年9_月3_日发行首张同名专辑《苏打绿》及进行全省签唱会表演。 青峰的小学时期，每天早上跟着跳的早操音乐是 我的未来不是梦。 无美丽电台的封面除了苏打绿六个人外还有谁？ 将将 5.获奖情况 2016 第27届台湾金曲奖 最佳国语专辑奖 冬未了 （获奖） 2016 第27届台湾金曲奖 最佳乐团奖 （获奖） 2016 第27届台湾金曲奖 最佳编曲人奖 痛快的哀艳 （获奖） 2015 第五届阿比鹿音乐奖 最受欢迎唱片 冬未了 （获奖） 2014 第36届十大中文金曲 全年最高销量歌手 秋:故事 （获奖） 2013 第7届无线咪咕汇音乐盛典 年度最受欢迎组合 韦瓦第计划 （获奖） 2012 Hito流行音乐奖 hito乐团 （获奖） 2011 第1届全球流行音乐金榜 年度最佳乐团 （获奖） 2011 第1届全球流行音乐金榜 年度20大金曲 十年一刻 （获奖） 2010 第10届华语音乐传媒大奖 最佳乐队 春·日光、夏/狂热 （获奖） 2010 MusicRadio中国TOP排行榜 年度最受欢迎乐团 （获奖） 2010 MusicRadio中国TOP排行榜 年度最佳乐团 （获奖） 2010 新加坡金曲奖 最佳创作歌手 夏/狂热 （获奖） 2010 新加坡金曲奖 最佳乐团 （获奖） 2010 第21届金曲奖 最佳音乐录像带奖 日光 （获奖） 2010 第21届金曲奖 最佳编曲人 日光 （提名） 2010 第21届金曲奖 最佳乐团 春·日光 （提名） 2010 第21届金曲奖 最佳乐团 夏/狂热 （提名） 2009 第32届香港十大中文金曲颁奖典礼 全国最佳组合 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊组合/乐团 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊金曲20 狂热 （获奖） 2009 第4届A8原创中国音乐盛典 年度最佳原创乐团 （获奖） 2009 MUSIC RADIO中国TOP排行榜 港台年度最佳乐团 （获奖） 2009 全球华语歌曲排行榜 地区杰出歌手奖 春·日光 （获奖） 2009 全球华语歌曲排行榜 年度20大金曲 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳编曲人 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳乐团 春·日光 （获奖） 2009 新城国语力颁奖礼 国语力亚洲乐团 （获奖） 2009 新城国语力颁奖礼 国语力歌曲 日光 （获奖） 2009 新城国语力颁奖礼 国语力乐团 （获奖） 2009 新浪网络盛典 年度最佳乐团 （获奖） 2008 第8届华语音乐传媒大奖 年度国语专辑 无与伦比的美丽 （获奖） 2008 第8届华语音乐传媒大奖 最佳编曲人 白日出没的月球 （获奖） 2008 第8届华语音乐传媒大奖 最佳乐队 无与伦比的美丽 （获奖） 2008 香港新城国语颁奖礼 新城国语歌曲 陪我歌唱 （获奖） 2008 香港新城国语颁奖礼 国语乐团 （获奖） 2008 第9届CCTV/MTV音乐盛典 港台年度最佳组合 （获奖） 2008 新城劲爆颁奖礼 新城全国乐迷投选劲爆突破表现大奖 （获奖） 2008 新加坡金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2008 第19届金曲奖 最佳音乐录影带导演奖 左边 （提名） 2008 第19届金曲奖 最佳年度歌曲 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳编曲人 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2007 新加坡金曲奖 最佳乐团 小宇宙 （获奖） 2007 第44届金马奖 最佳原创电影歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳年度歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳国语专辑 小宇宙 （提名） 2007 第18届金曲奖 最佳乐团 小宇宙 （获奖） 2006 第17届金曲奖 最佳编曲人 Oh Oh Oh Oh （提名） 2004 MTV百万乐团挑战赛 网路最佳人气乐团 （获奖） 2004 第5届海洋音乐祭 评审团大赏 （获奖） 2002 政大第19届金旋奖 乐团组冠军 空气中的视听与幻觉 （获奖） 2001 政大第18届金旋奖 乐团组最佳人气奖 （获奖） 6. 老吴写给别人的歌那英我的幸福刚刚好 林忆莲 寂寞拥挤 张惠妹 掉了，你和我的时光 李玟 想你的夜 莫文蔚 看着，老掉牙 容祖儿 在时间面前 江蕙 你讲的话 刘若英 没道理 陶晶莹 翔 蔡依林 彩色相片，栅栏间隙偷窥你，迷幻 杨丞琳 带我走，少年维特的烦恼，下个转弯是你吗，被自己绑架，一小节休息 张韶涵 最近好吗，刺情，蓝眼睛 王心凌 从未到过的地方 范玮琪 坏了良心 范晓萱 开机关机 杨乃文 女爵 许茹芸 爱人动物，飞行时光，I will be with you，最难的是相遇，现在该怎么好，我留下的一个生活 蔡健雅 极光，费洛蒙 周笔畅 别忘了 谢安琪 再见 张悬 两者 徐佳莹 乐园 魏如萱 被雨伤透，困在，开机关机 尚雯婕 什么？什么！ 吉克隽逸 我唱故我在 袁泉 等 吴映洁 一直 刘容嘉 没有人爱 潘玮仪 不同 路嘉欣 穿墙人，当我继续唱 王菀之 学会，迷湖，冬梦，是爱，爱与奇异果 旅行团 Bye Bye VOX玩声乐团 让我做你的家，朱古力 TFBOYS 小精灵 谭咏麟 超越，糖衣陷阱，蓝侬梦，魔毯，算爱，未知 张信哲 柔软 陈奕迅 放弃治疗，这样的一个麻烦，谋情害命 林俊杰 独舞，爱的鼓励，裂缝中的阳光，不存在的情人 萧敬腾 以爱之名，多希望你在 杨宗纬 想对你说 萧煌奇 下个街角 信 你存在，我记得，给自己的信 左光平 心里有鬼","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}]},{"title":"遇见小公举","slug":"jielun","date":"2016-10-23T13:21:31.000Z","updated":"2016-10-23T13:29:02.000Z","comments":true,"path":"2016/10/23/jielun/","link":"","permalink":"http://tankcat2.com/2016/10/23/jielun/","excerpt":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。","text":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。我记不得听周杰伦的第一首歌是什么。小学六年级的时候我妈同事的女儿买了一个MP3,需要分外安电池的那种，里面有两首杰伦的歌，一首发如雪，一首夜曲。我借过来听，很喜欢这两首歌的旋律，后来搜到了歌词，就经常买花花绿绿的本子抄写歌词。到了初中，很多我很不喜欢的男生疯狂地迷恋周杰伦，可能因为女孩子成熟得早，特别反感他们自以为帅气的非主流风格（还有一个原因是很多喜欢周杰伦的人同时喜欢着许嵩）。本来因为杰伦不清楚的发音，我对他是无感的，但是因为一些烦人的粉丝，对歌手本人也没什么好感。后来到了大学，某个逗比室友对此和我惊人的相似。初中的时候我可能更多的是在追日漫，喜欢主题曲，还记得初二家里买电脑之前，星空卫视每天晚上6点放犬夜叉，我就拿着复读机录那首change the world。到了高一，学校是明令禁止使用电子产品的，我用充饭卡的钱偷偷买了一个mp3,列了一个歌单让前桌的男生回家帮我下点歌，哪知道他全给我下的周杰伦。很幸运的是，他给我下的都是一些慢节奏情歌，最长的电影，给我一首歌的时间，甜甜的，说好的幸福呢，彩虹，七里香之类的。但也没有因为这些歌而粉上杰伦，还是听歌不看人的状态。后来的后来，杰伦当了好声音的导师，看了一两期，发现他其实很个很可爱的人。再到现在，他的新专辑床边故事，那首前世情人，告白气球和now you see me，让我感觉，这就是从前那个酷酷的杰伦呀，那种我以前不屑的风格原来这么奇妙。于是一点没犹豫地在网易云上买了数字专辑。今天大老远从上海跑到合肥听周杰伦的演唱会，虽然位子很不好，看不到人也算了，屏幕也看不到；虽然室外的音响效果也有点让人失望，那些快歌基本听不清歌词；虽然排队很长，座位坐得很乱…但是那些我自己很熟悉的旋律响起的时候，所有的举动只剩下舞动荧光棒，跟着一起唱。让我印象很最深刻的是点歌环节，点到的第三个女孩子，杰伦问她是和谁一起来的，她说她一个人来的。当时杰伦愣了一下，然后安慰她说，全场的观众都是她的朋友，都陪着她听唱歌。当时我特别想去拥抱那个女生。一个人去看演唱会，身边都是情侣或者闺蜜团。你的注意力本该只放在爱豆身上，可无法避免的，有些场景，有些歌词就是会触动你内心那块最柔软的地方。这种经历我体验过。错过了可以有多一点杰伦的年少时光有些遗憾，但也很幸运，我开始路转粉了，未来的路，还可以相伴而行。💗💗💗ps:当然啦，如果能遇见一个喜欢人的一起去苏打绿的after summer，那么会更加幸运~~~","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"周杰伦","slug":"周杰伦","permalink":"http://tankcat2.com/tags/周杰伦/"}]},{"title":"My Favorite Band","slug":"sodagreen","date":"2016-10-17T12:02:31.000Z","updated":"2016-10-17T12:22:48.000Z","comments":true,"path":"2016/10/17/sodagreen/","link":"","permalink":"http://tankcat2.com/2016/10/17/sodagreen/","excerpt":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name.","text":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name.Pure, artistic, indie, free, soft and simple. All of these are my impression of Sodagreen. The band is well known for its lead vocalist and songwriter Wu Tsing-Fong, who is excellent for his poetic lyrics, unique performing style and wide vocal range. As a typical Virgo, he is a paranoia that is disproportionate to an idol. He never does what an idol should do. He is unwilling to please the fans and doesn’t like to participate in the announcement program. To be a qualified artist is very hard; to be an artist who can satisfy all the fans is harder. I still remember the live show in Spring Wave Music And Art Festival this year. Sodagreen was arranged to the final appearance and didn’t finish all the songs in that the organizer advanced the end of the show and turned off the microphone domineeringly. Wu reluctantly left in the dark, but insisted on singing the rest of the songs through Weibo. One of my favorite albums is “Summer/Fever”. Whenever I feel sad, I will listen to this album, which has inspiring power. It was released on September 11, 2009 and is their fifth full-length studio album. It is the second of the band’s Vivaldi Project, a planned series of four albums representing the four seasons respectively. The recording of this album took place in London and the songs were mostly written by the lead vocalist Wu. The album contains Britpop elements and lyrical references to the supernatural, Faust, Madame Butterfly, Don Quixote and the Greek god Dionysus. Among the songs of this album, I like “The Sound That Remains” best. In the lyrics, it draws an analogy between the sound of cicadas and the flood of public opinion, which narrows our horizon. I think the metaphor of the song is what Sodagreen has being teaching us: Don’t always mind about what other people think of you and just be free to pursue the self-value realization. The Sodagreen’s last round of road show “After Summer” before their temporarily overturn has launched. I wish I could grab a ticket for the live show in Shanghai!","categories":[{"name":"随笔","slug":"随笔","permalink":"http://tankcat2.com/categories/随笔/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}]},{"title":"Streaming Similarity Self-Join","slug":"sssj","date":"2016-08-23T05:45:31.000Z","updated":"2016-09-10T10:47:10.000Z","comments":true,"path":"2016/08/23/sssj/","link":"","permalink":"http://tankcat2.com/2016/08/23/sssj/","excerpt":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。","text":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。 Introduction 介绍在数据库和数据挖掘领域，相关性自连接处理被广泛研究，其应用场景较为广泛，包括剽窃检测、查询优化、协同过滤、重复网页的检测与去除等。若使用暴力解法，其复杂度是\\(O(n^2)\\)。在现实应用中，数据项常以高维稀疏向量的形式呈现，那么相似度的计算即为余弦相似值的计算。为了方便处理，可以将向量归一化，则进一步将问题转换为两个向量的点积。目前解决相似度自连接的算法主要依赖于基于倒排索引的删减技术以及一些数值界限。计算相似度自连接，不仅适用于静态的数据集，在数据流领域也同样适用。这里举例两个现实应用，在数据流环境下进行相似度自连接处理。 趋势监测。社交平台，譬如Twitter，趋势监测算法识别频率陡增的主题标签。更细粒度的趋势监测算法也会识别微博集合。这些集合中的数据项出现的频率增加，并且同时出现某些相同的标签。对于该趋势监测算法，在数据流环境下找出相似的微博十分重要。 近似重复项过滤。同样，在社交平台，譬如Twitter，当某个事件发生时，用户们可能会接收到与该事件相关的近似重复的多条微博。这些微博通常连续地出现。因此，将这些近似重复的微博进行过滤或者分组有利于提高用户体验。 但问题是，关于数据流环境下的相似度自连接处理的研究并不多。这是由于缺乏无限内存：不能将先到来数据项随意删除，因为该数据项可能与未来到的数据项相似。本文引入一个时间因子来解决内存问题。我们设定，只有在到达的时间间隔在指定范围内，两个数据项才有可能相似。我们定义时间依赖的相似性：基于内容的点积与时间因子的乘积，该时间因子会随着时间间隔的增加呈现指数级减小。由于时间因子的存在，当某些数据项的到达时间超出一定范围，我们可以将这些数据项删除。下图表达了这个idea。 如上图所示，标记有时间戳的文档以数据流的形式源源不断到达。时间轴上方的文档包含相似的内容，标记为红色。在所有两两相似的文档对中，我们只关心到达时间相近的文档对。因此，图中所有的4-选-2的文档对里，只有两对文档被选中(用蓝色箭头标记)。与已有的相似性自连接处理算法类似，本文的算法也依赖于索引技术。已有的算法使用不同类型的索引过滤，以减少通过索引返回的潜在配对项数量。按照这种语义，我们定义时间过滤来，与时间依赖的相似性进行关联，以便将旧的索引项从索引列表中删除。本文提出了两种算法框架来解决数据流的相似性自连接处理，均依赖于时间过滤。MB框架使用现成的索引技术，在运行过程中以流水线的方式创建两个索引，随着时间的推移丢弃旧索引。STR框架对现有的索引技术进行调整，将时间过滤的因素内嵌到其中。此外，本文结合已有的索引技术AP和L2AP，设计了一种可处理流式数据的索引算法L2,它存在以下四点优势： 有效减少潜在相似数据项的配对数量； 不需要收集数据流的统计信息； 使用轻量级的索引维持； 当数据项变“旧”时，可以迅速丢弃。 Related Work 相关工作关于数据流上的相似性自连接，已有的研究很少。与之相关的主要就是相似性自连接，这个课题的研究相对广泛，由Chaudhuri等人首次在文献A primitive operator for similarity joins in data cleaning中提出，此后涌现出大量相关算法研究，与本文最为相关的是由谷歌的Bayardo提出的AP算法(Scaling up all pairs similarity search)以及Anastasiu与Karpis提出的L2AP算法(L2AP: Fast Cosine Similarity Search With Prefix \\(L_2\\) Norm Bounds)。有关这两个算法，下文会进行阐述。 Problem Statement 问题陈述我们定义数据项是m维的行向量，向量中的元素是实数值。我们定义\\(sim(x,y)\\)为计算向量\\(x\\)和向量\\(y\\)相似性的函数，并设定所有的向量均被归一化为单位向量，即\\(||x||_2=||x||=\\sqrt{\\sum_{j=1}^{m}x_j^2}=1\\)。则数据项的相似度计算可以简化为两个向量的点积，如下公式所示。$$sim(x,y)=dot(x,y)=xy^T=\\sum_{j=1}^{m}=x_j\\cdot y_j$$其中，\\(x_j\\)是向量\\(x\\)的第\\(j\\)个元素。在现实应用中，维数m通常较高，并且向量较为稀疏(向量中数值为0的元素相当多)。因此通常使用\\((j,x[j])\\)的集合来表示一个向量\\(x\\),并有\\(x[j]&gt;0,j=1…m\\)。在非数据流环境下，我们指定一个向量数据集\\(D=\\{x_1,…,x_n\\}\\)，同L2AP论文一致，我们使用\\(x^{’}=x^{’}_p = &lt; x_1,…,x_p,0,…,0 &gt; \\)来表示向量\\(x\\)的前缀，并使用\\(vm_x\\)来表示其中向量\\(x\\)所有元素的最大值，使用\\(\\sum_{x}=\\sum_{j}x_j\\)表示向量\\(x\\)所有元素之和，使用\\(|x|\\)来表示向量\\(x\\)的大小或者非零元素的个数（注意与向量的长度\\(||x||\\)之间的区别），使用\\(m_j\\)表示集合\\(D\\)中所有向量第\\(j\\)个元素的最大值，所有的\\(m_j\\)组合成向量\\(m\\)。在标准的all-pairs 相似查询问题(相似性自连接)中，给定一个向量集合和一个相似度阈值\\(\\theta\\),目标是找出所有的向量对\\((x,y)\\)满足\\(sim(x,y)\\geq\\theta\\)。在数据流环境下，每一个数据项被标记有其到达时间\\(t(x)\\)，则数据流可以表示为\\(S = &lt; …,(x_i,t(x_i)),(x_{i+1},t(x_{j+1})),… &gt; \\)。因此，我们在定义两个向量的相似度时不仅要考虑点积，还要考虑它们到达的时间之差\\(\\triangle t_{xy}=|t(x)-t(y)|\\)。则给定两个标有时间戳的向量\\(x\\)和\\(y\\),则它们的时间依赖相似度为$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}$$ 其中\\(\\lambda\\)是一个时间衰减参数。当\\(\\triangle t_{xy}=0\\)或者\\(\\lambda=0\\)时，时间依赖相似性回归到标准的相似性计算；当\\(\\triangle t_{xy}\\)趋于无穷大时，相似度为0。综上，我们可以给出SSSJ的问题定义，如下所示。 给定具有时间戳的向量流\\(S\\)，相似性阈值\\(\\theta\\)以及时间衰减因子\\(\\lambda\\),输出所有的向量对\\(x,y)\\)，满足\\(sim(x,y)\\geq\\theta\\)。 此外，根据向量的第二范式可知，\\(dot(x,y)\\leq 1\\),则有$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}\\leq e^{-\\lambda|t(x)-t(y)|}$$ 因此，\\(\\triangle t_{xy}\\geq \\lambda ^{-1}log \\theta ^{-1}\\)表明\\(sim_{\\triangle t}(x,y)&lt;\\theta\\)，意味着对于给定的向量，不可能与在$$\\Gamma=\\lambda ^{-1}log \\theta ^{-1}$$ 个时间单位之前到达的向量相似。相应地，我们可以将“比\\(\\Gamma\\)旧”的向量删除，并称\\(\\Gamma\\)为时间期限。 Overview of the Approach 方法的高层次概述本文提出了两个算法框架，MB-IDX和STR-IDX，其中IDX是在静态数据集上解决all-pairs相似查询问题的索引技术。为了使算法框架的阐述更加清晰，我们首先回顾一下该索引技术的概述。所有的索引技术均是基于倒排索引设计的，为m个列表的集合\\(I = \\{I_1,I_2,…,I_m\\}\\)，每一个列表\\(I_j\\)由序列对\\((\\iota (x),x_j)\\)组成，其中\\(x_j\\)是向量\\(x\\)中第\\(j\\)个元素，且\\(x_j \\neq 0\\)；\\(\\iota (x)\\)是指向向量\\(x\\)的引用。所有的索引技术均是在检索相似对的同时逐个创建索引的。具体来讲，初始化时定义一个空索引，迭代地处理数据集\\(D\\)中的向量。对于每一个最新处理的向量\\(x\\)，我们从索引中检索已经存在的且与\\(x\\)相似的向量\\(y\\)，输出相似对\\((x,y)\\)。接着，将向量\\(x\\)中某些不为0的元素插入到索引中。创建索引与检索相似对的过程可以归纳为以下三个步骤： index construction(IC)：向索引列表\\(I\\)中增加新的向量； candidate generation(CG)：使用索引列表来生成潜在的相似配对项； candidate verification(CV)：计算所有潜在配对项之间的相似度，输出超过阈值的数据对。 针对这三项步骤，本文提出了三个原语操作，分为是： \\((I,P) \\leftarrow IndConstr-IDX(D,\\theta)\\)：给定向量数据集\\(D\\)，相似度阈值\\(\\theta\\)，IndConstr-IDX返回结果集\\(P=\\{(x,y)\\}\\)；同时，IndConstr-IDX创建索引列表\\(I\\)。 \\(C \\leftarrow CandGen-IDX(I,x,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)和相似度阈值\\(\\theta\\)，CandGen-IDX返回与向量\\(x\\)潜在配对的向量集合\\(C=\\{y\\}\\)。 \\(P \\leftarrow CandVer-IDX(I,x,C,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)，潜在向量集合\\(C\\)和相似度阈值\\(\\theta\\)，CandVer-IDX返回符合要求的相似度配对项集合\\(P=\\{(x,y)\\}\\)。 MB-IDX与STR-IDX均依赖于IDX索引，并通过增加时间过滤因子使得该索引适用于数据流环境。这两个算法框架的区别在于时间过滤因子是如何在索引中调整与设置的。MB-IDX将IDX视为黑盒，使用时间过滤因子来创建IDX的独立实例对象，并在其失效时进行丢弃；相反地，STR-IDX直接应用时间过滤因子，适时地调整索引。关于STR-IDX的处理过程会在下文进行阐述，这里先介绍MB-IDX的处理过程。MB-IDX在时间间隔\\(\\Gamma\\)内运行，在第\\(k\\)个时间间隔里，从数据流中读取向量，并将它们缓存在\\(W\\)中。在这个时间间隔的末尾，调用IndConstr-IDX来检索\\(W\\)中所有的相似对，并对\\(W\\)创建索引列表\\(I\\)。在第\\(k+1\\)个时间间隔内，重置缓存\\(W\\)，重新从数据流中读取向量。此时MB-IDX针对最新读取的向量\\(x\\)查询索引列表\\(I\\)，找出前一个时间间隔到达的并与\\(x\\)相似的向量。相似对的计算通过调用CandGen-IDX和CandVer-IDX实现。在第\\(k+1\\)个时间间隔的末尾，重置索引列表\\(I\\)，并在该时间间隔到达的所有向量之间查询出相似对。下图给出MB-IDX的伪代码。 Filtering Framework 过滤框架对于每一种索引机制，我们描述它的三个基本原语(IC,CG,CV)，并讨论如何对其进行调整以适用于数据流(见下文的STR-IDX算法)。为了使文章自成一体，接下来的每一种索引，我们先展示其在静态数据集上的操作过程，然后再阐述在MB的使用和STR框架中的调整。 Inverted Index 倒排索引最简单的是不包含“优化删减索引项”的倒排索引。在所有的索引机制中，最直观的是如果两个向量相似，则它们必须至少有一个共同的坐标。因此两个相似的向量可在某些索引列表\\(I_j\\)中存在。在IC操作中，对于每一个新读取的向量\\(x\\)，需要将所有的坐标元素插入到索引中。在CG操作中，我们只用索引列表\\(I\\)检索与向量\\(x\\)相似的潜在向量。特别地，这些潜在向量\\(y\\)所在的索引列表中必定存在向量\\(x\\)的非0坐标元素。在CV操作中，输出相似度大于阈值的。 MB framework(MB-INV)三个操作IndConstr-INV、CandGen-INV和CandVer-INV直接按照上述的过程实现，这里就不细节讨论。 STR framework(STR-INV)关于INV索引的阐述是用于解决静态数据集的相似自连接问题的，这里我们考虑流式数据的相关性自连接。我们在将向量\\(x\\)的坐标元素按照向量的到达顺序插入到索引列表中。则索引列表\\(I_j\\)按照时间戳\\(t(x)\\)对索引项\\((\\iota (x),x_j)\\)进行排序。在列表中就时间维持顺序较为容易。则我们也按照相同的顺序处理数据，每次操作时只需要将向量的坐标元素插入到列表的尾部即可。 All-pairs Indexing Scheme all-pairs索引机制Bayard在INV的基础上作出了改进，提出了AP算法，减少了索引列表的大小：无需对向量\\(x\\)中的所有坐标元素建立索引，只需要保证索引列表中存在至少一个向量\\(y\\)的坐标元素与\\(x\\)的相同。类似于INV，AP在处理一个向量时逐步建立索引列表。如以下算法InConstr-AP所示，处理向量\\(x\\)时，按照事先预先指定的顺序扫描。这里维护了一个pscore变量，用于表示向量\\(x\\)的前缀与数据集中任意一个变量相似度的上界。AP使用向量\\(m\\)，即数据集的所有维的最大坐标元素值组成的向量，来计算这个上界。只要pscore小于阈值\\(\\theta\\)，则目前扫描到的向量\\(x\\)坐标值可以不加入索引列表，并且不用担心造成相似配对的遗失。一旦pscore超过阈值，则将向量\\(x\\)的剩余坐标值全部添加进索引中，而前缀\\(x^{’}\\)则添加进未加索引集合R中。 潜在配对项的生成算法如以下CandGen-AP所示，为与向量\\(x\\)相似的向量\\(y\\)的大小（非0元素个数）指定一个下界\\(sz_1\\)。如果向量\\(y\\)的大小小于这个下界，则该向量不可能与向量\\(x\\)相似，如算法第7行所示。此外，还维护了一个变量\\(rs_1\\),用于指定向量\\(x\\)和向量\\(y\\)相似度的上界。当该上界小于阈值时，停止将新的向量加入潜在配对项映射表C中，如算法第8行所示。映射表C存储了潜在配对的向量以及已扫描的部分点积值。 最后，判别最终符合要求的相似配对项操作如算法CandVer-AP所示，使用已计算的部分相似度(C中存储的值)与没有加入索引的前缀计算得到最终的相似度。 由于数据流形式的AP算法，包括MB和STR，性能均较低，所以这里我们不深入讨论。 L2-based Indexing Scheme L2AP索引机制L2AP是由Anastasiu和Karypis两人提出的、目前最优的解决all-pairs相似性自连接的算法。该算法在AP的基础上使用了更为严密的\\(l_2-bound\\)界限，不仅可以减少索引列表的大小，还可以减少潜在配对项的数量和fully-computed similarity的数量。L2AP主要借助柯西不等式，得到\\(dot(x,y) \\leq ||x|| \\cdot ||y||\\)。这个不等式同样适用于向量的前缀。我们假设向量已归一化，即\\(||y||=1\\),则有$$dot(x^{’},y) \\leq ||x^{’}|| \\cdot ||y|| \\leq ||x^{’}||$$ 另外，当对向量\\(x\\)建立索引时，L2AP对pscore的值进行存储，以\\(\\iota (x)\\)为键将pscore保存在映射表Q中，见算法第15行；同时，L2AP也存储了向量\\(x\\)前缀的长度，如算法第16行所示的索引列表中的三元数据项。Q和\\(||x_j||\\)这两个额外信息均存储在倒排表中，在算法CendVer-L2AP中用于减少潜在的配对向量数量。 在算法CendVer-L2AP中，给定一个向量\\(x\\),我们从后往前扫描它的元素，并累计它后缀的相似值。我们定义一个变量remsore，用于维护向量\\(x\\)的剩余部分的相似值。remscore结合AP中的\\(rs_1\\)和使用存储在倒排表中的\\(||y_j^{’}||\\)的\\(rs_2\\),表示当前处理向量的前缀与倒排索引中的任意一个向量之间相似度的上界。关于L2AP算法的更多细节可以参考原论文。 Improved L2-Based Indexing Scheme 改进的L2索引机制本文在L2AP的基础上进行调整，提出了适用于数据流环境的L2算法。L2AP结合了许多不同的界限：从算法AP中继承的界限(比如IC中的\\(b_1\\)和CG中的\\(rs_1\\))，新提出的基于\\(l_2\\)范式的界限(比如IC中的\\(b_2\\)和CG中的\\(rs_2\\))。我们可以观察到基于\\(l_2\\)范式的界限比AP中的界限更为有效，在大多情况下更为严密，此结论可在Anastasiu和Karypis的论文实验部分得到验证。除此之外，AP算法在索引中使用到了静态数据的统计信息，而L2AP算法只依赖于当前被建立索引的向量。这表明在使用\\(l_2\\)范式的界限时，不需要维持向量\\(m(t)\\)，因此也不需要重新建立索引。因此L2算法只使用\\(l_2\\)范式的界限，丢弃AP中的界限。 为了阐述在数据流环境下的运行，我们需要引入额外的符号定义。首先此处的输入数据是以向量的数据流\\(S\\)，其次最重要的变动是向量\\(m\\)，它的坐标元素将随着时间的推移发生改变。以下三个操作是STR框架下的L2算法使用，主回路是IndConstr-L2-STR算法。","categories":[{"name":"论文阅读","slug":"论文阅读","permalink":"http://tankcat2.com/categories/论文阅读/"}],"tags":[{"name":"similarity search","slug":"similarity-search","permalink":"http://tankcat2.com/tags/similarity-search/"},{"name":"stream processing","slug":"stream-processing","permalink":"http://tankcat2.com/tags/stream-processing/"},{"name":"data mining","slug":"data-mining","permalink":"http://tankcat2.com/tags/data-mining/"}]},{"title":"Leiningen安装","slug":"lein","date":"2016-07-18T05:09:31.000Z","updated":"2016-09-16T03:38:40.000Z","comments":true,"path":"2016/07/18/lein/","link":"","permalink":"http://tankcat2.com/2016/07/18/lein/","excerpt":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下：","text":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下： Make sure you have a Java JDK version 6 or later. Download the lein script from the stable branch of this project. Place it on your $PATH. (~/bin is a good choice if it is on your path.) Set it to be executable. (chmod 755 ~/bin/lein) Run it. 这段安装教程是摘录自Leiningen，翻译如下： 安装Java JDK，确保其版本是6.0以及6.0之后的； 下载lein脚本 在/bin目录下新建一个名为lein的文件，将上述脚本拷贝进去并保存； 运行命令以下命令使得lein为可执行文件: 1$ chmod 755 ~/bin/lein 运行lein，下载leiningen-xxx-standalone.jar(xxx为版本) 1$ lein 由于网络连接的问题，下载过程中可能出现错误，比如：1It&apos;s also possible that you&apos;re behind a firewall and haven&apos;t set HTTP_PROXY and HTTPS_PROXY. 解决方案：删除掉~/.lein目录后，执行export HTTP_CLIENT=&quot;wget --no-check-certificate -O&quot;，然后重新执行lein即可。","categories":[{"name":"环境部署","slug":"环境部署","permalink":"http://tankcat2.com/categories/环境部署/"}],"tags":[{"name":"leiningen","slug":"leiningen","permalink":"http://tankcat2.com/tags/leiningen/"}]},{"title":"Storm组件和拓扑结构","slug":"storm component","date":"2016-07-16T02:06:31.000Z","updated":"2016-09-16T03:38:18.000Z","comments":true,"path":"2016/07/16/storm component/","link":"","permalink":"http://tankcat2.com/2016/07/16/storm component/","excerpt":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。","text":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。 Storm总体结构 Storm的术语比较多，本文涉及的有Spout，Bolt，Stream，Tuple，Stream Grouping，Topology。下面简单介绍一下。 Tuple：包含了一个或者多个键值对的列表。默认情况下，元组tuple中的域可以是整型(integer)等基本类型对象。也可以通过定义可序列化(实现Serializable接口)的对象来实现自定义的元组类型。 Stream：Storm中最核心的概念，在分布式环境下并行创建、处理的元祖Tuple序列。在声明数据流的时候要给定一个有效的id，若不显示指定，则系统默认会给数据流定义一个名为”default”的id。 Spout：消息生产者，是数据流的数据来源，充当一个采集器的角色，连接到外部的数据源，并将数据转化为一个个tuple。 Bolt：消息处理者，封装了数据处理逻辑，将一个或者多个数据流作为输入，对数据实施运算后，选择性地输出一个或者多个数据流。几种典型的功能有数据过滤、连接、聚合和数据库读写等。 Stream Grouping：定义了消息分发策略，即定义了一个数据流中的tuple如何分发给topology中的不同bolt的任务实例。 Topology：用于封装一个实时计算应用程序的逻辑，类似于Hadoop中的Job。由Stream Grouping连接起来的Spout和Bolt的有向无环图，处理的是源源不断的消息流。 Storm编程基础 Storm的源码共分为三层，分为如下： Storm最上层的所有接口均是用Java定义的。 上层绝大多数接口的逻辑是用Clojure实现的。 最底层的数据结构是用Thrift定义的。Thrift是Apache下面的跨语言框架，它可以基于Thrift定义文件产生不同语言的代码，有关详细内容可以参考thrift。 本文主要就上层Java接口介绍Storm的拓扑和组件结构。 拓扑Topology 首先，从最外层开始，举一个如何创建topology的例子。代码中的对象下面会一一介绍。123456789TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"spout1\", new Spout1(), 1);builder.setSpout(\"spout2\", new Spout2(), 5);builder.setBolt(\"bolt\", new Bolt1(), 3) .directGrouping(\"spout1\", \"stream1\") .shuffleGrouping(\"spout2\");Config conf = new Config();conf.setDebug(true);StormSubmitter.submitTopology(\"my-topology\", conf, builder.createTopology()); TopologyBuilder第一行代码定义了一个TopologyBuilder对象。TopologyBuilder是一个工具类，用于构造topology。Topology最底层实际上是Thrift的一个数据结构，分为一下2个部分： StormTopology定义了Topology的组成，包括Spout和Bolt，每个Spout或者Bolt都有全局唯一的id。目前为止，StateSpoutSpec还没有在设计代码中用到。 12345struct StormTopology&#123; 1: required map&lt;string, SpoutSpec&gt; spouts; 2: required map&lt;string, Bolt&gt; bolts; 3: required map&lt;string, StateSpoutSpec&gt; state_spouts;&#125; TopologySummary定义了用户提交的Topology的基本情况，例如该topology分布在几个工作进程上，使用了多少个线程，有多少个任务实例。这些数据主要供Nimbus使用，以返回UI请求的数据。 123456789struct TopologySummary&#123; 1: required string id; 2: required string name; 3: required i32 num_tasks; 4: required i32 num_executors; 5: required i32 num_workers; 6: required i32 uptime_secs; 7: required string status;&#125; 由于Topology在Thrift中过于描述化的特性不便于直接使用，所以TopologyBuilder进行了上层的封装，提供了更加方便的构建方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public class TopologyBuilder &#123; private Map&lt;String, IRichBolt&gt; _bolts = new HashMap&lt;&gt;(); private Map&lt;String, IRichSpout&gt; _spouts = new HashMap&lt;&gt;(); private Map&lt;String, ComponentCommon&gt; _commons = new HashMap&lt;&gt;(); private Map&lt;String, StateSpoutSpec&gt; _stateSpouts = new HashMap&lt;&gt;(); ... public StormTopology createTopology() &#123; Map&lt;String, Bolt&gt; boltSpecs = new HashMap&lt;&gt;(); Map&lt;String, SpoutSpec&gt; spoutSpecs = new HashMap&lt;&gt;(); maybeAddCheckpointSpout(); for(String boltId: _bolts.keySet()) &#123; IRichBolt bolt = _bolts.get(boltId); bolt = maybeAddCheckpointTupleForwarder(bolt); ComponentCommon common = getComponentCommon(boltId, bolt); boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common)); &#125; for(String spoutId: _spouts.keySet()) &#123; IRichSpout spout = _spouts.get(spoutId); ComponentCommon common = getComponentCommon(spoutId, spout); spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common)); &#125; StormTopology stormTopology = new StormTopology(spoutSpecs, boltSpecs,new HashMap&lt;String, StateSpoutSpec&gt;()); stormTopology.set_worker_hooks(_workerHooks); return stormTopology; &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt, Number parallelism_hint)&#123; validateUnusedId(id); initCommon(id, bolt, parallelism_hint); _bolts.put(id, bolt); return new BoltGetter(id); &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125; ... public SpoutDeclarer setSpout(String id, IRichSpout spout, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, spout, parallelism_hint); _spouts.put(id, spout); return new SpoutGetter(id); &#125; public SpoutDeclarer setSpout(String id, IRichSpout spout)&#123; return setSpout(id, spout, null); &#125; ... private void validateUnusedId(String id) &#123; if(_bolts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Bolt has already been declared for id \" + id); &#125; if(_spouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Spout has already been declared for id \" + id); &#125; if(_stateSpouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"State spout has already been declared for id \" + id); &#125; &#125; private ComponentCommon getComponentCommon(String id, IComponent component) &#123; ComponentCommon ret = new ComponentCommon(_commons.get(id)); OutputFieldsGetter getter = new OutputFieldsGetter(); component.declareOutputFields(getter); ret.set_streams(getter.getFieldsDeclaration()); return ret; &#125; private void initCommon(String id, IComponent component, Number parallelism) throws IllegalArgumentException &#123; ComponentCommon common = new ComponentCommon(); common.set_inputs(new HashMap&lt;GlobalStreamId, Grouping&gt;()); if(parallelism!=null) &#123; int dop = parallelism.intValue(); if(dop &lt; 1) &#123; throw new IllegalArgumentException(\"Parallelism must be positive.\"); &#125; common.set_parallelism_hint(dop); &#125; Map conf = component.getComponentConfiguration(); if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf)); _commons.put(id, common); &#125; ... 成员变量_bolts包含了所有的Bolt对象，_spouts包含了所有的spout对象，它们的类型分别是IRichBolt和IRichSpout类型，关于组件的接口下文会慢慢介绍。 spout的thrift底层结构为SpoutSpec，包含了两个成员，一个是实现消息生产者具体逻辑的spout_object对象，另一个是用来描述其输入输出的common对象，具体定义如下： 1234struct SpoutSpec&#123; 1: required ComponentObject spout_object; 2: required ComponentCommon common;&#125; bolt的thrift底层结构为Bolt，结构与SpoutSpec是一致的，这里不再阐述。 _commons包含了所有的Bolt和Spout对象，它在Thrift中的数据结构是ComponentCommon: 123456struct ComponentCommon&#123; 1: required map&lt;GlobalStreamId, Grouping&gt; inputs; 2: required map&lt;string, StreamInfo&gt; streams; 3: optimal i32 parallelism_hint; 4: optimal string json_conf;&#125; ComponentCommon是用来表示Topology的基础组件对象,inputs表示该组件将从哪些GlobalStreamId以何种方式接收数据，其中GlobalStreamId是某个组件上定义的一条流，数据结构如下： 1234struct GlobalStreamId&#123; 1: required string componentId; 2: required string streamId;&#125; GlobalStreamId有两个域，componentId表示该流属于哪一个组件，streamId是流的标识。不同组件之间可以使用相同的streamId。分组方式Grouping决定了组件所发送的消息将以何种方式发送到接收端,Grouping被定义为union类型，即表示节点之间只能采取一种分组方式，其结构定义如下： 12345678910union Grouping&#123; 1: list&lt;string&gt; fields //若为空，则采取global grouping的分组方式; 2: NullStruct shuffle; //随机分组 3: NullStruct all; //全分组，即广播 4: NullStruct none; //无分组，全部发送到同一个任务实例上 5: NullStruct direct; //直接分组，直接发送到指定的任务实例上 6: JavaObject custom_object; 7: binary custom_serialized; 8: NullStruct local_or_shuffle;&#125; ComponentCommon中的streams指定了该组件需要输出的流，它给定了streamId以及StreamInfo。StreamInfo中定义了输出流的字段名列表以及该流的消息分组是否是直接分组方式，其结构定义如下： 1234struct StreamInfo&#123; 1: required list&lt;string&gt; output_fields; 2: required bool direct;&#125; ComponentCommon中的parallelism_hint表示组件的并行度，即有多少个线程，这些线程可分布在不同的进程空间或者机器中，默认值是１。 ComponentCommon中的json_conf保存了与该组件相关的一些设置。 第29~46行定义了setBolt()方法以及其重载。该方法主要功能是定义topology中的bolt对象，并指定其并行度。该方法在运行的过程中首先会通过方法validateUnusedId()检测输入的组件ID是否是唯一的，其次调用initCommon()方法为该组件构建一个ComponentCommon对象，并且只初始化其并行度和配置信息，配置信息被序列化成JSON的形式。setBolt()最后会返回一个BoltGetter对象，将利用其为bolt对象添加输入流信息。关于这个对象后文再具体介绍。 第50~59行定义了setSpout()方法以及其重载。该方法类似于setBolt()方法，也会产生ComponentCommon对象。 第63~73行定义了validateUnusedId()方法，用于检测输入的组件Id是否是唯一的，若不是则抛出异常。 第75~81行定义了getComponentCommon()方法，该方法是在createTopology()创建拓扑的时候被调用的，设置了组件的输出流信息。 第83~96行定义了initCommon()方法，主要是对ComponentCommon对象进行初始化，设置并行度和配置信息。 最后一个也是最为重要的，第8~27行定义了createTopology()方法，根据输入的Bolt和Spout对象创建Topology对象。从第16行和第21行可以看出，在创建拓扑的过程中，Bolt和Spout均为对象序列化后得到的字节数组。 组件ComponentsBolt接口再回头看创建topology的简单示例，第4行设置bolt的方法中构建了一个类Bolt1的对象实例。该类是一个自定义的Bolt类，可以是实现了Storm定义的Bolt接口，主要有IBolt，IRichBolt，IBasicBolt和IBatchBolt，它们之间的关系如下图所示： IComponent接口IComponent是通用的组件接口，所有的Bolt和Spout都会实现这个接口，其代码如下：1234public interface IComponent extends Serializable&#123; void declareOutputFields(OutputFieldsDeclarer declarer); Map&lt;String, Object&gt; getComponentConfiguration();&#125; 其中，declareOutputFields()的参数为OutputFieldsDeclarer接口，定义了拓扑中每个组件的输出字段声明，每个组件都需要它来指定输出到哪些流、声明输出的字段列表以及指出输出流是否是直接流，代码如下：123456public interface OutputFieldsDeclarer&#123; public void delcare(Fields fields); public void delcare(boolean direct, Fields fields); public void delcareStream(String streamId,Fields fields); public void declareStream(String streamId,boolean direct,Fields fields);&#125; 其中，第5~6行没有明确指定输出流的id，默认使用的是storm的default流。所有的方法中都有一个Fields对象参数，该类用于存储消息的字段名列表，比如一条学生个人信息的scheme为(“stu_name”,”stu_number”,”stu_age”,”stu_sex”)，其所需参数是字段名集合。对于同一条消息，在构建Fields对象时会为其所有的字段建立索引。它的代码定义如下：1234567891011121314151617181920212223242526public class Fields implements Iterable&lt;String&gt;,Serializable&#123; private List&lt;String&gt; _fields; private Map&lt;String,Integer&gt; _index = new HashMap&lt;String,Integer&gt;(); public Fields(String... fields)&#123; this(Arrays.asList(fields)); &#125; public Fields(List&lt;String&gt; fields)&#123; _fields= new ArrayList&lt;String&gt;(fields.size()); for(String field:fields)&#123; if(_fields.contains(field))&#123; throw new IllegalArgumentException(String.format(\"duplicate field '%s'\", field)); &#125; _fields.add(field); &#125; index(); &#125; private void index()&#123; for(int i=0;i&lt;_fields.size();i++)&#123; _index.put(_fields.get(i),i); &#125; &#125; ...&#125; Fields实现了Iterable&lt;String&gt;接口，表明Fields可以遍历存储的字段名列表；也实现了Serializable接口，表明Fields可以被序列化。第2~3行定义了一个保存所有字段名的列表以及一个保存了从字段名到它在字段名列表中位置的映射表。第5~7行的构造函数接收一个可变参数fields，将其转换为列表好调用第9~19行定义的构造函数。第9~19行定义的构造函数首先会检查传入的字段名列表中是否存在重复的字段名，并保存该字段名列表，最后调用index()方法为该字段名列表建立索引。 IBolt接口Bolt是Storm中的基础运行单位，当接收到一条数据时可以不立刻对其进行处理，可以先保存后处理。其生命周期如下： 创建提交Topology时创建IBolt实例并进行序列化操作(见createTopology())； 将序列化的Bolt组件发送给集群中的主节点； 主节点启动工作进程，并在进程中反序列化Bolt组件； 在开始执行任务之前，先调用Bolt的prepare()回调方法进行初始化，然后再具体处理接收的数据。 IBolt的具体代码如下：12345public interface IBolt extends Serializable&#123; void prepare(Map stormConf, TopologyContext context, OutputCollector collector); void execute(Tuple input); void cleanup();&#125; 在实现Bolt的过程中，用户可以编写其构造函数，然而构造函数并不会被实际调用，因为在提交Topology时，系统会调用Topology的构造函数，并将产生的对象序列化成字节数组。每一个节点上的Bolt都是通过反序列化的方式得到的，这可能导致某些成员没有被正确的初始化，因此用户应该将复杂对象的初始化放在prepare()回调方法中。第三个参数是一个OutputCollector类对象，它是Bolt的输出收集器，Bolt处理好的消息都是通过输出收集器发送出去的，不同类型的输出收集器也不同，这里先讲一下IRichBolt的输出收集器，它实现了IOutputCollector接口，是一个代理类。IOutputCollector接口如下：。emit()方法用来向外发送数据，它的返回值是该消息所发送目标的TaskId集合，其输入参数分布是消息将被输出的流Id，输出的消息标记(通常代表该条消息从哪些消息产生的)以及要输出的消息。emitDirect()与emit()类似，主要区别在于它发送的消息只有指定的任务实例才能接收。这个方法要求streamId对应的流必须被定义为直接流。如果下游节点没有接收到该消息，那么此类消息其实并没有真正发送。fail()和ack()用来表示消息是否被真正处理。OutputCollector是IOutputCollector的默认实现类，它实际上是一个代理，包含一个真正工作的IOutputCollector实例，这个对象是在Clojure中定义的。OutputCollector中提供了许多重载方法供用户使用，具体定义可参照OutputCollector。 123456public interface IOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void ack(Tuple input); void fail(Tuple input);&#125; 对象在被销毁时，将调用cleanup()回调方法，但是Storm并不保证该方法一定被执行，只有当在本地模式下运行时杀死topology该方法才保证一定能被执行。 execute()方法实现对输入消息的处理。其参数是一个Tuple实例。Tuple是Storm中的主要数据结构，在发送接收消息的过程中，每一条消息实际上都是一个Tuple对象。其接口代码如下：第5~14行的方法用于获取由参数i指定的字段位置的值，如果用户知道该字段对应的类型，就可以调用对应类型的获取方法获取字段的值。第15~24行与之类似，不过这里的方法是根据字段名获取相应的值。123456789101112131415161718192021222324252627282930313233public interface ITuple &#123; public int size(); public int fieldIndex(String field); public boolean contains(String field); public Object getValue(int i); public String getString(int i); public Integer getInteger(int i); public Long getLong(int i); public Boolean getBoolean(int i); public Short getShort(int i); public Byte getByte(int i); public Double getDouble(int i); public Float getFloat(int i); public byte[] getBinary(int i); public Object getValueByField(String field); public String getStringByField(String field); public Integer getIntegerByField(String field); public Long getLongByField(String field); public Boolean getBooleanByField(String field); public Short getShortByField(String field); public Byte getByteByField(String field); public Double getDoubleByField(String field); public Float getFloatByField(String field); public byte[] getBinaryByField(String field); public Fields getFields(); public List&lt;Object&gt; select(Fields selector); public List&lt;Object&gt; getValues(); public GlobalStreamId getSourceGlobalStreamId(); public String getSourceComponent(); public int getSourceTask(); public String getSourceStreamId(); public MessageId getMessageId();&#125; IRichBolt接口IRichBolt同时实现了IComponent和IBolt接口，其含义是一个具有Bolt功能的组件。在实际使用中，IRichBolt是实现Topology组件的主要接口，其定义如下：123public interface IRichBolt extends IBolt,IComponent&#123; &#125; IBasicBolt接口IBasicBolt接口的定义与IBolt基本一致，具体实现要求也与IBolt相同，主要区别为一下两点： 它的输出收集器使用的是BasicOutputCollector，并且该参数被放在了execute方法中而不是prepare中； 它实现了IComponent接口，表明它可以用来定义Topology组件。首先我们来看一下BasicOutputCollector,它是Storm提供的IBasicOutputCollector接口的默认实现。我们看一下该接口的定义。12345public interface IBasicOutputCollector&#123; List&lt;Integer&gt; emit(String streamId,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,List&lt;Object&gt; tuple); void reportError(Throwable t);&#125; 对比IOutputCollector可以看出两者的区别： IBasicOutputCollector中没有ack和fail方法； IBasicOutputCollector的emit和emitDirect方法中没有anchor参数。这样设计的原因是如果使用了IBasicBolt，Storm框架会自动帮用户进行Ack、Fail和Anchor操作，用户自己不需要关心这一点。所以为了确保这种机制能正常运行，避免用户在使用时出错，Storm提供了简化版的IBasicOutputCollector。BasicOutputCollector收集器实际上是OutputCollector的封装类，其中包含了一个OutputCollector类型的成员变量，实际上所有的消息最终都将由这个OutputCollector进行处理。 再回到IBasicBolt，之所以设计这个接口上文已有解释，Storm框架本身帮用户处理了所发出消息的Ack、Fail和Anchor操作，是由执行器BasicBoltExecutor实现的。该类实现了IRichBolt接口，同时还包含了一个IBasicBolt成员变量用于调用的转发，它是基于装饰模式的，定义如下：12345678910111213141516171819202122232425262728293031323334public class BasicBoltExecutor implements IRichBolt &#123; public static final Logger LOG = LoggerFactory.getLogger(BasicBoltExecutor.class); private IBasicBolt _bolt; private transient BasicOutputCollector _collector; public BasicBoltExecutor(IBasicBolt bolt) &#123; _bolt = bolt; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; _bolt.declareOutputFields(declarer); &#125; public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; _bolt.prepare(stormConf, context); _collector = new BasicOutputCollector(collector); &#125; public void execute(Tuple input) &#123; _collector.setContext(input); try &#123; _bolt.execute(input, _collector); _collector.getOutputter().ack(input); &#125; catch(FailedException e) &#123; if(e instanceof ReportedFailedException) &#123; _collector.reportError(e); &#125; _collector.getOutputter().fail(input); &#125; &#125; public void cleanup() &#123; _bolt.cleanup(); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return _bolt.getComponentConfiguration(); &#125;&#125; 第3行定义了成员变量_bolt，实现了IBasicBolt接口；第17行设置执行器运行的上下文，它表示经execute方法发送出去的消息都是由输出消息产生的，即输出消息都将标记为输入消息所衍生出来的消息，这是使用IBasicBolt消息跟踪的重要环节。第20行对输入的消息进行Ack操作。这一步意味着基于当前输入消息的处理以及衍生消息的发送已经完成，此时可以对该消息进行Ack操作了。用户实现了IBasicBolt接口的Bolt对象之后，在构建Topology时，Storm会调用TopologyBuilder的setBolt方法设置该Bolt对象。setBolt方法会用BasicBoltExecutor封装用户的实现类，这是Storm自动帮用户完成的，而且它还会调用可接收IRichBolt参数的重载方法完成Bolt对象的设置。这也解释了BasicBoltExecutor需要实现IRichBolt接口的原因。 IBatchBolt接口区别与IBasicBolt接口，IBatchBolt主要用于Storm中的批处理。Storm的事务Topology以及Trident主要是基于IBatchBolt的。相比前面的IBolt、IRichBolt、IBasicBolt，IBatchBolt中多了一个finishBatch()方法，它在一个批处理结束时被调用。此外，IBatchBolt还去除了cleanup()方法，其接口定义如下：12345public interface IBatchBolt&lt;T&gt; extends Serializable,IComponent&#123; void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, T id); void execute(Tuple input); void finishBatch();&#125; prepare()方法用来初始化一个Batch，最后一个参数是通用类型T，它可以用作该Batch的唯一标识。在目前的Storm实现中，每个事务都会对应一个Batch，而每个Batch的数据都会由一个新创建的IBatchBolt对象进行处理，当一个Batch被成功处理后，该Batch对应的IBatchBolt对象将被销毁，因此用户不能通过IBatchBolt对象自身存储需要在多个Batch之间进行共享的数据。第二个参数是IBatchBolt的输出收集器BatchOutputCollector,其代码定义如下：123456789public abstract class BatchOutputCollector&#123; public List&lt;Integer&gt; emit(List&lt;Object&gt; tuple); public List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple); public void emitDirect(int taskId, List&lt;Object&gt; tuple)&#123; emitDirect(taskId, Utils.DEFAULT_STREAM_ID,tuple); &#125; public abstract void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple); public abstract void reportError(Throwable error);&#125; 同IBasicOutputCollector类似，不需要自己去处理Ack、Fail和Anchor这3项操作。Storm提供了BatchOutputCollector的默认实现类BatchOutputCollectorImpl,该类是一个代理类，内部封装了OutputCollector变量，所有的方法都通过调用OutputCollector方法来实现。 finishBatch()方法仅当这批消息被完全处理之后才会被调用。 与IBasicBolt类似，使用IBatchBolt也不需要关心何时该对收到的信息进行Ack等操作，Storm框架内部通过BatchBoltExecutor自动帮我们实现了这些功能。BatchBoltExecutor也实现了IRichBolt接口，它会为每个Batch创建与之对应的BatchBolt对象。同时还实现了FinishedCallBack和TimeoutCallback接口。BatchBoltExecutor的代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class BatchBoltExecutor implements IRichBolt, FinishedCallback,TimeoutCallback&#123; public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class); byte[] _boltSet; Map&lt;Object, IBatchBolt&gt; _openTransactions; Map _conf; TopologyContext _context; BatchOutputCollector _collector; public BatchBoltExecutor(IBatchBolt bolt)&#123; _boltSet= Utils.serialize(bolt); &#125; public void prepare(Map conf, TopologyContext context, OutputCollector collector)&#123; _conf=conf; _context=context; _collector= new BatchOutputCollectorImpl(collector); _openTransactions= new HashMap&lt;Object, IBatchBolt&gt;(); &#125; public void execute(Tuple input)&#123; Object id = input.getValue(0); IBatchBolt bolt = getBatchBolt(id); try&#123; bolt.execute(input); _collector.ack(input); &#125;catch(FailedException e)&#123; LOG.error(\"Failed to process tuple in batch\",e); _collector.fail(input); &#125; &#125; public void cleanup()&#123;&#125; public void finishedId(Object id)&#123; IBatchBolt bolt = getBatchBolt(id); _openTransactions.remove(id); bolt.finishBatch(); &#125; public void timeoutId(Object id)&#123; _openTransactions.remove(id); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; newTransactionalBolt().declareOutputFields(declarer); &#125; public Map&lt;String, Object&gt; getComponentConfiguration()&#123; return new newTransactionalBolt().getComponentConfiguration(); &#125; private IBatchBolt getBatchBolt(Object id)&#123; IBatchBolt bolt = _openTransactions.get(id); if(bolt == null)&#123; bolt = new newTransactionalBolt(); bolt.prepare(_conf,_context,_collector,id); _openTransactions.put(id,bolt); &#125; return bolt; &#125; private IBatchBolt newTransactionalBolt()&#123; return (IBatchBolt)Utils.deserialize(_boltSet); &#125;&#125; 第3行是内含的BatchBolt对象的序列化字节数组。第17~27行实现了execute()方法，它规定输入消息的第一列用于标识Batch的id。第29~33行实现了FinishedCallback接口，这里调用finishBatch()方法清理BatchBolt对象。可以看出，BatchBolt对象在不同的Batch之间是不重复使用的。第34~36行实现了TimeoutCallback接口,仅仅将缓存的BatchBolt删除，这对于清理不再使用的BatchBolt对象是很关键的。第52~54行通过反序列化生成一个IBatchBolt对象。Storm通过反序列化对象的方式来弥补不断创建IBatchBolt对象所带来的负担。 Spout接口Storm中与spout相关的接口主要是ISpout和IRichSpout，下图描述了它们之间的关系： ISpout接口接口ISpout定义了Spout应该实现的功能集合:123456789public interface ISpout extends Serializable &#123; void open(Map conf, TopologyContext context, SpoutOutputCollector collector); void close(); void activate(); void deactivate(); void nextTuple(); void ack(Object msgId); void fail(Object msgId);&#125; 关于各个函数的功能，源代码中的注释部分已经给出了详细的描述，这里不再赘述。其中，nextTuple()由于和ack()、nextTuple()是在一个线程被调用的，如果nextTuple阻塞的话，其他方法也将被阻塞。因此，该方法必须是非阻塞的，任何Spout都将使用nextTuple来发送信息。 ISpout的fail和ack方法仅仅给出了发送消息时所对应的MessageId,而没有具体给出消息内容，表明如果要实现消息重传的话，用户需要自己来维护哪些已经发送的消息。 当Spout被设置为活跃或者不活跃时，会分别调用activate()和deactivate()方法将状态通知给用户代码。这样当Spout处于非活跃的状态时，nextTuple不会被调用。 open()方法的第3个参数是Spout的输出收集器SpoutOutputCollector，Storm只定义了一个Spout的输出收集器接口ISpoutOutputCollector，SpoutOutputCollector是它的默认实现类。首先看一下ISpoutOutputCollector的代码定义：emit()方法用来向外发送数据，它的返回值是该消息所有发送目标的任务实例集合。emitDirect()方法的输入列表与emit()类似，主要区别在于使用前者时，只有由参数taskId所指定的任务实例才能接收到这条消息。SpoutOutputCollector类实际上是一个代理类，本身也封装了一个ISpoutOutputCollector对象，所有的操作实际上都是通过该对象来实现的。除此之外，它还提供了一些重载方法。12345public interface ISpoutOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple, Object messageId); void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple, Object messageId); long getPendingCount();&#125; IRich接口IRichSpout需要同时实现IComponent和ISpout接口，因此它是一个具有Spout功能的组件，其定义如下：123public interface IRichSpout extends ISpout,IComponent&#123; &#125;","categories":[{"name":"源码分析","slug":"源码分析","permalink":"http://tankcat2.com/categories/源码分析/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}]}]}