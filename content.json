{"meta":{"title":"Tankcat","subtitle":"","description":"A cat who drives a tank","author":"Tankcat","url":"http://tankcat2.com"},"pages":[{"title":"","date":"2017-11-26T05:33:11.636Z","updated":"2016-07-10T17:29:36.000Z","comments":true,"path":"404.html","permalink":"http://tankcat2.com/404.html","excerpt":"","text":"404"},{"title":"","date":"2017-11-26T05:33:11.640Z","updated":"2017-03-16T11:25:50.000Z","comments":true,"path":"tool.js","permalink":"http://tankcat2.com/tool.js","excerpt":"","text":"\"use strict\"; var fs = require(\"fs\"); var path = \"photos/\"; fs.readdir(path, function (err, files) { if (err) { return; } var arr = []; (function iterator(index) { if (index == files.length) { fs.writeFile(\"photos/data.json\", JSON.stringify(arr, null, \"\\t\")); console.log('get img success!'); return; } fs.stat(path + files[index], function (err, stats) { if (err) { return; } if (stats.isFile()) { arr.push(files[index]); } iterator(index + 1); }) }(0)); });"},{"title":"Archive","date":"2016-07-10T08:14:45.000Z","updated":"2016-07-17T06:27:18.000Z","comments":true,"path":"archive/index.html","permalink":"http://tankcat2.com/archive/index.html","excerpt":"","text":""},{"title":"Categories","date":"2017-11-26T05:33:11.655Z","updated":"2016-07-17T06:27:32.000Z","comments":true,"path":"categories/index.html","permalink":"http://tankcat2.com/categories/index.html","excerpt":"","text":""},{"title":"About","date":"2017-11-26T05:33:11.645Z","updated":"2016-10-12T15:28:16.000Z","comments":true,"path":"about/index.html","permalink":"http://tankcat2.com/about/index.html","excerpt":"","text":""},{"title":"Photos","date":"2016-09-16T07:07:04.000Z","updated":"2017-05-07T10:29:36.000Z","comments":true,"path":"favorite/index.html","permalink":"http://tankcat2.com/favorite/index.html","excerpt":"","text":"劝君莫惜金缕衣，劝君惜取少年时。花开堪折直须折，莫待无花空折枝。——杜秋娘《金缕衣》 ​​​​ .hexo-image-steam-lazy {display:block;}.hexo-img-stream{width:90%;max-width:1100px;margin:3% auto}div.hexo-img-stream figure{background:#fefefe;box-shadow:0 1px 2px rgba(34,25,25,0.4);margin:0 0.05% 3%;padding:3%;padding-bottom:10px;display:inline-block;max-width:25%}div.hexo-img-stream figure img{border-bottom:1px solid #ccc;padding-bottom:15px;margin-bottom:5px}div.hexo-img-stream figure figcaption{font-size:.9rem;color:#444;line-height:1.5;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}div.hexo-img-stream small{font-size:1rem;float:right;text-transform:uppercase;color:#aaa}div.hexo-img-stream small a{color:#666;text-decoration:none;transition:.4s color}@media screen and (max-width:750px){.hexo-img-stream{column-gap:0}}逛街的两个人 南京市人民政府 行走在平江路 龙之梦的一家饰品店 回家 一天天长大的小叶子 胖嘟嘟的阿拉斯加 姐妹花 苏州图书馆 同德兴的拉面 星巴克的桃桃红茶 夜晚十一点的红房子 阿里家的面 上海城市规划馆 和府捞面 南京大屠杀纪念馆 沙面撸猫 广州的一家青旅 很高兴遇见你 太古汇的索尼体验店 Godiva的双色甜筒 丽娃河畔的樱花 顾村公园赏樱 不知名的紫色小花 五舍楼后的白月季 河畔芦苇 小黄花 河西食堂前的桃李 丽娃桥 依旧耀眼的落日 滴水湖的路标 家里的后院 十八舍前一抹红 云雾缭绕 南师西区操场 修剪的树枝 蝙蝠侠大战超人 缺月挂疏桐 敬文图书馆 中北的蓝天(一) 中北的蓝天(二) 广玉兰 南京97路公交 大行宫地铁站 南师晒太阳的老黄 顺和祥的汤包 爷爷的梅干菜扣肉 左庭右院的牛肉饼 南师西区食堂的炸酱汤面 $('img.hexo-image-steam-lazy').lazyload({ effect:'fadeIn' });"},{"title":"","date":"2017-11-26T05:33:11.672Z","updated":"2017-03-16T11:25:54.000Z","comments":true,"path":"photos/data.json","permalink":"http://tankcat2.com/photos/data.json","excerpt":"","text":"[\"14098405995146380.jpg\",\"227563740595514503.jpg\",\"340809002755226955.jpg\",\"775149960737431740.jpg\",\"IMG_20151027_202853.jpg\",\"IMG_20151105_184339.jpg\",\"IMG_20151105_202323.jpg\",\"spring.jpg\"]"},{"title":"Photos","date":"2017-03-16T08:14:45.000Z","updated":"2017-03-16T13:22:48.000Z","comments":true,"path":"photos/index.html","permalink":"http://tankcat2.com/photos/index.html","excerpt":"","text":""},{"title":"Search","date":"2017-11-26T05:33:11.689Z","updated":"2016-07-17T06:27:46.000Z","comments":true,"path":"search/index.html","permalink":"http://tankcat2.com/search/index.html","excerpt":"","text":""},{"title":"Tags","date":"2016-07-10T12:26:33.000Z","updated":"2016-07-17T06:27:58.000Z","comments":true,"path":"tags/index.html","permalink":"http://tankcat2.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Algorithm专项课程 IV The Master Method","slug":"Course4","date":"2017-12-19T00:29:00.000Z","updated":"2017-12-19T00:32:43.398Z","comments":true,"path":"2017/12/19/Course4/","link":"","permalink":"http://tankcat2.com/2017/12/19/Course4/","excerpt":"这一章节的课程主要介绍Master method，一种通用的分析分治算法运行时间的数学工具，先给出motivation，再介绍形式化描述，接着讲解6个示例，最后讨论Master method的证明。 Motivation / 动机 update：2017/12/18 潜在有用的算法通常需要数学分析来评估其性能，回顾小学时期的整数相乘算法使用$\\Theta(n^2)$的运行时间来计算两个n位整数的乘积$x\\times y$。递归算法是将x分解为$x=10^{\\frac{1}{2}n}a+b$，将y分解为$y=10^{\\frac{1}{2}n}c+d$，则$x\\times y = 10^nac+10^{\\frac{1}{2}n}(ad+bc)+bd$。即递归计算$ac,ad,bc,bd$的乘积即可。令T(n)表示计算两个n位整数乘积过程中操作的数量，基线条件是$T(1)\\leq $常数，对于$n\\geq 1$，有$T(n)\\leq 4T(\\frac{1}{2}n)+O(n)$。此外，一个更好的递归算法是减少一次递归调用，只计算ac,bd与(a+b)(c+d)，而ad+bc=(a+b)(c+d)-ac-bd。这样，对于$n\\geq1$，有$T(n)\\leq3T(\\frac{1}{2}n)+O(n)$。 Formal statement / 形式化叙述 update：2017/12/18 这一节课程视频主要介绍Master method的形式化叙述，可以理解为解决递归问题的一个black box，有一个前提假设，即所有的子问题拥有相同的size。其形式化表述如下： 首先是基线条件Base Case：当n足够小时，T(n)为常数 通常递归调用的情况下，对于较大的n，假设有$T(n)\\leq a\\cdot T(\\frac{n}{b})+O(n^d)$，其中a、b、d都是常数，与n大小无关 a是递归调用的次数($\\geq 1$) b是输入大小收缩系数($&gt; 1$) d是“合并阶段”运行时间的指数($\\geq 0$) 则有$ T(n)=\\begin{cases}O(n^dlogn) &amp;a = b^d \\ O(n^d) &amp; a &lt; b^d \\\\O(n^{log_ba}) &amp; a &gt; b^d \\end{cases}$ 需要注意的是，当$a=b^d$时，big-O里面的log基数省略没写，这是因为基数具体为哪个值并不重要，不同基数带来的变化是常量级别的；而当$a&gt;b^d$时，由于log是在指数的位置，所以基数是哪个值就很重要了，不能忽略。 Examples / 示例 update：2017/12/18 这一节课程主要是利用6个具体示例来说明Master method。 示例1：Merge sort首先确定a、b、d的数值： a=2 b=2 d=1 满足$a=b^d$，因此Merge sort的$T(n)=O(n^dlogn)=O(nlogn)$ 示例2：有序数组的二分查找首先确定a、b、d的数值： a=1 b=2 d=0 和Merge sort不同，只需要将要查找的数与中间元素比较，如果小于中间元素，则递归调用左半部分，否则调用右半部分，所以a的值为1；递归之外的操作就是元素的比较，因此d的值为0。由于满足$a=b^d$，因此有序数组的二分查找的$T(n)=O(n^dlogn)=O(logn)$ 示例3：整数相乘的递归算法I首先确定a、b、d的数值： a=4 b=2 d=1 整数相乘的基础递归算法总共有4次调用，所以a=4；每一次调用的整数位数是原来的一半，所以b=2；递归调用之外的加操作是线性的，所以d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^2)$ 示例4：整数相乘的递归算法II首先确定a、b、d的数值： a=3 b=2 d=1 整数相乘的改进递归算法总共有3次调用，所以a=3；每一次调用的整数位数是原来的一半，所以b=2；递归调用之外的加操作是线性的，所以d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^{log_23})=O(n^{1.59})$ 示例5：Strassen矩阵乘法首先确定a、b、d的数值： a=7 b=2 d=2 Strassen矩阵乘法是递归调用7次$P_1$~$P_7$加减操作，所以a=7；$P_1$~$P_7$子矩阵的规模是原始矩阵的一半，所以b=2；递归调用之外的加操作规模是子矩阵的大小，因此d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^{log_27})=O(n^{2.81})$ 示例6：Fictitious recurrence for $a&lt;b^d$假设a、b、d的数值： a=2 b=2 d=2 由于满足$a&lt;b^d$，因此$T(n)=O(n^d)=O(n^2)$","text":"这一章节的课程主要介绍Master method，一种通用的分析分治算法运行时间的数学工具，先给出motivation，再介绍形式化描述，接着讲解6个示例，最后讨论Master method的证明。 Motivation / 动机 update：2017/12/18 潜在有用的算法通常需要数学分析来评估其性能，回顾小学时期的整数相乘算法使用$\\Theta(n^2)$的运行时间来计算两个n位整数的乘积$x\\times y$。递归算法是将x分解为$x=10^{\\frac{1}{2}n}a+b$，将y分解为$y=10^{\\frac{1}{2}n}c+d$，则$x\\times y = 10^nac+10^{\\frac{1}{2}n}(ad+bc)+bd$。即递归计算$ac,ad,bc,bd$的乘积即可。令T(n)表示计算两个n位整数乘积过程中操作的数量，基线条件是$T(1)\\leq $常数，对于$n\\geq 1$，有$T(n)\\leq 4T(\\frac{1}{2}n)+O(n)$。此外，一个更好的递归算法是减少一次递归调用，只计算ac,bd与(a+b)(c+d)，而ad+bc=(a+b)(c+d)-ac-bd。这样，对于$n\\geq1$，有$T(n)\\leq3T(\\frac{1}{2}n)+O(n)$。 Formal statement / 形式化叙述 update：2017/12/18 这一节课程视频主要介绍Master method的形式化叙述，可以理解为解决递归问题的一个black box，有一个前提假设，即所有的子问题拥有相同的size。其形式化表述如下： 首先是基线条件Base Case：当n足够小时，T(n)为常数 通常递归调用的情况下，对于较大的n，假设有$T(n)\\leq a\\cdot T(\\frac{n}{b})+O(n^d)$，其中a、b、d都是常数，与n大小无关 a是递归调用的次数($\\geq 1$) b是输入大小收缩系数($&gt; 1$) d是“合并阶段”运行时间的指数($\\geq 0$) 则有$ T(n)=\\begin{cases}O(n^dlogn) &amp;a = b^d \\ O(n^d) &amp; a &lt; b^d \\\\O(n^{log_ba}) &amp; a &gt; b^d \\end{cases}$ 需要注意的是，当$a=b^d$时，big-O里面的log基数省略没写，这是因为基数具体为哪个值并不重要，不同基数带来的变化是常量级别的；而当$a&gt;b^d$时，由于log是在指数的位置，所以基数是哪个值就很重要了，不能忽略。 Examples / 示例 update：2017/12/18 这一节课程主要是利用6个具体示例来说明Master method。 示例1：Merge sort首先确定a、b、d的数值： a=2 b=2 d=1 满足$a=b^d$，因此Merge sort的$T(n)=O(n^dlogn)=O(nlogn)$ 示例2：有序数组的二分查找首先确定a、b、d的数值： a=1 b=2 d=0 和Merge sort不同，只需要将要查找的数与中间元素比较，如果小于中间元素，则递归调用左半部分，否则调用右半部分，所以a的值为1；递归之外的操作就是元素的比较，因此d的值为0。由于满足$a=b^d$，因此有序数组的二分查找的$T(n)=O(n^dlogn)=O(logn)$ 示例3：整数相乘的递归算法I首先确定a、b、d的数值： a=4 b=2 d=1 整数相乘的基础递归算法总共有4次调用，所以a=4；每一次调用的整数位数是原来的一半，所以b=2；递归调用之外的加操作是线性的，所以d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^2)$ 示例4：整数相乘的递归算法II首先确定a、b、d的数值： a=3 b=2 d=1 整数相乘的改进递归算法总共有3次调用，所以a=3；每一次调用的整数位数是原来的一半，所以b=2；递归调用之外的加操作是线性的，所以d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^{log_23})=O(n^{1.59})$ 示例5：Strassen矩阵乘法首先确定a、b、d的数值： a=7 b=2 d=2 Strassen矩阵乘法是递归调用7次$P_1$~$P_7$加减操作，所以a=7；$P_1$~$P_7$子矩阵的规模是原始矩阵的一半，所以b=2；递归调用之外的加操作规模是子矩阵的大小，因此d=1。由于满足$a&gt;b^d$，因此整数相乘的基础递归算法的$T(n)=O(n^log_ba)=O(n^{log_27})=O(n^{2.81})$ 示例6：Fictitious recurrence for $a&lt;b^d$假设a、b、d的数值： a=2 b=2 d=2 由于满足$a&lt;b^d$，因此$T(n)=O(n^d)=O(n^2)$ Proof I / 证明 I update：2017/12/18 这一节课程主要是介绍Master method的证明。这个证明是概念性层面上的，理解Master method的三种case的概念性意义以及对于的recursion tree是有很大帮助的。如果理解了，就不用对Master method的三种case进行死记硬背。 证明的前提假设如下： $T(1)\\leq c$ $T(n)\\leq a\\cdot T(\\frac{n}{b})+c\\cdot n^d$ n是b的幂次方 按照Merge sort讲解recursion tree的思路，在第j层，有$a^j$次方个子问题，且每个子问题的输入规模是$\\frac{n}{b^j}$。层数j的取值范围是[0,1,…,$log_bn$]。分析第j层的总工作量为$\\leq a^j\\cdot c\\cdot [\\frac{n}{b^j}]^d=c\\cdot n^d\\cdot (\\frac{a}{b^d})^j$，其中$(\\frac{n}{b^j})^d$是第j层每个子问题的总工作量，对公式进行化简整理，使得与j相关、无关的变量分离。对所以层的工作量求和，即$\\leq c\\cdot n^d\\cdot \\sum_{j=0}^{log_bn}(\\frac{a}{b^d})^j$。后续视频会讲解这个式子与Master method的三种case之间的关联。 Interpretation of the 3 cases / 3种情况的演绎 update：2017/12/18 上一节课程已经介绍了第j层的工作量上界是$c\\cdot n^d\\cdot(\\frac{a}{b^d})^j$。令a为子问题增生的速率RSP，$b^d$是每个子问题工作量的收缩速率RWS，有下面三种结论： 如果RSP&lt;RWS，则每一层工作量随着层数j而递减，recursion tree根节点处的工作量更多，起决定性作用，而根节点的输入规模是n，因此$T(n)=O(n^d)$ 如果RSP&gt;RWS，则每一层工作量随着层数j而递增，recursion tree叶子节点处的工作量更多，起决定性作用 如果RSP=RWS，则每一层的工作量是相同的，而层数规模是$O(logn)$，每一层的工作量为$O(n^d)$，因此$T(n)=O(n^dlogn)$ Proof II / 证明 II update：2017/12/19 前面的两节课程分别分析了recursion tree的工作量(放大聚焦于给定的第j层工作量，然后对所有层的工作量求和)，以及赋予它三种直观语义。这一节课程继续完成Master method的精确证明。 首先回顾一下总工作量：$\\leq c\\cdot n^d\\cdot \\sum_{j=0}^{log_bn}(\\frac{a}{b^d})^j \\star$ 当$a=b^d$时，$\\star=c\\cdot n^d (log_bn+1)=O(n^dlogn)$ 在讨论另外两种语义时，先回顾一下等比数列求和，当$r\\neq1$时，我们有$1+r+r^2+…+r^k=\\frac{r^{k+1}-1}{r-1}$。当$r1$时，和$\\leq r^k(1+\\frac{1}{r-1})$ 当$a&lt;b^d$，即$r&lt;1$，则$\\star=O(n^d)$ 当$a&gt;b^d$，即$r&gt;1$，则$\\star=O(n^d\\cdot(\\frac{a}{b^d})^{log_bn})$，由于$b^{-dlog_bn}=(b^{log_bn})^-d=n^{-d}$，所以$\\star=O(n^d\\cdot a^{log_bn}\\cdot n^{-d})$，而$a^{log_bn}$是recursion tree叶子节点的规模，且$a^{log_bn}=n^{log_ba}$(左右两边同时取对数logb)，所以$\\star=O(n^log_ba)$ ​","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Algorithm Specialization","slug":"Algorithm-Specialization","permalink":"http://tankcat2.com/tags/Algorithm-Specialization/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Algorithm专项课程 III Dive and Conquer Algorithms","slug":"Course3","date":"2017-12-18T09:05:00.000Z","updated":"2017-12-19T00:33:00.480Z","comments":true,"path":"2017/12/18/Course3/","link":"","permalink":"http://tankcat2.com/2017/12/18/Course3/","excerpt":"这一章节的课程围绕Divide and conquer分治算法的思想，讨论三个比较有意思的例子，分别是： 找出给定数组中的逆序对，这个问题和计算两个排名表的相似度有关，因此又与推荐算法中的协同过滤有关 矩阵乘法，利用Strassen提出的递归算法 在给定平面中找出最近点对，三个问题中最具挑战性的一个 Counting inversions I/ 统计逆序对I update：2017/12/12 这节课的视频复习了分治思想的paradigm，即先将大问题分解为小问题，再递归求解小问题，最后合并每个小问题的求解结果。此外，继续给出一个具体的问题：找出一个数组中所有的逆序对。逆序对的定义是：数据对(i,j)，使得$iA[j]$。给定输入数组[1,3,5,2,4,6]，可以看出其中逆序对为(3,2)，(5,2)和(5,4)。可以通过如右图所示的连线方法来求解逆序对的数量，第一行是有序，第二行是原序，将数字相同的连接起来，则交叉点的数量就是逆序对的个数。有个题外话，为什么要计算逆序对？其实它的应用场景可以是两个排名list的相似度计算，比如协同过滤。 逆序对求解，这里只讲两种方法，暴力求解与分治求解。 暴力求解：直接嵌套循环，逐个判断，时间复杂度是$O(n^2)$ 递归求解：先定义inversion，分为三种，如下图所示。求解过程就是先递归对左半数组求解left inversion的数量x，再递归对右半数组求解right inversion的数量y，然后求解split inversion的数量z，最后返回$x+y+z$之和就是最终结果了。这节课视频暂时没有讲解计算split inversion的实现，但是目标是以线性时间的复杂度$O(n)$来实现。 Counting inversions II/ 统计逆序 II update：2017/12/13 这一节课的视频主要解决上一个视频中遗留下来的问题，即如何统计split inversions。关键点在于利用Merge sort的思路，在递归调用时，除了统计左右半数组的left inversion和right inversion外，还需要顺便对左右半数据进行排序。这就是在递归时多做一点工作来简化split inversion的统计。为什么这么做呢？因为可以发现Merge sort的merge步骤可以自然地用来统计split inversion数量。 如果左半数组B中的所有元素都小于右半数组C中的元素，那么split inversion的个数就是0。按照下图所示的merge操作，可以总结出一个结论，如果右半数组中的元素y比左半数组中的某个元素x小，则x之后剩余的所有元素都能和y组成逆序对。这是显然的，因为左右半数组前提已经是有序的了。这个统计split inversion的运行时间也是线性的，因为前面的课程已经分析了merge操作是$O(n)$，统计求和本身也是$O(n)$，则$O(n)+O(n)=O(n)$。递归调用和Merge sort一样，则找出数组中的逆序对，这个问题的总运行时间是$O(n\\cdot log n)$。","text":"这一章节的课程围绕Divide and conquer分治算法的思想，讨论三个比较有意思的例子，分别是： 找出给定数组中的逆序对，这个问题和计算两个排名表的相似度有关，因此又与推荐算法中的协同过滤有关 矩阵乘法，利用Strassen提出的递归算法 在给定平面中找出最近点对，三个问题中最具挑战性的一个 Counting inversions I/ 统计逆序对I update：2017/12/12 这节课的视频复习了分治思想的paradigm，即先将大问题分解为小问题，再递归求解小问题，最后合并每个小问题的求解结果。此外，继续给出一个具体的问题：找出一个数组中所有的逆序对。逆序对的定义是：数据对(i,j)，使得$iA[j]$。给定输入数组[1,3,5,2,4,6]，可以看出其中逆序对为(3,2)，(5,2)和(5,4)。可以通过如右图所示的连线方法来求解逆序对的数量，第一行是有序，第二行是原序，将数字相同的连接起来，则交叉点的数量就是逆序对的个数。有个题外话，为什么要计算逆序对？其实它的应用场景可以是两个排名list的相似度计算，比如协同过滤。 逆序对求解，这里只讲两种方法，暴力求解与分治求解。 暴力求解：直接嵌套循环，逐个判断，时间复杂度是$O(n^2)$ 递归求解：先定义inversion，分为三种，如下图所示。求解过程就是先递归对左半数组求解left inversion的数量x，再递归对右半数组求解right inversion的数量y，然后求解split inversion的数量z，最后返回$x+y+z$之和就是最终结果了。这节课视频暂时没有讲解计算split inversion的实现，但是目标是以线性时间的复杂度$O(n)$来实现。 Counting inversions II/ 统计逆序 II update：2017/12/13 这一节课的视频主要解决上一个视频中遗留下来的问题，即如何统计split inversions。关键点在于利用Merge sort的思路，在递归调用时，除了统计左右半数组的left inversion和right inversion外，还需要顺便对左右半数据进行排序。这就是在递归时多做一点工作来简化split inversion的统计。为什么这么做呢？因为可以发现Merge sort的merge步骤可以自然地用来统计split inversion数量。 如果左半数组B中的所有元素都小于右半数组C中的元素，那么split inversion的个数就是0。按照下图所示的merge操作，可以总结出一个结论，如果右半数组中的元素y比左半数组中的某个元素x小，则x之后剩余的所有元素都能和y组成逆序对。这是显然的，因为左右半数组前提已经是有序的了。这个统计split inversion的运行时间也是线性的，因为前面的课程已经分析了merge操作是$O(n)$，统计求和本身也是$O(n)$，则$O(n)+O(n)=O(n)$。递归调用和Merge sort一样，则找出数组中的逆序对，这个问题的总运行时间是$O(n\\cdot log n)$。 Strassen’s subcubic matrix multiplication algorithm这一节课的视频讲解了利用分治思想来求解矩阵乘法，先定义了问题是什么，然后讲了最naivee的求解方法，接着讲了基础分治算法，最后引出1969年的Strassen矩阵乘积算法。 Naivee：先假设涉及的矩阵维度都是n，$X\\cdot Y=Z$，$Z_{i,j}=\\sum_{k=1}^{n}X_{i,k}\\times Y_{k,j}$，即结果矩阵Z的第i行第j列的元素是矩阵X的第i行与矩阵Y的第j列的点积，如下图所示。求点积的运行时间是$\\Theta(n)$，因此利用矩阵乘法的定义求解乘积的运行时间是$\\Theta(n^3)$。 分治求解：思想是将X划分成四个子矩阵ABCD，将Y划分成四个子矩阵EFGH，因此A-H都是$\\frac{1}{2}n\\times \\frac{1}{2}n$的矩阵。这样，$X\\cdot Y=\\left\\{\\begin{matrix}AE+BG&amp; AF+BH\\ CE+DG &amp; CF+DH\\end{matrix} \\right\\}$。因此，求解过程可以分为两部： Step-1，递归计算出上面8个子矩阵乘积 Step-2，求和，这一步的运行时间为$\\Theta(n^2)$ 实际上，这个算法的运行时间仍然是$\\Theta(n^3)$，但是这不是坏事，回顾之前利用分治思想计算整数乘积，其中有一个步骤是减少一次递归调用。按照这个想法，引出下面要讲的Strassen算法。 Strassen求解：这是一个经典的算法，1969年提出的，它只递归求解7个子矩阵乘积，虽然只少了一个，但是运行时间却是sub-cubic的。求解过程和分治求解一致，稍微不同的就是step-1只求解7个，step-2除了addition还有一部分substraction操作。那么，7个子矩阵乘积是什么？利用$P_1-P_7$表示如下。 $P_1=A(F-H)$ $P_2=(A+B)H$ $P_3=(C+D)E$ $P_4=D(C+E)$ $P_5=(A+D)(E+H)$ $P_6=(B-D)(G+H)$ $P_7=(A-C)(E+F)$ 则利用下面的方法可以得出$X\\cdot Y=\\left\\{\\begin{matrix}AE+BG&amp; AF+BH\\ CE+DG &amp; CF+DH\\end{matrix} \\right\\}=\\left\\{\\begin{matrix}P_5+P_4-P_2+P_6 &amp; P_1+P_2\\ P_3+P_4 &amp; P_1+P_5-P_3-P_2\\end{matrix}\\right\\}$ 关于后面的公式如何得到的，Strassen是如何想到这个方法的，以及它的时间复杂度分析，后续课程将给出！ Closest pair I / 最近点对问题 I update：2017/12/17 这节课的视频讲解的是最近点对问题，要求找出给定平面上距离最近的两个点。这个问题是分治求解问题的经典算法之一，在其他应用领域也会经常遇到这个问题，比如计算机图形学、机器人等。 首先是问题的形式化定义：给定一个平面内的若干个点，每个点由其横坐标x与纵坐标y的序列定义。两个点的距离是指欧氏距离(坐标差的平方和开根号)。最近点对问题就是要求找出一对点，使得它们的距离是所有点对中最小的。 求解的前提假设：不存在ties，即所有端点的x坐标不同，y坐标也不同。没有这个前提假问题也能解决，这里只是为了方便算法的论述。 沿袭前面的课程统计数组中的inversion求解过程：如果允许平方的运行时间，依旧可以利用暴力求解找出最近点对，即嵌套循环遍历所有不同的点对，计算各自的距离，最终找出最短的。但是，是否存在与counting inversion一样O(nlogn)的求解？在1-dimension的情况下，所有点在一条坐标轴上，可以先排序O(nlogn)，再遍历一遍找出最短的O(n)。 2-dimension情况下O(nlogn)的high-level求解： Preprocessing：对输入点集按照x坐标排序得到点集$P_x$，再根据y坐标排序得到点集$P_y$ 利用分治求解 2-dimension情况下O(nlogn)的具体求解ClosestPair($P_x，P_y$)： 根据x坐标把原始点集划分成左右两部分，Q与R，再利用Preprocessing分别得到x与y轴有序的点集$Q_x，Q_y，R_x，R_y$ 递归调用ClosestPair($Q_x，Q_y$)=$(p_1,q_1)$得到左半点集中的最近点对 递归调用ClosestPair($R_x，R_y$)=$(p_2，q_2)$得到右半点集中的最近点对 $\\delta=min(d(p_1,q_1)，d(p_2,q_2))$ 调用CloestSplitPair($P_x，P_y，\\delta$)得到最近点对($p_3，q_3$)，因为两个点可能各自位于Q与R两个点集中，这一步的运行时间是O(n)线性的 返回$min(d(p_1,q_1)，d(p_2,q_2)，d(p_3,q_3))$ 求解Split closest pair的subroutine要求：O(n)线性运行时间，始终是正确的无论最近点对是否是split closest pair，具体求解过程如下CloestSplitPair($P_x，P_y，\\delta​$)： 过滤，修剪掉部分不需要的点，只考虑部分点集，而这部分点集位于一条位于整体点集中部的垂直的宽带中(令$\\bar{x}=$原始点集左半部分的最大x坐标，这个操作是O(1)的，因为在Preprocessing中已经对左半点集按照x坐标进行了排序) 利用$\\delta$来决定垂直宽带的宽度：$2\\times \\delta$，即以$\\bar{x}$为中心先，左右两边各取$\\delta$得宽度，如下图所示。这样就忽略到不在这个宽带中得点，接下来的操作只针对位于宽带中的部分点，这部分点的x坐标的上下界为[$\\bar{x}-\\delta，\\bar{x}+\\delta$] 对宽带中的点集按照y坐标排序得到点集$S_y$，可以直接从$P_y$中按照上一步的要求进行提取，因此这一步的操作是O(n)的 遍历$S_y$中的点，找出距离小于$\\delta$的最短点对，具体过程如下图所示。首先是初始化$best$与$best_pair$，分为用来记录最短点对的距离与最短点对本身；接着是嵌套循环遍历$S_y$，由于内层嵌套的迭代次数是常量7，因此内层的运行时间是O(1)，外层的运行时间是O(n)，总体的运行时间依旧是O(n)。如果最终找出距离小于$\\delta$的点对(p,q)，则p与q在$S_y$中至多相间7个点。以上的证明在下个视频课程给出。 Closest pair II / 最近点对问题 II update：2017/12/18 这节视频主要是证明上一节求解ClosestSplitPair中的结论：令$p\\in Q, q\\in R$是一对split pair，且$d(p,q)&lt;\\delta$，则有 p和q是$S_y$中的两个点 p与q在$S_y$中的位置最多间隔7个点 令p的坐标是$(x_1,y_1)$，q的坐标是$(x_2，y_2)$，p来自左半点集Q，q来自右半点集R，且$d(p,q)&lt;\\delta$，则有$|x_1-x_2|\\leq \\delta$，且$|y_1-y_2|\\leq \\delta$。 证明第一个结论，就是要证明$x_1,x_2\\in [\\bar{x}-\\delta，\\bar{x}+\\delta]$。由于p来自左半点集Q，则必有$x_1\\leq\\bar{x}$，同样q来自右半点集R，则有$x_2\\geq\\delta$。由于这里与y轴坐标无关，可以借助1-dimension的坐标轴来证明这个。如下图所示，由于$|x_1-x_2|\\leq\\delta$，所以如果$x_1&lt;\\bar{x}-\\delta$，则$x_2$肯定必须小于$\\bar{x}$，这就矛盾了。同样，如果$x_2&gt;\\bar{x}-\\delta$，则$x_1$肯定必须大于$\\bar{x}$。因此，$x_1$和$x_2$肯定介于$\\bar{x}-\\delta$与$\\bar{x}+\\delta$之间。 证明第二个结论，关键是画出如下图所示的8个box，这些box的横坐标必定包含p和q两个点，介于$\\bar{x}-\\delta$与$\\bar{x}+\\delta$之间，box的bottom是p和q两个中较小的y坐标，$|y_1-y_2|\\leq \\delta$，因此单个盒子的高度是$\\frac{1}{2}\\delta$。在证明之前，先证明两个辅助定理。 辅助定理1：纵坐标位于p与q之间、且属于$S_y$的点必定存在于这8个box之中。首先关于$S_y$的定义，x坐标必须满足介于$\\bar{x}-\\delta$与$\\bar{x}+\\delta$之间；其次，p与q的y坐标之差绝对值是小于$\\delta$的，这是upper bound。 辅助定理2：每个box中至多存在一个点。可以用反证法证明，假设a和b两个点存在于同一个盒子中，那个它们必定要么都来自Q，要么都来自R，且它们的距离$d(a,b)\\leq \\frac{\\sqrt{2}}{2}\\delta\\leq\\delta$，这就与原始条件(split pair分别来自Q与R且$\\delta$本身已经是Q或者R中最短的距离)相矛盾。 结合以上两个辅助定理，可以推出包含p和q在内，这8个box中总共至多包含8个点。因此p与q至多相隔7个点。","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Algorithm Specialization","slug":"Algorithm-Specialization","permalink":"http://tankcat2.com/tags/Algorithm-Specialization/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Algorithm专项课程—Problem Sets and Programming Assignments","slug":"Assignment1","date":"2017-12-12T11:00:00.000Z","updated":"2017-12-18T09:09:02.594Z","comments":true,"path":"2017/12/12/Assignment1/","link":"","permalink":"http://tankcat2.com/2017/12/12/Assignment1/","excerpt":"Problems set 3-way-Merge Sort : Suppose that instead of dividing in half at each step of Merge Sort, you divide into thirds, sort each third, and finally combine all of them using a three-way merge subroutine. What is the overall asymptotic running time of this algorithm? (Hint: Note that the merge step can still be implemented in O(n) time.) $n log (n)$ $n(log(n))^2$ $n^2log(n)$ n 这道题是问如果Merge sort每次是把一个数组划分成三个部分，那么运行时间复杂度是多少。尽管是每个大问题划分成三个小问题，但是反映到recursion tree上，树的深度依旧是logarithmic，且每一层的merge操作的运行时间依旧是线性的，所以最终的复杂度仍然是$O(nlog(n))$。 You are given functions f and g such that $f(n)=O(g(n))$. Is $f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n))) $? (Here c is some positive constant.) You should assume that f and g are nondecreasing and always bigger than 1. True False Sometimes yes, sometimes no, depending on the constant $c$ Sometimes yes, sometimes no, depending on the function $f$ and $g$ 这道题是问如果函数$f$与$g$满足$f(n)=O(g(n))$，那么$f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n))) $是否成立。根据条件可知，存在$c_1，n_0$使得当$n\\geq n_0$时有$f(n)\\leq c_1\\cdot g(n)$。假设所问结论成立，则存在$c_2，n_0$使得当$n\\geq n_0$时有$f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n)))$，即$f(n)\\cdot log_2(f(n)^c)=c_2\\cdot g(n)\\cdot log_2(g(n))$，化简规约一下，$c\\cdot f(n)\\cdot log_2(f(n))=c_2\\cdot g(n)\\cdot log_2(g(n))$，将$f(n)\\leq c_1\\cdot g(n)$代入，显然是可以成立的 Assume again two (positive) nondecreasing functions f and gsuch that $f(n)=O(g(n))$. Is $2^{f(n)}=O(2^{g(n)})$ ? (Multiple answers may be correct, you should check all of those that apply.) Always Never Sometimes yes, sometimes no (depending on $f$ and $g$) Yes if $f(n)\\leq g(n)$ for all sufficiently large $n$ 这道题是问如果函数$f$与$g$满足$f(n)=O(g(n))$，那么 $2^{f(n)}=O(2^{g(n)})$ 是否成立。我们可以通过一些特殊的例子来解答。 假设$f(n)=g(n)=n$，这种情况下，$2^n=O(2^n)$是显然成立的 假设$f(n)=10n，g(n)=n$，这种情况下，显然有$2^{10n}\\neq O(2^n)$，这个在Course2课程的案例中介绍过证明 综上所述，这个结论不总是对的，也不总是错的。 k-way-Merge Sort. Suppose you are given $k$ sorted arrays, each with $n$ elements, and you want to combine them into a single array of $kn$ elements. Consider the following approach. Using the merge subroutine taught in lecture, you merge the first 2 arrays, then merge the $3^{rd}$ given array with this merged version of the first two arrays, then merge the $4^{th}$ given array with the merged version of the first three arrays, and so on until you merge in the final ($k^{th}$) input array. What is the running time taken by this successive merging algorithm, as a function of $k$ and $n$? (Optional: can you think of a faster way to do the k-way merge procedure ?) $\\Theta(nlog(k))$ $\\Theta(nk^2)$ $\\Theta(n^2k)$ $\\Theta(nk)$ 这道题是问在k-way归并排序中，如果给定k个有序数组，每个数组有n个元素，现在要将它们合并成一个包含$kn$个元素的大数组，归并操作如下：首先归并前两个数组，得到的新数组继续和第三个数组合并，依此类推。问这个连续的归并操作的运行时间复杂度是多少。每次归并操作都是线性时间$O(kn)$的，且有k次归并，因此上界的时间复杂度是$O(k^2n)$。对于下界，每次至少有$\\frac{1}{2}k$个元素参与合并，因此下界运行时间是$\\Omega(nk^2)$。综上所述，运行时间满足$\\Theta(nk^2)$ Arrange the following functions in increasing order of growth rate (with g(n) following $f(n)$ in your list if and only if $f(n)=O(g(n)))$. a) $\\sqrt n$ b)10n c) $n^{1.5}$ d)$2^{\\sqrt log(n)}$ e)$n^{\\frac{5}{3}}$ 这道题是要求把以下五个函数按照增长率由小到大排序。答案是$2^{\\sqrt log(n)} \\leq \\sqrt n \\leq n^{1.5} \\leq n^{\\frac{5}{3}} \\leq 10^n$。前面两个的证明可以是左右两边同时去对数。后面几个显然成立。","text":"Problems set 3-way-Merge Sort : Suppose that instead of dividing in half at each step of Merge Sort, you divide into thirds, sort each third, and finally combine all of them using a three-way merge subroutine. What is the overall asymptotic running time of this algorithm? (Hint: Note that the merge step can still be implemented in O(n) time.) $n log (n)$ $n(log(n))^2$ $n^2log(n)$ n 这道题是问如果Merge sort每次是把一个数组划分成三个部分，那么运行时间复杂度是多少。尽管是每个大问题划分成三个小问题，但是反映到recursion tree上，树的深度依旧是logarithmic，且每一层的merge操作的运行时间依旧是线性的，所以最终的复杂度仍然是$O(nlog(n))$。 You are given functions f and g such that $f(n)=O(g(n))$. Is $f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n))) $? (Here c is some positive constant.) You should assume that f and g are nondecreasing and always bigger than 1. True False Sometimes yes, sometimes no, depending on the constant $c$ Sometimes yes, sometimes no, depending on the function $f$ and $g$ 这道题是问如果函数$f$与$g$满足$f(n)=O(g(n))$，那么$f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n))) $是否成立。根据条件可知，存在$c_1，n_0$使得当$n\\geq n_0$时有$f(n)\\leq c_1\\cdot g(n)$。假设所问结论成立，则存在$c_2，n_0$使得当$n\\geq n_0$时有$f(n)\\cdot log_2(f(n)^c)=O(g(n)\\cdot log_2(g(n)))$，即$f(n)\\cdot log_2(f(n)^c)=c_2\\cdot g(n)\\cdot log_2(g(n))$，化简规约一下，$c\\cdot f(n)\\cdot log_2(f(n))=c_2\\cdot g(n)\\cdot log_2(g(n))$，将$f(n)\\leq c_1\\cdot g(n)$代入，显然是可以成立的 Assume again two (positive) nondecreasing functions f and gsuch that $f(n)=O(g(n))$. Is $2^{f(n)}=O(2^{g(n)})$ ? (Multiple answers may be correct, you should check all of those that apply.) Always Never Sometimes yes, sometimes no (depending on $f$ and $g$) Yes if $f(n)\\leq g(n)$ for all sufficiently large $n$ 这道题是问如果函数$f$与$g$满足$f(n)=O(g(n))$，那么 $2^{f(n)}=O(2^{g(n)})$ 是否成立。我们可以通过一些特殊的例子来解答。 假设$f(n)=g(n)=n$，这种情况下，$2^n=O(2^n)$是显然成立的 假设$f(n)=10n，g(n)=n$，这种情况下，显然有$2^{10n}\\neq O(2^n)$，这个在Course2课程的案例中介绍过证明 综上所述，这个结论不总是对的，也不总是错的。 k-way-Merge Sort. Suppose you are given $k$ sorted arrays, each with $n$ elements, and you want to combine them into a single array of $kn$ elements. Consider the following approach. Using the merge subroutine taught in lecture, you merge the first 2 arrays, then merge the $3^{rd}$ given array with this merged version of the first two arrays, then merge the $4^{th}$ given array with the merged version of the first three arrays, and so on until you merge in the final ($k^{th}$) input array. What is the running time taken by this successive merging algorithm, as a function of $k$ and $n$? (Optional: can you think of a faster way to do the k-way merge procedure ?) $\\Theta(nlog(k))$ $\\Theta(nk^2)$ $\\Theta(n^2k)$ $\\Theta(nk)$ 这道题是问在k-way归并排序中，如果给定k个有序数组，每个数组有n个元素，现在要将它们合并成一个包含$kn$个元素的大数组，归并操作如下：首先归并前两个数组，得到的新数组继续和第三个数组合并，依此类推。问这个连续的归并操作的运行时间复杂度是多少。每次归并操作都是线性时间$O(kn)$的，且有k次归并，因此上界的时间复杂度是$O(k^2n)$。对于下界，每次至少有$\\frac{1}{2}k$个元素参与合并，因此下界运行时间是$\\Omega(nk^2)$。综上所述，运行时间满足$\\Theta(nk^2)$ Arrange the following functions in increasing order of growth rate (with g(n) following $f(n)$ in your list if and only if $f(n)=O(g(n)))$. a) $\\sqrt n$ b)10n c) $n^{1.5}$ d)$2^{\\sqrt log(n)}$ e)$n^{\\frac{5}{3}}$ 这道题是要求把以下五个函数按照增长率由小到大排序。答案是$2^{\\sqrt log(n)} \\leq \\sqrt n \\leq n^{1.5} \\leq n^{\\frac{5}{3}} \\leq 10^n$。前面两个的证明可以是左右两边同时去对数。后面几个显然成立。 Programming assignmentIn this programming assignment you will implement one or more of the integer multiplication algorithms described in lecture. To get the most out of this assignment, your program should restrict itself to multiplying only pairs of single-digit numbers. You can implement the grade-school algorithm if you want, but to get the most out of the assignment you’ll want to implement recursive integer multiplication and/or Karatsuba’s algorithm. So: what’s the product of the following two 64-digit numbers? 3141592653589793238462643383279502884197169399375105820974944592 2718281828459045235360287471352662497757247093699959574966967627 这道题是要求利用前面课程所讲的分治法Karatsuba整数求乘积来求解，具体的代码如下： 1234567891011121314151617181920212223242526public class Karatsuba &#123; public static BigInteger karatsuba(BigInteger x, BigInteger y) &#123; int N = Math.max(x.bitLength(), y.bitLength()); if (N &lt;= 2000) return x.multiply(y); N = (N / 2) + (N % 2); BigInteger b = x.shiftRight(N); BigInteger a = x.subtract(b.shiftLeft(N)); BigInteger d = y.shiftRight(N); BigInteger c = y.subtract(d.shiftLeft(N)); BigInteger ac = karatsuba(a, c); BigInteger bd = karatsuba(b, d); BigInteger abcd = karatsuba(a.add(b), c.add(d)); return ac.add(abcd.subtract(ac).subtract(bd).shiftLeft(N)).add(bd.shiftLeft(2*N)); &#125; public static void main(String[] args) &#123; BigInteger a = new BigInteger(\"3141592653589793238462643383279502884197169399375105820974944592\"); BigInteger b = new BigInteger(\"2718281828459045235360287471352662497757247093699959574966967627\"); BigInteger c = karatsuba(a, b); System.out.println(c.toString()); &#125;&#125;","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Algorithm Specialization","slug":"Algorithm-Specialization","permalink":"http://tankcat2.com/tags/Algorithm-Specialization/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Algorithm专项课程 II  Asymptotic Analysis","slug":"Course2","date":"2017-12-12T10:55:00.000Z","updated":"2017-12-18T09:07:47.019Z","comments":true,"path":"2017/12/12/Course2/","link":"","permalink":"http://tankcat2.com/2017/12/12/Course2/","excerpt":"The gist / 要旨 update：2017/12/11 Asymptotic analysis是设计和分析算法的vocabulary，比如大O表示法： 从较高的层次分析算法的sweet spot 足够coarse来压缩依赖于系统/编程语言/编译器的细节 足够sharp来进行不同算法之间的有效对比，尤其是针对大规模输入集 High-level Idea：比如$6\\cdot n\\cdot log_2n$== $n\\cdot log n$，也就是说运行时间是$O(n\\cdot log n)$ 压缩常数因子：依赖于系统/编程语言/编译器等 压缩低阶因子：与输入规模的增大无关 举了四个例子： 判断数组A中是否包含给定的整数t，$O(n)$ 判断数组A或者B中是否包含给定的整数t，$2\\cdot n = O(n)​$ 判断数组A和B中是否包含相同的整数，$O(n^2)$ 判断数组A中是否包含重复的整数，$\\frac{1}{2}n\\cdot (n+1) = O(n^2)$ Big-Oh notation / 大O表示法 update：2017/12/11 这一节课讲解大O表示法的定义。令T(n)是n的函数f(n)，通常表示一个算法的worst case运行时间，那么问题是：什么时候$T(n)=O(f(n))$？答案是，当n足够大，最终T(n)的上限是一个常数与f(n)的乘积。下图所示，给出形式化定义： 当存在常数c和$n_0$，对于所有的$n\\geq n_0$，有$T(n)\\leq c\\cdot f(n)$，则称$T(n) = O(f(n))$。其中，常数c和$n_0$与n无关。","text":"The gist / 要旨 update：2017/12/11 Asymptotic analysis是设计和分析算法的vocabulary，比如大O表示法： 从较高的层次分析算法的sweet spot 足够coarse来压缩依赖于系统/编程语言/编译器的细节 足够sharp来进行不同算法之间的有效对比，尤其是针对大规模输入集 High-level Idea：比如$6\\cdot n\\cdot log_2n$== $n\\cdot log n$，也就是说运行时间是$O(n\\cdot log n)$ 压缩常数因子：依赖于系统/编程语言/编译器等 压缩低阶因子：与输入规模的增大无关 举了四个例子： 判断数组A中是否包含给定的整数t，$O(n)$ 判断数组A或者B中是否包含给定的整数t，$2\\cdot n = O(n)​$ 判断数组A和B中是否包含相同的整数，$O(n^2)$ 判断数组A中是否包含重复的整数，$\\frac{1}{2}n\\cdot (n+1) = O(n^2)$ Big-Oh notation / 大O表示法 update：2017/12/11 这一节课讲解大O表示法的定义。令T(n)是n的函数f(n)，通常表示一个算法的worst case运行时间，那么问题是：什么时候$T(n)=O(f(n))$？答案是，当n足够大，最终T(n)的上限是一个常数与f(n)的乘积。下图所示，给出形式化定义： 当存在常数c和$n_0$，对于所有的$n\\geq n_0$，有$T(n)\\leq c\\cdot f(n)$，则称$T(n) = O(f(n))$。其中，常数c和$n_0$与n无关。 Basic examples / 基础示例 update：2017/12/11 这节课的视频给出两个有关大O表示法的示例证明。 利用反证法证明 Big omega and theta / 大$\\Omega$ 和大$\\Theta$ update：2017/12/11 前面两节课已经讲了$O()$表示法的形式化定义与基础示例证明，这节课视频再介绍三个相关的表示法。 $\\Omega()$表示法：关注的对象是运行时间的下界，形式化定义是如果存在常数$c，n_0$使得对于任意$n\\geq n_0$都有$T(n)\\geq c\\cdot f(n)$，则$T(n)=\\Omega(f(n))$。示意图如下图所示。 $\\Theta()$表示法：关注的对象是同时满足$O()$和$\\Omega()$，形式化定义是存在常数$c_1，c_2，n_0$使得对于任意$n\\geq n_0$都有$c_1 \\cdot f(n)\\leq T(n)\\leq c_2\\cdot f(n)$。 $o()$表示法：和$O()$有所区别，形式化定义是对于所有的常数$c$，存在常数$n_0$使得对于任意$n\\geq n_0$都有$T(n)\\leq c\\cdot f(n)$。 大多数情况下，还算关注$O()$表示法，因为设计者最关心的还是运行时间的上界。 最后需要说明的是，这些表示法并不是算法设计者或者计算机科学家发明的，早在19世纪就出现了，但是作为描述增长速率的标准语言，却是D.E. Knuth在1976年提出的$O，\\Omega，\\Theta$。","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Algorithm Specialization","slug":"Algorithm-Specialization","permalink":"http://tankcat2.com/tags/Algorithm-Specialization/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Algorithm专项课程 I Introduction","slug":"Course1","date":"2017-12-11T07:11:00.000Z","updated":"2017-12-18T09:08:07.284Z","comments":true,"path":"2017/12/11/Course1/","link":"","permalink":"http://tankcat2.com/2017/12/11/Course1/","excerpt":"Introduction / 简介 update: 2017/12/05 求解：两个n位整数相乘 输入：两个n位的整数x与y 输出：两者的乘积 解决方案： Naive方法：确定基本运算，即y的每一位都要与x中的n位相乘，就是n次基本运算了，乘积十位上的数字还要进上去，这就涉及一些额外的加法运算，但是总之在任何情况下，基本运算的总量最多是2n。同理，得到每一部分乘积最多都需要2n步运算，而部分乘积的总次数为n，即要做$2n^2$次基本运算。然而还没有结束，还要把这些部分乘积累加起来，才能得到最终结果，这一步的求和运算量最多也是$2n^2$次，因此计算两个n位整数的乘积的基本运算操作量为$4n^2$，是输入长度n的二次函数。 Karatsuba方法：利用分治的思想，把x表示为ab，y表示成cd，即a=56，b=78，c=12，d=34。通过以下前三个递归步骤，利用$x\\cdot y = 10000\\cdot a\\cdot c + 100\\cdot a\\cdot d + 100\\cdot b\\cdot c + b\\cdot d$这一思想来求得最终的结果。可以发现，第三步化简之后就是ad+bc，但是在具体操作时不是分别计算ad和bc的乘积再求和，而是使用化简之前的表示，先各自求得a与b之和、c与d之和，然后求积，最后减去已知的ac与bd。这样是为了减少一次递归次数，毕竟加减法才是最基本的运算。 形式化的方式来归纳以下，就是： $x = 10^{\\frac{1}{2}}\\cdot a + b$，$y= 10^{\\frac{1}{2}}\\cdot c + d$ $x\\cdot y=10^{n}a\\cdot c + 10^{\\frac{1}{2}}(a\\cdot d+ b\\cdot c) + b\\cdot d$，$n=4$ ($\\star$) 递归计算ac，ad，bc，bd，然后代入上面的星式求得最终乘积 为了减少一次递归运算，不需要单独计算ad与bc的乘积 对于一个优秀的算法设计者而言，最重要的原则就是拒绝满足。 Perhaps the most important principle for the good algorithm designer is to refuse to be content. The algorithm design space is surprisingly rich! 算法的设计空间，比我们想象中的要广阔得多！ 讲解模式： 确定输入与输出 给出解决方案，即算法，使得输入转化为输出 About the course /关于这门课程 update: 2017/12/06 课程涵盖了5个话题： 设计与分析算法性能所需要的基本知识，比如大O表示法 分治算法的设计与分析，使用场景很多，比如上一节课提到的Karatsuba算法，还有其他的比如排序、矩阵乘法等，需要分析类似递归算法的运行之间 随机化算法的设计与分析，涉及到快速排序、图分解、哈希等 图论分析的基本知识，涉及计算连通信息量、最短路径、社交网络的结构等 基本数据结构的实现与运用，涉及堆、平衡二叉搜索树、哈希表及其变种，比如布隆过滤器等 后续课程可能会涵盖的话题： 贪心算法，涉及最小生成树、调度问题和信息编码理论 动态规划算法，涉及基因序列和社交网络中的最短路径 NP完全问题，涉及是什么以及解法 能够解决特殊问题的快速算法 高效的有可证效率的回溯算法 具有指数时间复杂度的算法，本质上会比暴力搜索优化 从这门课学到什么：视频里说了不少，我挑我最感兴趣且觉得最重要的几点说说 Become a better programmer.虽说读博对代码技巧要求未必很高，但还是希望自己除了学术分析、写作之上，有扎实的代码功底。毕竟还是希望自己毕业之后能去工业界待一段时间的。 Shapen mathematica analytical skills.这是实实在在我需要提神的一项技能！不仅老板指出来了，我自己也深有感触，写paper时构造不出定理与证明，实在是一大败笔！ Explain why things are the way the are, why we analyze the algorithms in the way that we do. 除了设计一个高效的算法，更重要的是你需要能够给别人讲懂，为什么是这样设计，为什么用这种方法来分析。这就回到上一点，良好的算法思路需要扎实功底的数学分析，这样才能充分理解！","text":"Introduction / 简介 update: 2017/12/05 求解：两个n位整数相乘 输入：两个n位的整数x与y 输出：两者的乘积 解决方案： Naive方法：确定基本运算，即y的每一位都要与x中的n位相乘，就是n次基本运算了，乘积十位上的数字还要进上去，这就涉及一些额外的加法运算，但是总之在任何情况下，基本运算的总量最多是2n。同理，得到每一部分乘积最多都需要2n步运算，而部分乘积的总次数为n，即要做$2n^2$次基本运算。然而还没有结束，还要把这些部分乘积累加起来，才能得到最终结果，这一步的求和运算量最多也是$2n^2$次，因此计算两个n位整数的乘积的基本运算操作量为$4n^2$，是输入长度n的二次函数。 Karatsuba方法：利用分治的思想，把x表示为ab，y表示成cd，即a=56，b=78，c=12，d=34。通过以下前三个递归步骤，利用$x\\cdot y = 10000\\cdot a\\cdot c + 100\\cdot a\\cdot d + 100\\cdot b\\cdot c + b\\cdot d$这一思想来求得最终的结果。可以发现，第三步化简之后就是ad+bc，但是在具体操作时不是分别计算ad和bc的乘积再求和，而是使用化简之前的表示，先各自求得a与b之和、c与d之和，然后求积，最后减去已知的ac与bd。这样是为了减少一次递归次数，毕竟加减法才是最基本的运算。 形式化的方式来归纳以下，就是： $x = 10^{\\frac{1}{2}}\\cdot a + b$，$y= 10^{\\frac{1}{2}}\\cdot c + d$ $x\\cdot y=10^{n}a\\cdot c + 10^{\\frac{1}{2}}(a\\cdot d+ b\\cdot c) + b\\cdot d$，$n=4$ ($\\star$) 递归计算ac，ad，bc，bd，然后代入上面的星式求得最终乘积 为了减少一次递归运算，不需要单独计算ad与bc的乘积 对于一个优秀的算法设计者而言，最重要的原则就是拒绝满足。 Perhaps the most important principle for the good algorithm designer is to refuse to be content. The algorithm design space is surprisingly rich! 算法的设计空间，比我们想象中的要广阔得多！ 讲解模式： 确定输入与输出 给出解决方案，即算法，使得输入转化为输出 About the course /关于这门课程 update: 2017/12/06 课程涵盖了5个话题： 设计与分析算法性能所需要的基本知识，比如大O表示法 分治算法的设计与分析，使用场景很多，比如上一节课提到的Karatsuba算法，还有其他的比如排序、矩阵乘法等，需要分析类似递归算法的运行之间 随机化算法的设计与分析，涉及到快速排序、图分解、哈希等 图论分析的基本知识，涉及计算连通信息量、最短路径、社交网络的结构等 基本数据结构的实现与运用，涉及堆、平衡二叉搜索树、哈希表及其变种，比如布隆过滤器等 后续课程可能会涵盖的话题： 贪心算法，涉及最小生成树、调度问题和信息编码理论 动态规划算法，涉及基因序列和社交网络中的最短路径 NP完全问题，涉及是什么以及解法 能够解决特殊问题的快速算法 高效的有可证效率的回溯算法 具有指数时间复杂度的算法，本质上会比暴力搜索优化 从这门课学到什么：视频里说了不少，我挑我最感兴趣且觉得最重要的几点说说 Become a better programmer.虽说读博对代码技巧要求未必很高，但还是希望自己除了学术分析、写作之上，有扎实的代码功底。毕竟还是希望自己毕业之后能去工业界待一段时间的。 Shapen mathematica analytical skills.这是实实在在我需要提神的一项技能！不仅老板指出来了，我自己也深有感触，写paper时构造不出定理与证明，实在是一大败笔！ Explain why things are the way the are, why we analyze the algorithms in the way that we do. 除了设计一个高效的算法，更重要的是你需要能够给别人讲懂，为什么是这样设计，为什么用这种方法来分析。这就回到上一点，良好的算法思路需要扎实功底的数学分析，这样才能充分理解！ Divide and Conquer / 分治Merge Sort ：Example/ 归并排序：示例 update: 2017/12/07 为什么在这里要讲解Merge Sort？有以下五点原因。 Merge sort是一个著名的、古老但是很有用的排序算法，现在已经被列入许多标准库中了 Merge sort完美体现了分治的思想：把一个大问题分解为多个小问题，然后递归地解决小问题，最后合并小问题的求解结果，比Selection / Insetion / Bubble sort算法更直观更有优势 Merge sort可以为学生的未来课程做更好的定位，即calibrate your preparation，后续的算法讲解会越来越复杂，所以这是一个很好的热身，来帮助我们判断是否适合这个课程 Merge sort帮助看清，分析算法与分析其他事物有所不同，需要在分析之前做假设性前提，分析worst-case，然后采用asymptotic analytics渐近分析法来观察算法效率的增长 Merge sort是利用Recusive-tree递归树来分析的，这是一个Master method 然后简要讲解了Merge sort是解决什么问题的？当然是解决乱序数组的排序问题啦。 输入：n个无序数字，假设没有重复数 输出：n个有序排列的数字 处理：把输入的数分成两半，先递归地解决左半部分，再解决右半部分，最后整合出结果，如下图所示，第一步可以想象成在递归调用之前先把左右两部分各自拷贝到新的数组中。 Merge sort：pseudocode / 归并排序：伪代码 update：2017/12/08 伪代码：不管归并排序的子程序具体是如何实现的，假设子程序已经存在了，那就直接合并。所以Merge sort的伪代码就比较明了，如下三步。 递归地对输入数组第一半子数组进行排序 递归地对输入数组第二半子数组进行排序 将两个排好序的子数组进行合并 递归算法需要一个基准，就是当输入为什么的时候，算法就得停止了，返回一个结果。那么在排序中，这个基准就是子数组中只剩下0或者1个数字的时候，不需要再进行计算，直接返回这个数字就可以了。算法实现的细节视频直接忽略了，比如如果数组长度为奇数钙怎么没办？也不会给出递归排序的具体细节，比如在递归调用中，如何把子数组的值返回给函数？这个视频要讨论的是抽象出来的有关算法的概念！ 比较hard的部分：归并部分。下面给出归并部分的伪代码。 分析归并排序的运行时间：从直观上看，应该从一个调试者的角度来考虑算法的运行，也就是说算法运行的时间就是所执行操作的数量，可以理解为实际执行代码的行数。虽然这是一个复杂的问题，但是这次视频忽略了递归相关的操作数量，只考虑归并操作。初始化有2步，进入for循环，每一次迭代有3步，然后还有循环本身的递增，即每一次迭代有4步。把这些加到一起，就得到归并操作的运行时间。给定一个有M数字的数组，最多执行$4\\cdot M+2$步操作。这个上界可以放宽到$6\\cdot M$，因为可能你要考虑循环递增数与总长度的比较，这类小细节操作。但这不是重点。分析Merge sort的主程序会更复杂，因为它在不断调用自身，所以需要分析递归调用的次数，这个次数是呈指数级增长的。_现在还有一个矛盾_，那就是进行递归调用时，输入数组会不断的减小，每次都是之前的一半大小。一方面是子问题的分裂造成的膨胀， 而另一面又是子问题会越来越小，二者之间形成了牵制 ，要解决这二者之间的矛盾就要取决于是什么在驱动Merge sort。后续视频会给出证明，这里先给出一个结论，就是算法的总步骤不超过$6\\cdot N\\cdot log N + 6\\cdot N$。其他类似冒泡排序算法的总步骤是$N^2$，这两个的比较可以看下图，N越大，优势就越明显。 Merge sort：analysis / 归并排序：运行时间分析 update：2017/12/09 这节课的视频是分析Merge sort的运行时间，用数学方式证明：递归Merge sort算法对给定的包含n个数字的数组进行排序，输出有序数组总共需要$6\\cdot n\\cdot log_{2}n + 6\\cdot n$次操作。 证明方法是使用recursion tree递归树，在树结构中写下Merge sort所做的全部工作，一个节点每次递归调用就创建两个孩子节点，如下图所示。根节点root是首次对Merge sort的调用，这一层称作level-0。level-1对应root接下来的两次递归调用，输入是原数组的一半。以此类推，直到最后子数组里面数字的个数为0或者1。显然，叶子节点所在的层是level-$log_{2}n+1$。 确定好树的深度，需要计算每层的操作数量。首先回答两个问题： 对于给定的第j层，有多个sub problem？ 对于第j层的每个sub problem，输入子数组的size是多少？ 答案也显然，第j层有$2^j$个sub problem，每个sub problem的输入size是$\\frac{n}{2^j}$。整个递归树的总操作数量是每层的操作数量之和。由于递归调用本身可以忽略不计，因此只考虑merge操作里的操作数量。分析如下图，第j层的merge次数是$2^{j}\\cdot 6\\cdot \\frac{n}{2^j}$，其中$6$在上一节课的视频中分析过。最后，总共有$log_{2}n+1$层，所以最终结果是$6\\cdot (log_2n+1)$，证明结束。 Guiding principles for analysis of algorithms update：2017/12/11 这一节课的视频是回过头来，介绍算法分析的三项指导原则，也可以说是三项假设，从而帮助我们分析推到算法，并给出”fast”算法的定义。 只考虑worst-case。和avarage-case和benchmark analytics相反，最坏情况的分析对输入没有要求，不需要domain knowledge。 忽略小的常数因子。原因比较简单，第一，简化分析；第二，就这个课程而言，纠结于常数因子对算法的影响没有意义，毕竟常数因子对算法的影响很大程度上还取决于硬件结构、编译器、使用的编程语言、程序员编码习惯等。 关注大规模输入asymptotic analytics。当输入规模逐渐增大至无限，分析算法的性能。关注小规模输入没有意思，以排序为例，也许只对100个数字排序，Merge sort的对数级运行时间会高于其他平方级算法。但是某个临界点过后，对数级的运行时间优势就越来越明显。","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Algorithm Specialization","slug":"Algorithm-Specialization","permalink":"http://tankcat2.com/tags/Algorithm-Specialization/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"十二月的第二周","slug":"diary1210","date":"2017-12-10T14:51:00.000Z","updated":"2017-12-12T11:18:20.329Z","comments":true,"path":"2017/12/10/diary1210/","link":"","permalink":"http://tankcat2.com/2017/12/10/diary1210/","excerpt":"被各种杂事扰乱的工作日结束了上周五组会上的简陋survey，这周的工作日就不打算继续读新的paper了，给自己定了下面几个需要完成的目标： 明珠这周讨论班要将讨论班，再读08年的CRB，把checkpoint的过程梳理一遍； Demo文章初稿，和蒋程交涉好前端需要做的； 关注NVMe； 思考并设计survey的维度； 开始学习Algorithm Specialization课程的学习 周一一早来老板便交代了要开始准备1月初的Demo投稿，因为这个工作的后端实现是和房叔一起做的，已经出了两篇paper了，前端明珠之前实现了一个相对简单的版本，所以论文写起来应该不费事，所以上午先搞了注册的事情。今年有点懵逼啊，按照Demo track给的submission链接注册好个人信息了之后发现没有任何提交paper信息的按钮？？？最后只在Research track板块里给的链接找到了能投稿的地方，占了一个坑。paper的名字也是随意起的。正式写之前先找了几篇已经发表的demo paper看了看，页数限制，就简单安排了一下论文框架。然后抽出了之前工作的关键技术，简要介绍了一番。画系统架构图废了点时间，虽然以前的图也能用，但是太Low了，还是要好好设计一下，毕竟中了的话还能去土澳浪一波~下午三四点的时候在大众点评上选中了两家轰趴馆，晚上约了欢欢去“实地考察”了一番，第一家新开的，环境很不错但是价格不低；第二家环境一般价格也不是很低。回来之后和实验室的小伙伴讨论了一下，大家觉得场地费平摊下来人均不低，所以当天晚上没有确定结果。 周二早上6点半就起床了，因为天气预报说有太阳，于是洗漱之前把床单被单拿到楼下洗衣房洗，洗漱收拾好把被子和洗好的床被单拿出去晒。上午就继续写demo。中午回去发现晒被子的地方没有阳光了，所以就转移场地，拿到宿舍西边楼道的窗台上晒了。下午抽空继续商讨轰趴馆的选择。由于现在是黄金档，就近的几家价格都不便宜，大家因为价格原因不太愿意去，有的同学甚至建议直接组织出去唱歌然后举个餐就算了事了。但是毕竟有的人不喜欢唱歌啊，这样他们参与不进来，所以迟迟出不了结果。最后学姐让步了，她说她和小龙愿意承担1000块，算作是她毕业请吃饭。我把情况和大家说明了一下，可能是觉得学姐都让步到这个程度了，所以最终确定了还去轰趴。几个学弟还是很好说话的，他们本来也建议去轰趴，也不是很在乎钱的问题。轰趴馆就选的第一家，和店家确定之后交了五百押金，这事就算是安定了。晚饭期间去操场跑了三公里，快走了一公里。 周三是最忙的一天了。早上化了妆出门，以为下午颁奖典礼需要上台。上午本来打算把CRB好好再看一遍的，老板一个消息发过来要帮她买一桶羽毛球。一开始以为大活那边有得卖，哪知道体育器材店没有开门。遇到一个保安大叔，他说校门那边的体育馆有得卖，我就急匆匆地跑过去，发现就剩几只了，没有成桶卖的。然后又跑到校外的联华超市，被告知又没有！最后只能寄希望于教育超市了，如果在没有只能让老板自己淘宝了。万幸，教超有得卖。买到手的时候内心mmp，为啥一开始不先去教超呢，明明离得最近……买完已经10点半了，迅速回宿舍补了妆，然后去河东食堂吃了午饭，就去停车场等校车去闵行路。颁奖典礼一直开到下午三点多，根本用不着我们上台，事先确定好了学生代表上台领奖。返程路上看完了行尸走肉这周新更，全程高度紧张，片尾又留悬念，看完更饿了。到了学校之后赶紧去全家买了一个鸡肉卷饼。晚饭之前迅速浏览了一遍CRB，弄清楚了一些基本概念，比如COW、WAL。晚上我们FToS小分队三人继续了这周的小讨论班，帮明珠解答了论文中看不懂的疑惑，我自己也顺带加深一遍理解。","text":"被各种杂事扰乱的工作日结束了上周五组会上的简陋survey，这周的工作日就不打算继续读新的paper了，给自己定了下面几个需要完成的目标： 明珠这周讨论班要将讨论班，再读08年的CRB，把checkpoint的过程梳理一遍； Demo文章初稿，和蒋程交涉好前端需要做的； 关注NVMe； 思考并设计survey的维度； 开始学习Algorithm Specialization课程的学习 周一一早来老板便交代了要开始准备1月初的Demo投稿，因为这个工作的后端实现是和房叔一起做的，已经出了两篇paper了，前端明珠之前实现了一个相对简单的版本，所以论文写起来应该不费事，所以上午先搞了注册的事情。今年有点懵逼啊，按照Demo track给的submission链接注册好个人信息了之后发现没有任何提交paper信息的按钮？？？最后只在Research track板块里给的链接找到了能投稿的地方，占了一个坑。paper的名字也是随意起的。正式写之前先找了几篇已经发表的demo paper看了看，页数限制，就简单安排了一下论文框架。然后抽出了之前工作的关键技术，简要介绍了一番。画系统架构图废了点时间，虽然以前的图也能用，但是太Low了，还是要好好设计一下，毕竟中了的话还能去土澳浪一波~下午三四点的时候在大众点评上选中了两家轰趴馆，晚上约了欢欢去“实地考察”了一番，第一家新开的，环境很不错但是价格不低；第二家环境一般价格也不是很低。回来之后和实验室的小伙伴讨论了一下，大家觉得场地费平摊下来人均不低，所以当天晚上没有确定结果。 周二早上6点半就起床了，因为天气预报说有太阳，于是洗漱之前把床单被单拿到楼下洗衣房洗，洗漱收拾好把被子和洗好的床被单拿出去晒。上午就继续写demo。中午回去发现晒被子的地方没有阳光了，所以就转移场地，拿到宿舍西边楼道的窗台上晒了。下午抽空继续商讨轰趴馆的选择。由于现在是黄金档，就近的几家价格都不便宜，大家因为价格原因不太愿意去，有的同学甚至建议直接组织出去唱歌然后举个餐就算了事了。但是毕竟有的人不喜欢唱歌啊，这样他们参与不进来，所以迟迟出不了结果。最后学姐让步了，她说她和小龙愿意承担1000块，算作是她毕业请吃饭。我把情况和大家说明了一下，可能是觉得学姐都让步到这个程度了，所以最终确定了还去轰趴。几个学弟还是很好说话的，他们本来也建议去轰趴，也不是很在乎钱的问题。轰趴馆就选的第一家，和店家确定之后交了五百押金，这事就算是安定了。晚饭期间去操场跑了三公里，快走了一公里。 周三是最忙的一天了。早上化了妆出门，以为下午颁奖典礼需要上台。上午本来打算把CRB好好再看一遍的，老板一个消息发过来要帮她买一桶羽毛球。一开始以为大活那边有得卖，哪知道体育器材店没有开门。遇到一个保安大叔，他说校门那边的体育馆有得卖，我就急匆匆地跑过去，发现就剩几只了，没有成桶卖的。然后又跑到校外的联华超市，被告知又没有！最后只能寄希望于教育超市了，如果在没有只能让老板自己淘宝了。万幸，教超有得卖。买到手的时候内心mmp，为啥一开始不先去教超呢，明明离得最近……买完已经10点半了，迅速回宿舍补了妆，然后去河东食堂吃了午饭，就去停车场等校车去闵行路。颁奖典礼一直开到下午三点多，根本用不着我们上台，事先确定好了学生代表上台领奖。返程路上看完了行尸走肉这周新更，全程高度紧张，片尾又留悬念，看完更饿了。到了学校之后赶紧去全家买了一个鸡肉卷饼。晚饭之前迅速浏览了一遍CRB，弄清楚了一些基本概念，比如COW、WAL。晚上我们FToS小分队三人继续了这周的小讨论班，帮明珠解答了论文中看不懂的疑惑，我自己也顺带加深一遍理解。 周四也是不消停的一天。这天的原计划是把CRB没看完的看完，再关注一下NVMe，顺带复习一下存储技术的基础知识，于是把去年翁老师的课件翻出来又刷了一遍。中途想起来还没问教务老师，转博要提交的材料到底有哪些，邮件上也没说明。跑过去问清楚了，缺的材料不少，专家推荐信还缺一份，于是加了老金的微信，老金当天去北京出差了，晚上还抽时间帮我填好了材料，感动。这天上午男票还犯傻，想骗我说521，被我识破，这个小插曲我写在前一篇日记里了。晚上去参加了三位同学的转预备党员小会。 周五上午继续学习NVMe。这天没什么特别的事情，下午讨论班也顺利进行了。除了将论文，李学长和老师还一起帮我梳理了后面利用NVMe可以考虑的研究点，跟李学长要了几篇文章，下周看看。晚上整理轰趴需要准备的食材、娱乐项目清单整理出来，在实验室群里说明了一下各项事宜。 PS：每天早上来实验室刷完单词之后花四十分钟左右的时间学习算法课程，并整理笔记，笔记以及相关的学习资料我挂在github上了algorithm-specialization-notes，看视频的时候也想锻炼自己的英语，所以放的是英文字幕，希望自己以后能脱离字幕看视频吧。除了这个，这周把饮食健身打卡也从豆瓣小组的帖子转移到github上了diet-record，因为这样本地编辑起来更加方便。 日程满满的周六周日这次周六周日日程安排得满满当当：周六中午和原来313的两个学姐、一个学弟(欢欢、妍虹和高竹)一起去避风塘吃饭，周六晚上约了小慧去吃上周抽中的一家霸王餐；周日要组织实验室的小伙伴们一起去轰趴。 周六早上晚起了一会儿，发现牛油果再放就要坏了，于是打奶昔的时候顺便给早来实验室的一个学弟也做了一杯。学习好算法视频，写好周报，收拾了一下就出门去环球港的避风塘占座了。记得三四年前和本科室友们第一次在南京马群吃避风塘的时候，觉得超级好吃，今天发现质量严重下降，点心太甜菜太咸，连云吞面的汤也是，烧鹅肉质一点也不新鲜。席间学弟学姐们讲了不少工作上的事情。饭后他们三人去Coco各自买了奶茶，后来转战到宝珠奶酪，我又买了一杯牛油果雪酿，这次喝得比较慢，看着它慢慢融化，喝到最后越来越美味~在宝珠奶酪待了很久，听学弟说互联网的加班很辛苦，最近这段时间他经常加班到晚上十一二点才回家，说工作了之后才发现，真的比在学校辛苦太多，在学校就算老师给了deadline，其实没有按时交出结果也不是大问题，但是工作上很不一样，没有人会设身处地地为你想，没有结果就等于没做。想想自己决定转博，也不乏逃避找工作这一因素。聊完之后就在环球港逛了逛衣服，中午让老妈资助了五百，去买了上周相中的那件粉色卫衣。因为中午吃得太饱，下午又喝了一大杯雪酿，晚饭就没吃，回宿舍锻炼了半小时的HIIT。洗完澡回到实验室把临时不去轰趴吃午饭的人记录一下，确定了周日早上去菜市场采购的人员和时间点。安排好所有事情之后又看了会算法视频。 周日早上6点30起来，洗漱完撸了一个简易妆，然后就和采购小分队去买菜了。从枣阳路门出去，到光复西路左转，穿过强家角桥，发现了好多家崇明蔬菜市场，按照周五晚上定的食材清单采购完，就骑了单车去轰趴馆。由于时间还早，交接的人还没到，我们五个人就先在新客来大食堂坐了一会儿，给大家买了大肉包当早饭吃，后来直接在店里买了一些打包饭盒和一次性筷子。进了轰趴馆后，分配好洗菜任务，就继续和其中一个学弟去环球港买了点零食、主食和酱料。还买了一瓶牛奶，因为今天带了自己的电炖锅，给大家做了奶茶喝~陆陆续续人到了，中午就涮起火锅吃，16个人围着一个小吧台、两口锅还是有点挤的，不过吃得还算开心，口味还不错，嘿嘿，自夸一下。吃完大家就开始游戏了：有玩狼人杀的，有打桌球的，有打麻将的，有玩街机的，还有唱K的。这家KTV效果还是不错的，今天唱得很尽兴，哲神和韩易唱歌都超级厉害，周杰伦、陈奕迅的每首歌都唱的很6~很开心今天和哲神合唱了青峰和Ella的《你被写在我的歌里》，也很开心他们俩唱双截棍，我、小慧还有卷积在旁边附和，真心嗨爆了，可爱又迷人~再有就是后来祝翔加入，一起唱霍元甲~唱到一半累了，出去和鹏鑫打了两局桌球，平手~只要不让我设计那种可以反弹折射的角度打法，我还是可以的哈哈。晚上就把中午没吃掉的饺子煮了，由于锅不大，两大袋饺子足足分了四次才煮完。怕不够吃，还点了三个达美乐披萨，土豆味的一如既往地好吃~吃完饭又继续进去唱K，一直到最后，大家合影留念。这是我第一次主动组织大家聚会，我这个人吧喜欢简单，所以安排事情的时候就可能没有考虑很全面，反思一下，以后说话态度也要和善一点~总之，大家玩得还算尽兴，也辛苦起早陪我一起买菜得小慧、杨康、蒋程和汤路明，辛苦可人给大家拍照，也辛苦参与和支持的每一位小伙伴~","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"周记","slug":"周记","permalink":"http://tankcat2.com/tags/周记/"},{"name":"自省","slug":"自省","permalink":"http://tankcat2.com/tags/自省/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"十二月的第一周","slug":"diary1203","date":"2017-12-03T12:44:00.000Z","updated":"2017-12-12T10:51:48.919Z","comments":true,"path":"2017/12/03/diary1203/","link":"","permalink":"http://tankcat2.com/2017/12/03/diary1203/","excerpt":"十一月就这么静悄悄地溜走了，回头一看，这一个月好像正经事没做成，但是仔细数数又似乎达成了不少目标。 拿到了研究生国国家奖学金、校优秀学生、全国研究生数学建模二等奖 给爸爸换了手机，买了种草许久的Mate 9 体重下百了 订制了心仪的中古包、买了一整套Martiderm的平衡和臻活安瓶、买了性价比高的牛角梳 工作日的日常这周周五的讨论班轮到我讲，因为十一月的主要工作就是读paper，于是就准备做个简短的survey。算下来看了有二十多篇，由于没找到读paper的技巧，为了赶进度导致很多篇都没有真正理解，做起survey的时候才发现很多东西自己也讲不清楚。 一开始给自己这周的规划是，新论文就不读了，专心整理已经读过的。然而，周一又把时间花在自己的个人博客上了。无意间发现有人利用Github Issue改写了一个评论系统，我觉得还不错就想着借鉴一下，替换现有的disqus。一些配置问题占用了我一整个下午。晚上参加数模的三个团队，其实也就是实验室的小伙伴，说拿了奖一起约顿饭，于是就去秋林阁三楼点了一桌子菜。每次都必点青椒炒蛋和冬瓜排骨汤。吃完洗了澡，回到实验室继续看paper。但是又被一个妹纸的个人博客吸引住了，她的首页是 一张cv，而博客是二级域名。她把cv的制作公开在github上了，由于一些域名设置问题，我捣鼓了不少时间，最终还是放弃，十点半的时候回寝室睡觉了。 周二早上醒来觉得还是不甘心，cv我肯定是也要做一张的，但是不换域名了，直接利用HTML按照她的样式做了一个一模一样的，简单粗暴，替换成了博客的首页。博客的改造终于完成了，就继续做起了survey。原本打算先把上周没看完的一篇02年的survey看完，可是越看越觉得不理解，心情很烦躁。不巧中午来了大姨妈，肚子很疼，就荒废了一下午躺在床上休息。 周三算是真正开始写survey的presentation了。 没有跳出前人整理的框架，我继续沿用，只是合并了一些小分类，在整理具体系统的容错实现时发现自己还是无从下手，不知道怎么才能讲的清楚，又陷入了烦躁的情绪，没忍住还和男票哭诉了一番。晚上洗完澡发现自己感冒了，真是祸不单行。 周四继续写presentation，但是不巧，遇到老板开始帮我改老早之前的期刊文章。仍旧是定理证明的那块儿写的不好。这一块儿反反复复修改了不知道多少遍了，我早已厌烦。我承认，想要投稿期刊论文，又或者说以后想投稿顶会论文，没有两三个定理与证明是真说不过去的，这一块技能我确实欠缺了。下午老板把我叫过去，讲完问题所在之后我主动和她交流了一下这一个月来读论文的感受，说明了自己的矛盾所在，其实烦躁的根源在于自己平日偷懒、效率低下导致论文没有理解透彻，后来回了实验室，老板在微信了给我打了一剂鸡血。","text":"十一月就这么静悄悄地溜走了，回头一看，这一个月好像正经事没做成，但是仔细数数又似乎达成了不少目标。 拿到了研究生国国家奖学金、校优秀学生、全国研究生数学建模二等奖 给爸爸换了手机，买了种草许久的Mate 9 体重下百了 订制了心仪的中古包、买了一整套Martiderm的平衡和臻活安瓶、买了性价比高的牛角梳 工作日的日常这周周五的讨论班轮到我讲，因为十一月的主要工作就是读paper，于是就准备做个简短的survey。算下来看了有二十多篇，由于没找到读paper的技巧，为了赶进度导致很多篇都没有真正理解，做起survey的时候才发现很多东西自己也讲不清楚。 一开始给自己这周的规划是，新论文就不读了，专心整理已经读过的。然而，周一又把时间花在自己的个人博客上了。无意间发现有人利用Github Issue改写了一个评论系统，我觉得还不错就想着借鉴一下，替换现有的disqus。一些配置问题占用了我一整个下午。晚上参加数模的三个团队，其实也就是实验室的小伙伴，说拿了奖一起约顿饭，于是就去秋林阁三楼点了一桌子菜。每次都必点青椒炒蛋和冬瓜排骨汤。吃完洗了澡，回到实验室继续看paper。但是又被一个妹纸的个人博客吸引住了，她的首页是 一张cv，而博客是二级域名。她把cv的制作公开在github上了，由于一些域名设置问题，我捣鼓了不少时间，最终还是放弃，十点半的时候回寝室睡觉了。 周二早上醒来觉得还是不甘心，cv我肯定是也要做一张的，但是不换域名了，直接利用HTML按照她的样式做了一个一模一样的，简单粗暴，替换成了博客的首页。博客的改造终于完成了，就继续做起了survey。原本打算先把上周没看完的一篇02年的survey看完，可是越看越觉得不理解，心情很烦躁。不巧中午来了大姨妈，肚子很疼，就荒废了一下午躺在床上休息。 周三算是真正开始写survey的presentation了。 没有跳出前人整理的框架，我继续沿用，只是合并了一些小分类，在整理具体系统的容错实现时发现自己还是无从下手，不知道怎么才能讲的清楚，又陷入了烦躁的情绪，没忍住还和男票哭诉了一番。晚上洗完澡发现自己感冒了，真是祸不单行。 周四继续写presentation，但是不巧，遇到老板开始帮我改老早之前的期刊文章。仍旧是定理证明的那块儿写的不好。这一块儿反反复复修改了不知道多少遍了，我早已厌烦。我承认，想要投稿期刊论文，又或者说以后想投稿顶会论文，没有两三个定理与证明是真说不过去的，这一块技能我确实欠缺了。下午老板把我叫过去，讲完问题所在之后我主动和她交流了一下这一个月来读论文的感受，说明了自己的矛盾所在，其实烦躁的根源在于自己平日偷懒、效率低下导致论文没有理解透彻，后来回了实验室，老板在微信了给我打了一剂鸡血。 男票跟我说，做presentation的时候就尽力止损吧，把自己理解的讲清楚，不理解的说明一下情况。没想到，presentation竟然顺利进行了，主要也有李学长的助攻。李学长真的是个神童，经常就是我们自己讲presentation的人还没有他听的人理解得透彻。这大概就是知识储备的差距吧。讨论班结束之后，学弟还反映说这次连他也听懂了，嘿嘿。 这一周的工作日也就这些了，主要就是对看过的文章做了整理，但是层次还很浅显；这一周锻炼和背单词也搁置了没进行。接下来又重新制定工作计划，把没有理解的概念、技术重新弄懂，paper也要继续读，设计分类的维度，整理好每一维度上各个系统、技术的实现、优缺点等等。 ps：看到一段鸡血，或者说拖延症的治疗技巧。 朋友们，我的一点切身经验，如果你觉得某个任务让你特别焦虑，压得你喘不过气来，那么最好的排解方法就是直接去做这事，什么都别管，就是使劲做，努力地推进其进度，这棘手的事情在进度上每发展一点，你的焦虑就会少一分，同时你的焦虑越少，推进的速度也就越快，只要咬紧牙关，不停地推进，总会有解脱的那一天，而且你每完成一个棘手的任务，你或多或少都会比之前牛逼强大那么一点，这件苦差事总是会改变你一些。真的，诸位，有什么难事千万别耗着，别等着，那只会让人在无尽的焦虑中煎熬，你就先大吼一句：“去你妈的。” 然后两眼血丝地去推进，去做事，做着做着就有出路了。 短暂的周末相聚原本周五晚上讨论班结束就要去无锡的，老早就跟男票定好这周末去无锡玩，正好两人折中路程。然后计划赶不上变化，今天他公司有年会，再加上我感冒了，于是去无锡的计划就推迟了。一个月没见面了，甚是想念，最后决定这周他来上海找我。这次来上海就不打算去较远的地方拔草了，就带带他去我我比较喜欢的黄焖鸡、汤包店还有甜品店吃吃喝喝，顺便去商场逛逛，买点衣服，还要带他去枣阳路的那家星巴克，我实在是太喜欢这家店了。 周五晚上关掉了闹钟，早早爬床，周六早上睡到自然醒，睡前用了臻活安瓶，洗脸的时候发现毛孔又小了欸~早饭去全家买了一个黑胡椒鸡排三明治，里面有鱼子蛋沙拉，木有喜士多的好吃~买完回实验室，在Coursera上找了一门Standford的算法课，打算以后每周周末抽出半天时间恶补一下薄弱的算法基础。中午去河东食堂吃了一碗冒菜，十二点了人还是很多，这次没吃全素，加了一根里脊肉和两块午餐肉，可能是人多煮得比较急吧，番茄汤底又油又咸……吃完回实验室继续啃算法，中途微信上不断骚扰男票，问他酒宴吃得怎么样了。原来说好坐四点的高铁过来的，可以他们吃完酒宴还要聊天，他就改签了五点。想到他周一上午要开例会，所以周日下午就得回去，这次相聚的时间本来就比较短，他还改签推迟了！于是当然要生气啦！反正后面他给我发微信我都没鸟他，自己一个人收拾好洗漱用品，去宾馆办了入住，然后就静静地看起了电视。。。他将近七点多才到，我自己饿得受不了了就点了一份老乡家香酥鸡柳和炸里脊，里脊还是小时候在文峰大世界妈妈买给我吃的味道~接到他之后，两人就去环球港觅食啦~去宝珠奶酪家发现没有牛油果奶昔了T^T，想吃点菜的但是想到我们是贫穷夫妻俩，就决定去大时代随便吃吃，买了一笼富春苏式汤包，还点了三个蒸菜，都很难吃！后悔了，还不如去吃点菜呢！敲生气！吃完就在环球港随便逛了逛，发现了一家新开的衣服店叫MM麦檬，衣服都挺不错了，试了一件白色的韩系羽绒服，穿着像披了一条棉被哈哈，不过店里的镜子都是显瘦的，虽然臃肿，但是时尚感不减~由于价格原因，又因为上周刚在网上预定了一件羽绒服，没买就回去了~上楼之前又去林间小屋买了一个巧克力杯子蛋糕，一如既往的好吃~ 周日早上也是早早起床，因为想带他去吃很多我觉得好吃的小吃。他昨晚没怎么吃，蔡师傅汤包店又有点远，就先去林间小屋买了一个巧克力麻薯填填肚子~意外得好吃呀，难怪大众点评上不少人推荐。穿过了金沙江路，到与枣阳路的交汇处就是蔡师傅汤包店了。早上人还是挺多的，中老年人居多，大概都是周围的居民，点了两碗小馄饨、一笼汤包还有一个小小的甜口烧饼。小馄饨的汤里估计是放了猪油，很好喝，汤包有点酸，不知道怎么回事。我吃了两个汤包和几颗小馄饨就饱了，他估计是真饿了，把我剩下的小馄饨也都吃光光了，哈哈哈~吃完带他去买了老香斋的蝴蝶酥和榴莲酥，然后步行回宿舍放下书包，两人就又去环球港逛吃逛吃了。主要原因是前一天晚上决定还是去把那件羽绒服买下来，网上预定的那一件可以退了。先带他去吃说了好几次的Godiva的冰淇淋，今天没有优惠券，50块买了一只全巧克力味的，好甜好甜，我自己是不太喜欢吃啦。吃完就去买衣服了，为了下周的奖学金颁奖典礼，还特地买了一条正式一点的半身裙。买完在店里休息了一会儿，顺便又试了一件粉色的卫衣，外加N件大衣~哎，要是有钱就全拿下啦，都很喜欢~时间还早，两人就去宝珠奶酪坐了会儿，买了一杯新出的酒酿酸奶。上上周在店里有店员拿给我试吃，当时觉得好不错，今天吃感觉一言难尽……午饭就带他去吃很喜欢的一家黄焖鸡，点了两碗小份，我的另加了一份土豆和金针菇，他不喜欢吃金针菇，就给换成了青菜。中途还遇到了室友，哈哈，这也是她第一次和男票打招呼。吃完在学校里逛了逛，坐在河边看看风景，落叶、小河还有垂钓的老爷爷，好不休闲~因为感冒还没好，原本买的六点的车票，让他提前改签到三点了。这次没送他去车站，看着他进地铁的时候实在是太难过了，忍不住又哭了起来。现在相聚的次数越来越少，每次相聚的时间也很短暂。原来刚在一起的时候不觉得异地恋有啥，现在感情越来越深，对他越来越依赖，就感觉异地恋真的很辛苦。送他走后，我实在是太累了，回去一觉睡到五点。 洗完澡回到实验室，更新了这篇日记。这一周就要过去啦，明天迎来新的一周，继续奋斗吧，为了美好的将来~","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"周记","slug":"周记","permalink":"http://tankcat2.com/tags/周记/"},{"name":"自省","slug":"自省","permalink":"http://tankcat2.com/tags/自省/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"十一月的第四周","slug":"diary1126","date":"2017-11-26T10:29:45.000Z","updated":"2017-12-12T10:51:30.695Z","comments":true,"path":"2017/11/26/diary1126/","link":"","permalink":"http://tankcat2.com/2017/11/26/diary1126/","excerpt":"工作日的survey这个月进行survey的第四周，工作日的五天细读了两篇微软的流处理系统paper，一篇Naiad和一篇Falkirk Wheel；一篇很经典的Dynamo，十年前就发表的这篇今年喜获SIGOPS名人堂奖，重点关注了它的高可用是如何实现的；一篇综述，其中一个作者和Naiad、Falkirk Wheel联系密切，这篇综述关注的点很基础。 算下来从开始做survey，看过的文章也有二十多篇了，关于容错，心中确实有了一点感觉，但觉得还不够，有的时候回头想想看过的文章，竟然不能一句话讲出它做了什么，看过就忘成了很大的问题。男票告诉我，这是因为我没有理解透彻。所以，现在就有点迷茫，到底什么文章该精读，精读到什么程度是够的，要想真正理解一篇文章的proposal，耗时不短，这当中该怎么做权衡呢？ 还有一点需要反省，就是效率问题，一周只看了四篇文章，很大一部分时间还是被我浪费了，经常看文章看到一半就去找人聊天、逛豆瓣微博，没有完整、高效率的学习时间，这应该也是导致文章理解不透彻的原因之一吧。 不过，有进步的是，开始定制周计划了，也切实进行了，希望在次基础上提高效率并继续保持。","text":"工作日的survey这个月进行survey的第四周，工作日的五天细读了两篇微软的流处理系统paper，一篇Naiad和一篇Falkirk Wheel；一篇很经典的Dynamo，十年前就发表的这篇今年喜获SIGOPS名人堂奖，重点关注了它的高可用是如何实现的；一篇综述，其中一个作者和Naiad、Falkirk Wheel联系密切，这篇综述关注的点很基础。 算下来从开始做survey，看过的文章也有二十多篇了，关于容错，心中确实有了一点感觉，但觉得还不够，有的时候回头想想看过的文章，竟然不能一句话讲出它做了什么，看过就忘成了很大的问题。男票告诉我，这是因为我没有理解透彻。所以，现在就有点迷茫，到底什么文章该精读，精读到什么程度是够的，要想真正理解一篇文章的proposal，耗时不短，这当中该怎么做权衡呢？ 还有一点需要反省，就是效率问题，一周只看了四篇文章，很大一部分时间还是被我浪费了，经常看文章看到一半就去找人聊天、逛豆瓣微博，没有完整、高效率的学习时间，这应该也是导致文章理解不透彻的原因之一吧。 不过，有进步的是，开始定制周计划了，也切实进行了，希望在次基础上提高效率并继续保持。 周六周日的吃吃玩玩周六中午出门，骑单车去武康路的一家星巴克臻选店买工业风的杯子，哪里知道好不容易找到这家店却被告之没有存货，感觉委屈的不行。直接返程肯定是不行的，不然就白出来了，于是就在武昌路和湖南路附近逛逛，后来又去一家网红面店吃了一碗辣肉面。然而，十个网红店九个是垃圾，这家面店很荣幸也是垃圾，38块一碗的面还不如河西食堂3块一碗的阳春面。返程的时候选择步行，正好记一记路线。从武康路-兴国路-华山路-江苏路-愚园路-长宁路-凯旋路-万航渡路-光复西路-枣阳路，短短五公里串联了这么多道路，有些路很小资、适合拍照，有些路就普普通通、大众化。走到长宁路上的兆丰广场，看到有Bose专柜就想顺便试试音质，试玩就心动了，比我的大法轻多了。走之前顺便又去nitori买了一口雪平锅，想着宿舍厨房里有电磁炉，以后可以熬奶茶、煮泡面吃，可是买的时候没看清楚，今天早上发现这锅只能用煤气加热。晚上懒得回实验室了，和男票商量了一下要讨论微信小程序的需求问题，于是又背着包出门，选择在枣阳路的这家星巴克讨论。环境还是不错的，有无线，周六晚上人很少，买了一杯太妃榛果，店员还给了一些蛋糕试吃。讨论完肚子又饿了，就去蔡师傅汤包店点了一碗小馄饨和一块素鸡，美味又划算。最后，回寝室洗漱。哦对了，昨天是老爸生日，舅舅一家去家里，燕子姐姐又给爸爸买了礼物。听老妈说，爸爸这次很感动，不仅燕子姐姐送了礼物，我又送了手机，好像老爸都哭了。 周日赖了床，早上8点半才醒，准备用雪平锅煮奶茶的时候发现电磁炉不能加热，没办法只能换成电炖锅来做了。牛奶是在盒马鲜生上买的明治鲜奶，加了两包立顿红茶包，这次冰糖又放多了，不过口感还是很醇厚的，虽然比较甜，和外面比起来还是健康很多的。给实验室的小伙伴带了一点，评价都不错。中午去了实验室，忙着把Hexo个人博客需要的环境在新笔记本上重新搭建了一下，遇到了不少问题，但是都解决了。搞完已经三点了，在大众点评上选择了一家中江路那边的日料店，骑了单车过去，由于不是饭点，店里除了我没其他顾客，点了三文鱼刺身、炸土豆饼、煎饺和炸鸡肉串，很快就消灭了~ 最后这一周过得还是挺充实了，周一早上体重还下百了，激动得我发了一条朋友圈。可是下百就一天，接着几天就无所顾忌地吃，周一把剩余的牛油果继续做了奶昔喝，周二吃了冒菜，周三又跑去吃了煲宫，周五还吃了麻辣香锅和芝麻糊小圆子，周六周日前面已经说了。减肥我还是会继续的，只是不会再像一开始那么严格了，该吃吃还是要吃，该锻炼也还是会锻炼，健康的前提下保持身材。技能上， 就这样吧，明天又是新的一周，下周五轮到我讲survey，还有四天半的时间好好准备，加油吧二筒子~","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"周记","slug":"周记","permalink":"http://tankcat2.com/tags/周记/"},{"name":"自省","slug":"自省","permalink":"http://tankcat2.com/tags/自省/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"冬日就该来杯热乎的焦糖奶茶~","slug":"milktea","date":"2017-11-25T01:22:47.000Z","updated":"2017-11-27T13:43:01.445Z","comments":true,"path":"2017/11/25/milktea/","link":"","permalink":"http://tankcat2.com/2017/11/25/milktea/","excerpt":"关注的好几个美食博主都各自分享了自制奶茶的教程，我选择了大胃爱丽丝的焦糖奶茶，因为最简单，除了原材料就只需要一个微博里就可以了，我这么懒，怎么会还专门买个奶锅回来呢~买了奶锅回来可能就只煮方便面了吧hahaha… 下面是我交的作业，我的材料和微波炉和教程里有出入，所以每个步骤耗时也会有所不同。 首先是原材料： 安佳全脂牛奶一瓶 250ml。这里大家可以自行选择牛奶的品牌，我是喝惯了安佳，觉得奶香很足。 冰糖 6颗。我没用白砂糖的原因是宿舍里只有冰糖，我懒，不想出去买，哈哈哈。 立顿红茶包 2袋。 玻璃杯，可微波炉加热。 微波炉。宿舍厨房的微波炉功率感觉不行，每个加热步骤我都比原教程里的长。 其次是具体加热步骤： 冰糖加一点儿水，放进微波炉叮4分半钟。因为冰糖不容易化，加上微波炉功率不行，原教程只要叮1分半钟就能熬出焦糖色，我足足叮了三次。 取出杯子，加入全脂牛奶和茶包，放进微波炉里叮2分钟。 再次取出杯子，用勺子轻轻挤压茶包，轻轻搅拌杯底的焦糖，放进微波炉里叮2分钟。这里一定要注意！立顿的茶包好容易挤破，我就是挤破了QAQ茶叶碎都溢出来了，最后只能等到沉淀了才能喝。 开喝。 PS: 个人觉得冰糖放得还是有点儿多，因为喝的时候觉得好甜啊，连室友也说太甜了…下次我要试试不放冰糖，看看是不是真的很影响口感~ 以上，谢谢观看~","text":"关注的好几个美食博主都各自分享了自制奶茶的教程，我选择了大胃爱丽丝的焦糖奶茶，因为最简单，除了原材料就只需要一个微博里就可以了，我这么懒，怎么会还专门买个奶锅回来呢~买了奶锅回来可能就只煮方便面了吧hahaha… 下面是我交的作业，我的材料和微波炉和教程里有出入，所以每个步骤耗时也会有所不同。 首先是原材料： 安佳全脂牛奶一瓶 250ml。这里大家可以自行选择牛奶的品牌，我是喝惯了安佳，觉得奶香很足。 冰糖 6颗。我没用白砂糖的原因是宿舍里只有冰糖，我懒，不想出去买，哈哈哈。 立顿红茶包 2袋。 玻璃杯，可微波炉加热。 微波炉。宿舍厨房的微波炉功率感觉不行，每个加热步骤我都比原教程里的长。 其次是具体加热步骤： 冰糖加一点儿水，放进微波炉叮4分半钟。因为冰糖不容易化，加上微波炉功率不行，原教程只要叮1分半钟就能熬出焦糖色，我足足叮了三次。 取出杯子，加入全脂牛奶和茶包，放进微波炉里叮2分钟。 再次取出杯子，用勺子轻轻挤压茶包，轻轻搅拌杯底的焦糖，放进微波炉里叮2分钟。这里一定要注意！立顿的茶包好容易挤破，我就是挤破了QAQ茶叶碎都溢出来了，最后只能等到沉淀了才能喝。 开喝。 PS: 个人觉得冰糖放得还是有点儿多，因为喝的时候觉得好甜啊，连室友也说太甜了…下次我要试试不放冰糖，看看是不是真的很影响口感~ 以上，谢谢观看~","categories":[{"name":"Skills","slug":"Skills","permalink":"http://tankcat2.com/categories/Skills/"}],"tags":[{"name":"焦糖奶茶","slug":"焦糖奶茶","permalink":"http://tankcat2.com/tags/焦糖奶茶/"}],"keywords":[{"name":"Skills","slug":"Skills","permalink":"http://tankcat2.com/categories/Skills/"}]},{"title":"Martiderm安瓶使用感","slug":"Martiderm","date":"2017-11-24T01:28:31.000Z","updated":"2017-11-27T13:18:12.937Z","comments":true,"path":"2017/11/24/Martiderm/","link":"","permalink":"http://tankcat2.com/2017/11/24/Martiderm/","excerpt":"这又是一记安利~ 先交代一下本人的肤质吧： 初中开始长青春痘 高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印 前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干 目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长 总结一句话，就是混油痘肌。 半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，臻活和平衡系列各五只，我的目的很明确，祛痘印+提亮肤色。 臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用伊索的绿茶水和无油保湿精华打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。 可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~","text":"这又是一记安利~ 先交代一下本人的肤质吧： 初中开始长青春痘 高中不懂事，乱扣红肿痘痘，导致右脸比较深的痘印 前两年断断续续吃过维安脂和泰尔斯，现在出油不严重，冬天甚至会有点干 目前下巴仍然会长痘痘，以红肿为主，闭口很少；下巴以上部位不怎么长 总结一句话，就是混油痘肌。 半个月前左右我抱着尝试的心理买了Martiderm家的安瓶，臻活和平衡系列各五只，我的目的很明确，祛痘印+提亮肤色。 臻活系列貌似是价格最高的，浓度和粘稠度也是，这个我是睡前用；平衡系列是早上用，没有臻活那么黏。这两个我都是两天之内用完一瓶，一开始我是先用伊索的绿茶水和无油保湿精华打个底，后来嫌麻烦，就直接把安瓶和无油保湿精华混在一起抹了，吸收挺快的。 可能是刚开始用的时候不耐受，加上我没控制好量，涂的有点多，导致不论是睡醒还是白天一天下来，都觉得自己脸色暗沉，毛孔更大….但是！从前天早上开始，我发现脸上干净了好多，下巴上的痘痘痘印(除红肿外)淡了不少，关键是毛孔小了！看来这个安瓶在微博上风很大是有道理的！现在快用完了，打算入手一个全套装~价格好像更划算~","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"护肤","slug":"护肤","permalink":"http://tankcat2.com/tags/护肤/"},{"name":"安瓶","slug":"安瓶","permalink":"http://tankcat2.com/tags/安瓶/"},{"name":"Martiderm","slug":"Martiderm","permalink":"http://tankcat2.com/tags/Martiderm/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"视频导出音频小技能","slug":"ffmped","date":"2017-11-22T02:44:51.000Z","updated":"2017-11-27T13:24:11.915Z","comments":true,"path":"2017/11/22/ffmped/","link":"","permalink":"http://tankcat2.com/2017/11/22/ffmped/","excerpt":"早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。 视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——http://audio-extractor.net/cn/，需要先上传视频，再点击转换，最后再下载音频。 视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！http://ffmpeg.org/ 这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~ 我是把MP4转换成MP3，在别人的博客里找到了下面的命令： 1ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3 以上，谢谢阅读。","text":"早上看到青峰发的新作品小视频，就想把它down下来，于是找到了一个很实用的chrome插件——video download helper。 视频下载下来了，又想提取音频，这样上传到我的网易云网盘就能随时听啦。一开始不太想装国产的转换软件，发现了一个在线的转换平台——http://audio-extractor.net/cn/，需要先上传视频，再点击转换，最后再下载音频。 视频上传实在是太慢了！于是乎我又去知乎上搜搜看有没有大神提供一些轻量级的软件~果不其然，让我发现了FFmpeg的存在！http://ffmpeg.org/ 这个是homepage，支持Linux/Windows/OS X。下载好压缩包后解压，然后把bin目录添加到环境变量中去就能愉快地使用啦~ 我是把MP4转换成MP3，在别人的博客里找到了下面的命令： 1ffmpeg -i video.mp4 -vn-acodec libmp3lame -ac 2 -qscale:a 4 -ar 48000audio.mp3 以上，谢谢阅读。","categories":[{"name":"Skills","slug":"Skills","permalink":"http://tankcat2.com/categories/Skills/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://tankcat2.com/tags/ffmpeg/"}],"keywords":[{"name":"Skills","slug":"Skills","permalink":"http://tankcat2.com/categories/Skills/"}]},{"title":"鼓楼半日记","slug":"gulou","date":"2017-08-15T12:11:31.000Z","updated":"2017-11-27T13:23:57.620Z","comments":true,"path":"2017/08/15/gulou/","link":"","permalink":"http://tankcat2.com/2017/08/15/gulou/","excerpt":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","text":"今天跟着zf去鼓楼的办公室，发现大门口右手边就是云南路地铁站口，右拐过去就是上海路。想到小厨娘就在附近，决定扔下zf一个人去买蛋糕。不知道怎么想的，可能天不热，没骑车步行过去的。以前步行只知道跟着导航急匆匆地赶到目的地，不在意沿途的风景。今天边走变看，走着走着就看到了最喜欢吃的朱师傅梅花糕。以前领过很多人来吃，都是跟着导航走，今天无意间走到，感觉很奇妙。上海路起起伏伏，回来骑车的时候感觉更明显。从上海路拐进广州路，人越来越多，后来发现是到了儿童医院。最后终于找到小厨娘，被告知想吃的抹茶盒子下午两点才有，说好的要芒果班戟，回来一吃发现拿的是榴莲。 快到办公室的时候开始下雷阵雨，快去跑回去，没过一会儿雨就停了。两个人中午商量着吃什么，其实这个商圈好吃的很多，韩料啦，串串啦，西餐啦，大众点评上好多评分高的店铺。但是雨停了之后太阳出来了，有点热，两个人都不太想吃辣的，于是就索性吃了鸡鸣汤包。上次去还是清明节。去的路上无意间看到一家小咖啡店，发现店家品味跟我一样哈，竟然想起来用伊索的瓶子插花。","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"Tragic Ending or Peace Ending ?","slug":"my chester","date":"2017-07-20T16:00:00.000Z","updated":"2017-11-27T13:17:58.461Z","comments":true,"path":"2017/07/21/my chester/","link":"","permalink":"http://tankcat2.com/2017/07/21/my chester/","excerpt":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","text":"那个一直嘶吼的他走了，在很多人的青春中躁动的声音消失了，这个世界总是留不住想要留住的人…. 收拾好准备出宿舍门的时候，打开朋友圈，看到有好友转发西菇自杀了，晴天霹雳。 各大媒体、社交平台都开始报道这个消息，朋友圈也开始各种转发，大家明明都还沉浸在新收到的新单mv的推送中，可他就这么离开了。 有的人可能只知道lol登陆界面上的numb，有的人可能是变形金刚的bgm what i’ve done，new divide和iridescent而知道linkin park，有的人可能是因为今天的朋友圈被告知有个乐队的主场自杀了。高三一次月考作文我就以西菇为题材，写了他从悲惨的童年到获得如今的成就，写了他的纹身，他的耳洞，他的嗓音转变，他的专辑，他的这条路到底是有多心酸、坚强与挣扎。他的作品获得了无数粉丝的喜爱，无疑他的作品来源于悲惨的童年经历，但这段经历如今又带走了他的生命，这些因果到底是矛盾的。 西菇的自杀让我想到台湾女作家林奕含，一样是童年被x侵，一样是在作品中透露出自己的无奈和无助，他们感受到的痛苦是真真切切的。可能在挣扎中想要积极向上，也确实创造了许多作品激励并拯救了许多同样饱受苦痛折磨的人，但喧嚣与欢乐始终都是别人的，音乐只是病痛的舒缓剂，不是所有的经历都能云淡风轻地过去，有些事每每回想，总是锥心地痛一次。时间不是万能的，抑郁的人自杀也不是矫情。 他的死对至亲和粉丝来说无疑是悲惨的结局，但他的前半生可能一直在寻找somewhere i belong，而今日凌晨，他找到了。 I wanna let go of the pain I’ve felt so long… somewhere i belong…","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"Linkin Park","slug":"Linkin-Park","permalink":"http://tankcat2.com/tags/Linkin-Park/"},{"name":"Chester Bennington","slug":"Chester-Bennington","permalink":"http://tankcat2.com/tags/Chester-Bennington/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"Storm UI详解","slug":"storm_ui","date":"2017-05-22T08:32:31.000Z","updated":"2017-11-27T13:08:44.357Z","comments":true,"path":"2017/05/22/storm_ui/","link":"","permalink":"http://tankcat2.com/2017/05/22/storm_ui/","excerpt":"","text":"","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Storm Kafka之KafkaSpout","slug":"KafkaSpout","date":"2017-05-18T12:11:31.000Z","updated":"2017-11-27T13:20:19.551Z","comments":true,"path":"2017/05/18/KafkaSpout/","link":"","permalink":"http://tankcat2.com/2017/05/18/KafkaSpout/","excerpt":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。 StaticHosts。这是另一个选择，不过broker$\\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建GlobalPartitionInformation类的实例，如下： 12345678Broker brokerForPartition0 = new Broker(\"localhost\");//localhost:9092,端口号默认为9092Broker brokerForPartition1 = new Broker(\"localhost\",9092);//localhost:9092,显示地指定端口号Broker brokerForPartition2 = new Broker(\"localhost:9092\");GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();partitionInfo.addPartition(0, brokerFroPartition0);// partition0 到 brokerForPartition0的映射partitionInfo.addPartition(1, brokerFroPartition1);partitionInfo.addPartition(2, brokerFroPartition2);StaticHosts hosts = new StaticHosts(partitionInfo); KafkaConfig创建KafkaSpout需要的另一个参数是KafaConfig，通过调用以下两个函数进行对象创建： 12public KafkaConfig(BrokerHosts host,String topic)public KafkaConfig(BrokerHosts host,String topic,String clientId) 其中，host可以为BrokerHost的任何一种实现，topic是一个topic的名称，clientId是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。 目前，KafkaConfig有两种扩展形式，SpoutcConfig提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。zkRoot用于存储consumer的offset，id用于唯一标识当前的spout。 1public SpoutConfig(BrokerHosts hosts,String topic,String zkRoot,String id) 除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout： 1234567891011//将当前的offset保存到Zookeeper的频率public long stateUpdateIntervals = 2000;//用于失效消息的重试策略public String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。public long retryInitialDetails = 0;public double retryDelayMultiplier = 1.0;//连续重试之间的最大延时public long retryDelayMaxMs = 60 * 1000;//当retryLimit低于0时，不停地重新发送失效的消息public int retryLimit = -1;","text":"storm-kafka-XXX.jar提供了核心Storm与Trident的组件Spout的代码实现，用于消费Kafka中存储的数据(0.8.x之后的版本)。本文只介绍核心Storm的KafkaSpout。 对于核心Storm与Trident两个版本的Spout实现，提供了BrokerHost接口，跟踪Kafka broker host$\\rightarrow$partition的映射，并提供KafkaConfig接口来控制Kafka相关的参数。下面就这以上两点进行讲解。 BrokerHost为了对Kafka spout进行初始化，我们需要创建一个BrokerHost的实例，Storm共提供了两种实现方式： ZkHosts。ZkHosts使用Zookeeper的实体对象，可动态地追踪Kafka broker$\\rightarrow$partition之间的映射，通过调用下面两种函数创建ZkHosts: 12public ZkHosts(String brokerZkStr,String brokerZkPath)public ZkHosts(String brokerZkStr) 其中，brokerZkStr是ip:host(主机:端口)，brokerZkPath是存放所有topic和partition信息的根目录，默认值为\\broker。默认地，Zookepper每60秒刷新一次broker$\\rightarrow$partition，通过host:refreshFreqSecs可以改变这个时间。 StaticHosts。这是另一个选择，不过broker$\\rightarrow$partition之间的映射关系是静态的，创建这个类的实例之前，需要首选创建GlobalPartitionInformation类的实例，如下： 12345678Broker brokerForPartition0 = new Broker(\"localhost\");//localhost:9092,端口号默认为9092Broker brokerForPartition1 = new Broker(\"localhost\",9092);//localhost:9092,显示地指定端口号Broker brokerForPartition2 = new Broker(\"localhost:9092\");GlobalPartitionInformation partitionInfo = new GlobalPartitionInformation();partitionInfo.addPartition(0, brokerFroPartition0);// partition0 到 brokerForPartition0的映射partitionInfo.addPartition(1, brokerFroPartition1);partitionInfo.addPartition(2, brokerFroPartition2);StaticHosts hosts = new StaticHosts(partitionInfo); KafkaConfig创建KafkaSpout需要的另一个参数是KafaConfig，通过调用以下两个函数进行对象创建： 12public KafkaConfig(BrokerHosts host,String topic)public KafkaConfig(BrokerHosts host,String topic,String clientId) 其中，host可以为BrokerHost的任何一种实现，topic是一个topic的名称，clientId是一个可选择的参数，作为Zookeeper路径的一部分，存储spout当前数据读取的offset。 目前，KafkaConfig有两种扩展形式，SpoutcConfig提供额外的Zookeeper连接的字段信息，用于控制KafkaSpout特定的行为。zkRoot用于存储consumer的offset，id用于唯一标识当前的spout。 1public SpoutConfig(BrokerHosts hosts,String topic,String zkRoot,String id) 除了以上参数，SpoutConfig包括如下的字段值，用来控制KafkaSpout： 1234567891011//将当前的offset保存到Zookeeper的频率public long stateUpdateIntervals = 2000;//用于失效消息的重试策略public String failedMsgRetryManagerClass = ExponentialBackofMsgRetryManager.class.getName();//指数级别的back-off重试设置。在一个bolt调用OutputCollector.fail()后，用于重新设置的ExponentialBackoffMsgRetryManager。只有在ExponentialBackoffMsgRetryManager被使用时，才有效果。public long retryInitialDetails = 0;public double retryDelayMultiplier = 1.0;//连续重试之间的最大延时public long retryDelayMaxMs = 60 * 1000;//当retryLimit低于0时，不停地重新发送失效的消息public int retryLimit = -1; KafkaSpout只接收一个SpoutConfig的实例作为参数。 下面给出一个实例： 首先创建一个名为couple的topic，如下： 1bin/kafka-topics.sh --create --zookeeper localhost:3030 --partitions 4 --replication-factor 1 --topic couple 写一个简单的Producer，将文件string_data.txt的记录发送到couple中，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class kafkaFileProducer&#123; private final String topic_name; private final String file_name; private final KafkaProducer&lt;String,String&gt; producer; private Boolean isAsync; public kafkaFileProducer(String topic_name,String file_name,Boolean isAsync)&#123; this.file_name=file_name; this.topic_name=topic_name; Properties properties=new Properties(); properties.put(\"bootstrap.servers\", \"localhost:9092\"); properties.put(\"client.id\",\"CoupleProducer\"); properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"value.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\"); producer=new KafkaProducer&lt;String, String&gt;(properties); this.isAsync=isAsync; &#125; public void sendMessage(String key,String value)&#123; long start_time=System.currentTimeMillis(); if(isAsync)&#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key),new CoupleCallBack(start_time,key,value)); &#125;else&#123; try &#123; producer.send(new ProducerRecord&lt;String, String&gt;(topic_name,key,value)).get(); System.out.println(\"Sent message : ( \"+key+\" , \"+value+\" )\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args)&#123; String file_name=\"DataSource/Data/string_data.txt\"; String topic_name=\"couple\"; kafkaFileProducer producer=new kafkaFileProducer(topic_name,file_name,false); int lineCount=0; FileInputStream fis=null; BufferedReader br=null; try &#123; fis=new FileInputStream(file_name); br=new BufferedReader(new InputStreamReader(fis)); String line=null; while((line=br.readLine())!=null)&#123; lineCount++; producer.sendMessage(lineCount+\"\",line); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; class CoupleCallBack implements Callback&#123; private long start_time; private String key; private String message; public CoupleCallBack(long start_time, String key, String message) &#123; this.start_time = start_time; this.key = key; this.message = message; &#125; /* A callback method The user can implement to provide asynchronous handling of request completion. The method will be called when the record sent to the server has been acknowledged. */ @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; long elapsed_time=System.currentTimeMillis()-start_time; if(recordMetadata!=null)&#123; System.out.println(\"Message( \"+key+\" , \"+ message+\" ) sent to partition(\"+recordMetadata.partition()+\" ) , offset(\" +recordMetadata.offset()+\" ) in \"+elapsed_time+\" ms\"); &#125;else&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 编写一个简单的Storm Topology，利用KafkaSpout读取couple中的数据(一条条的句子)，并分割成一个个的单词，统计单词个数，如下： SplitSentenceBolt 1234567891011121314151617public class SplitSentenceBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String sentence=tuple.getStringByField(\"msg\"); System.out.println(tuple.getSourceTask()+\":\"+sentence); String[] words=sentence.split(\" \"); for(String word:words)&#123; basicOutputCollector.emit(new Values(word)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\")); &#125;&#125; WordCountBolt 123456789101112131415161718192021222324public class WordCountBolt extends BaseBasicBolt&#123; private Map&lt;String,Long&gt; counts=null; @Override public void prepare(Map conf,TopologyContext context)&#123; this.counts=new ConcurrentHashMap&lt;&gt;(); super.prepare(conf,context); &#125; public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=this.counts.get(word); if(count==null) count=0L; count++; this.counts.put(word,count); basicOutputCollector.emit(new Values(word,count)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"word\",\"count\")); &#125;&#125; ReportBolt 1234567891011121314public class ReportBolt extends BaseBasicBolt&#123; @Override public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) &#123; String word=tuple.getStringByField(\"word\"); Long count=tuple.getLongByField(\"count\"); String reportMsg=\"&#123; word : \"+word+\" , count : \"+count+\" &#125;\"; basicOutputCollector.emit(new Values(reportMsg)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields(\"message\")); &#125;&#125; KafkaWordCountTopology 123456789101112131415161718192021222324252627282930313233343536373839public class WordCountKafkaTopology &#123; private static final String KAFKA_SPOUT_ID=\"kafka-spout\"; private static final String SPLIT_BOLT_ID=\"split-bolt\"; private static final String WORD_COUNT_BOLT_ID=\"word-count-bolt\"; private static final String REPORT_BOLT_ID=\"report-bolt\"; private static final String CONSUME_TOPIC=\"couple\"; private static final String PRODUCT_TOPIC=\"test\"; private static final String ZK_ROOT=\"/couple\"; private static final String ZK_ID=\"wordcount\"; private static final String TOPOLOGY_NAME=\"word-count-topology\"; public static void main(String[] args)&#123; BrokerHosts brokerHosts=new ZkHosts(\"192.168.1.118:3030\"); SpoutConfig spoutConfig=new SpoutConfig(brokerHosts,CONSUME_TOPIC,ZK_ROOT,ZK_ID); spoutConfig.scheme = new SchemeAsMultiScheme(new MessageScheme()); TopologyBuilder builder=new TopologyBuilder(); builder.setSpout(KAFKA_SPOUT_ID,new KafkaSpout(spoutConfig),3);//需要注意的是，spout的并行度不能超过topic的partition个数！ builder.setBolt(SPLIT_BOLT_ID,new SplitSentenceBolt(),1).shuffleGrouping(KAFKA_SPOUT_ID); builder.setBolt(WORD_COUNT_BOLT_ID,new WordCountBolt()).fieldsGrouping(SPLIT_BOLT_ID,new Fields(\"word\")); builder.setBolt(REPORT_BOLT_ID,new ReportBolt()).shuffleGrouping(WORD_COUNT_BOLT_ID); //builder.setBolt(KAFKA_BOLT_ID,new KafkaBolt&lt;String,Long&gt;()).shuffleGrouping(REPORT_BOLT_ID); Config config=new Config(); Map&lt;String,String&gt; map=new HashMap&lt;&gt;(); //map.put(\"metadata.broker.list\", \"localhost:9092\"); map.put(\"bootstrap.servers\", \"localhost:9092\"); map.put(\"serializer.class\",\"kafka.serializer.StringEncoder\"); config.put(\"kafka.broker.properties\",map); config.setNumWorkers(3); LocalCluster cluster=new LocalCluster(); cluster.submitTopology(TOPOLOGY_NAME,config,builder.createTopology()); Utils.sleep(10000); cluster.killTopology(TOPOLOGY_NAME); cluster.shutdown(); &#125;&#125; ​","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://tankcat2.com/tags/Kafka/"},{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"列存储中常见压缩技术","slug":"compression","date":"2017-05-11T05:40:00.000Z","updated":"2017-11-27T13:25:45.231Z","comments":true,"path":"2017/05/11/compression/","link":"","permalink":"http://tankcat2.com/2017/05/11/compression/","excerpt":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种. 行程编码 Run-length Encoding行程编码的核心思想是将有序列中的相同元素转化成一个三元组&lt;属性值，该值第一次出现的位置，出现的次数&gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势. 位向量 Bit-Vector位向量的核心思想是，将一个列中相同的值转为一个二元组&lt;属性值，在列中出现的位置的位图&gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩. 词典编码 Dictionary Encoding词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文Processing a trillion cells per mouse click上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.","text":"在列数据库中，实用面向列的压缩算法进行数据压缩，并且在处理数据时保持压缩的形式，即不通过解压来处理数据，很大程度上提升了查询性能.凭直觉就能知道，以列为存储形式的数据比以行为存储形式的数据更容易压缩.当处理的数据信息熵较低，即数据的局部性较高，那么压缩算法的性能越好.举个列子来说吧，现在有一张顾客表，包含了[姓名，电话，邮箱，传真]等属性.列存储使得所有的姓名存储在一起，所有的电话号码存储在一起.有一点可以确定的是，电话号码各自之间是要比周围其他属性的数值来得更加相似的. 压缩的优势具体是什么呢？总结起来呢有两点： 减少I/O操作次数.如果数据被压缩了，那么其实一次I/O读取(磁盘到内存/CPU)实际对应的源数据是远远超过不使用压缩技术的读取. 提高查询性能.如果查询执行器可以直接在压缩后的数据上进行操作，在进行具体的操作时不需要进行解压，而这个操作一般开销较大. 列存储的压缩技术一般有消零和空格符算法(Null Supression)、Lerrpel-Ziv算法、词典编码算法(Dictionary Encoding)、行程编码算法(Run-length Encoding)、位向量算法(Bit-Vector Encoding)，其中较为常见的是后三种，接下来也重点介绍这三种. 行程编码 Run-length Encoding行程编码的核心思想是将有序列中的相同元素转化成一个三元组&lt;属性值，该值第一次出现的位置，出现的次数&gt;,适用于有序的列或者可转为有序的列.下面给出一个具体的例子.下图给出一个身高的有序列，使用行程编码，可转化为两个三元组.为了便于管理，可以在三元组上构建索引.需要注意的是，该算法比较适合distinct值较少的列，因为如果列中不同的值较多，比如所有的值都不同，那么创建的三元组的数量就会很大，施展不出该算法的优势. 位向量 Bit-Vector位向量的核心思想是，将一个列中相同的值转为一个二元组&lt;属性值，在列中出现的位置的位图&gt;.下面给出一个简单的例子，图中给出的列是无序的，其中160这个值出现在第0、3、4、6个位置，162出现在第1、2、5个位置，则其位图的表示分别是1001101和0110010.使用该算法，整个列只要用两个简单的二元组就能表示出来.若列中distinct的值较少，则位图还可以用行程编码进行二次压缩. 词典编码 Dictionary Encoding词典编码，顾名思义，主要针对的是字符串的压缩，核心思想是利用简短的编码代替列中某些重复出现的字符串，维护一个字符串与编码的映射，就可以快速确定编码所指代的字符串，这个映射也就是所谓的Dictionary.下面给出12年Google在VLDB论文Processing a trillion cells per mouse click上有关这个算法的例子，将列search_string划分为三个块，每个块中都存在重复的字符串。首先创建一个全局的字典表global_dictionary，该表中包含了search_string中的所有distinct字符串，且每个字符串分配一个全局唯一的id.接着，为每个块也创建一个字典表chunk_dictionary，包含在该块中的所有distinct字符串，为每个字符串分配一个块范围内的id，并且将这个id与该字符串的全局id对应起来，通过这种二级字典表的方式，一个字符串就可以通过全局字典表映射到一个全局id，再通过块字典表映射到一个块id，这样快中就不用再存储真正的字符串了，而是字符串对应的块id，也就是图中的elements.例如要查找chunk 0中第4个element对应的字符串时，找到该element对应的块id是4，对应的全局id是12，再查找全局字典表可知，该element对应字符串”yellow pages”.同样该算法适用于列中distinct字符串较少的情况.","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"压缩","slug":"压缩","permalink":"http://tankcat2.com/tags/压缩/"},{"name":"行程编码","slug":"行程编码","permalink":"http://tankcat2.com/tags/行程编码/"},{"name":"词典编码","slug":"词典编码","permalink":"http://tankcat2.com/tags/词典编码/"},{"name":"位向量","slug":"位向量","permalink":"http://tankcat2.com/tags/位向量/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"视图漫游","slug":"view","date":"2017-05-10T06:17:31.000Z","updated":"2017-11-27T13:13:55.140Z","comments":true,"path":"2017/05/10/view/","link":"","permalink":"http://tankcat2.com/2017/05/10/view/","excerpt":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select. 从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查. 那视图有优点呢？换句话说，为什么要使用视图呢？归纳起来有四点： 简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系. 限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等. 支持列的动态计算.基本表一般都不支持这个功能，比如有一张orders订单表，包含产品数量produce_num与单价produce_price_each两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列total_price(product_num*produce_price_each)就可以直接查询出订单的总价了. 兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用.","text":"百度百科里面有关视图(View)的定义是，“指数据库中的视图，是一个虚拟表，其内容由查询定义”. 从用户的角度来看，一个视图是从一个特定的角度来查看数据库中的数据； 从数据库系统内部来看，视图是存储在数据库中的SQL Select语句，从一个或者多个基本表(相对于虚表而言)中导出的，和基本表类似，视图也包含行和列，但是在物理上不以存储的数据值集的形式存在.下面给出一张图来解释这段话的意思.从图上我们可以看出，数据库并没有对视图的数据进行物理上的存储，存储的只是视图的定义，也就是响应的Select. 从数据库系统外部来看，视图就如同一张基本表，对基本表能够进行的一般操作都可以应用在视图上，比如增删改查. 那视图有优点呢？换句话说，为什么要使用视图呢？归纳起来有四点： 简化负载查询.视图的定义是基于一个查询声明，这个查询声明可能关联了很多底层的表，可以使用视图向数据库的使用者或者外部程序隐藏复杂的底层表关系. 限制特定用户的数据访问权.处于安全原因，视图可以隐藏一些数据，比如社会保险基金表，可以用视图只显示姓名，地址，不显示社会保险号和工资数等. 支持列的动态计算.基本表一般都不支持这个功能，比如有一张orders订单表，包含产品数量produce_num与单价produce_price_each两个列，当需要查询总价时，就需要先查询出所有记录，再在代码中进行计算；而如果使用视图的话，只要在视图中添加一列total_price(product_num*produce_price_each)就可以直接查询出订单的总价了. 兼容旧版本.假设需要对数据库进行重新设计以适应一个新的业务需求，可能需要删除一些旧表，创建一些新表，但是又不希望这些变动会影响到那些旧程序，此时，就可以使用视图来适配那些旧程序.这就像公共API一样，无论内部发生什么改变，不影响上层的使用. 既然说视图也是一种SQL语句，那么它和查询的区别是什么呢？简单来说，有三点： 存储上，视图存储是数据库设计的一部分，而查询则不是； 更新限制上，因为视图来自于基本表，所以可间接地对基本表进行更新，但是存在诸多限制，比如不能在使用了group by语句的视图中插入值. 排序结果上，通过查询，可以对一个基本表进行排序，而视图不可以. 此外，经常对视图的增删改查还是会转换为对基本表的增删改查，会不会降低操作的效率呢？其实未必，尤其是对于多表关联，视图创建后数据库内部会作出相应处理，建立对应的查询路径，反而有利于查询的效率，这就涉及到物化视图的知识了. 维基百科里解释道，物化视图(Materialized View)，也叫做快照，是包含了查询结果的数据库对象，可能是一个远程数据的本地副本，或者是一张表或连接结果的行或者列的子集，等.创建物化视图的过程有时候也被称作是物化，一种缓存查询结果的形式，类似于函数式编程中将函数值进行缓存，有时也称作是“预计算”，用来提高查询的性能与效率. 在关系型数据库中，如果涉及到对基本视图的查找或修改，都会转化为与之对应的基本表的查找或修改.而物化视图采取不同的方法.查询的结果被缓存在一个实体表中，而不是一个视图里，实际存储着数据的，这个实体表会随着基本表的更新而更新. 这种方法利用额外的存储代价和允许部分数据过期的代价，使得查询时的数据访问更加高效.在数据仓库中，物化视图经常使用，尤其是代价较大的频繁基本表查询操作. 和基本视图还有一点不同的是，在物化视图中，可以在任何一列上建立索引，相反，基本视图通常只能在与基本表相关的列上建立索引.","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"视图","slug":"视图","permalink":"http://tankcat2.com/tags/视图/"},{"name":"物化视图","slug":"物化视图","permalink":"http://tankcat2.com/tags/物化视图/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"近日爱读诗词","slug":"poet","date":"2017-05-09T04:44:31.000Z","updated":"2017-11-27T13:17:33.781Z","comments":true,"path":"2017/05/09/poet/","link":"","permalink":"http://tankcat2.com/2017/05/09/poet/","excerpt":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景初夏即事石梁茅屋有弯碕，流水溅溅度两陂。晴日暖风生麦气，绿阴幽草胜花时。ps:读这首诗的那天正好是立夏苔白日不到处，青春恰自来。苔花如米小，也学牡丹开。ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。","text":"闲居初夏午睡起梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。ps:很喜欢杨万里的写景初夏即事石梁茅屋有弯碕，流水溅溅度两陂。晴日暖风生麦气，绿阴幽草胜花时。ps:读这首诗的那天正好是立夏苔白日不到处，青春恰自来。苔花如米小，也学牡丹开。ps:那日选这首诗是有原因的，自己阴差阳错地参加了学院的杰出青年评比。由于自己是保研来的华师大，现在又才研一，除了屈指可数的科研成果，其余方面均为有所建树。不出所料，只拿了一个靠亲朋好友投票来的人气奖。但是，个人成果的匮乏，没有使我退缩，依然自信上场，这毕竟也是一种锻炼，也可以看看别人是如何展示自己的。","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"诗词","slug":"诗词","permalink":"http://tankcat2.com/tags/诗词/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"聚类索引与非聚类索引","slug":"index","date":"2017-05-06T13:08:00.000Z","updated":"2017-11-27T13:22:30.291Z","comments":true,"path":"2017/05/06/index/","link":"","permalink":"http://tankcat2.com/2017/05/06/index/","excerpt":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能. 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示. 非聚集索引非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示. 两者的区别实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别. 我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。我们把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。 如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”。 通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。 两种索引的应用场合 动作描述 聚集索引 非聚集索引 经常对列进行分组排序 √ √ 返回某个范围内的数据 √ × 一个或者极少不同的值 × × 小数目的不同值 √ × 大数目的不同值 × √ 频繁更新的列 × √ 频繁更新索引列 × √ 外键列 √ √ 主键列 √ √","text":"索引，是对数据库表中一列或者多列的值进行排序的一种数据结构，以便于快速访问数据库表的特定信息.如果没有索引，则需要遍历整张表，直到定位到所需的信息为止.可见，索引是用来定位的，加快数据库的查询速度. 基本知识索引可分为聚集索引与非聚集索引.下面就两者分别介绍. 聚集索引在聚集索引中，表中行的物理顺序与索引的顺序相同，且一张表只能包含一个聚集索引.聚集索引类似物电话簿，索引可以包含一个或者多个列，类似电话簿按照姓氏和名字进行组织一样. 聚集索引很适用于那些经常要搜索范围值的列。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理上相邻.比如，若应用程序执行的某个查询经常检索某一个日期范围的记录，使用聚集索引可以迅速找到包含开始日期的行，接着检索相邻的行，直到到达结束日期.这样有助于提高类似查询的性能. 索引是通过二叉树的数据结构来描述的，我们可以这么理解聚集索引：索引的叶子节点就是数据节点，如下图所示. 非聚集索引非聚集索引的逻辑顺序与表中行的物理存储数据不同，数据结构中的叶节点仍然是索引节点，有一个指针指向对应的数据块，如下图所示. 两者的区别实际上，可把索引理解为一种特殊的目录。下面举个例子来说明一下聚集索引与非聚集索引的区别. 我们的汉语字典的正文本身就是一个聚集索引。比如，我们要查“安”字，就会很自然地翻开字典的前几页，因为“安”的拼音是“an”，而按照拼音排序汉字的字典是以英文字母“a”开头并以“z”结尾的，那么“安”字就自然地排在字典的前部。如果翻完了所有以“a”开头的部分仍然找不到这个字，那么就说明您的字典中没有这个字；同样的，如果查“张”字，那也会将您的字典翻到最后部分，因为“张”的拼音是“zhang”。也就是说，字典的正文部分本身就是一个目录，您不需要再去查其他目录来找到您需要找的内容。我们把这种正文内容本身就是一种按照一定规则排列的目录称为“聚集索引”。 如果我们认识某个字，可以快速地从自动中查到这个字。但也可能会遇到不认识的字，不知道它的发音，这时候，就不能按照刚才的方法找到我们要查的字，而需要去根据“偏旁部首”查到要找的字，然后根据这个字后的页码直接翻到某页来找到您要找的字。但结合“部首目录”和“检字表”而查到的字的排序并不是真正的正文的排序方法，比如查“张”字，我们可以看到在查部首之后的检字表中“张”的页码是672页，检字表中“张”的上面是“驰”字，但页码却是63页，“张”的下面是“弩”字，页面是390页。很显然，这些字并不是真正的分别位于“张”字的上下方，现在看到的连续的“驰、张、弩”三字实际上就是他们在非聚集索引中的排序，是字典正文中的字在非聚集索引中的映射。我们可以通过这种方式来找到您所需要的字，但它需要两个过程，先找到目录中的结果，然后再翻到所需要的页码。我们把这种目录纯粹是目录，正文纯粹是正文的排序方式称为“非聚集索引”。 通过以上例子，我们可以理解到什么是“聚集索引”和“非聚集索引”。进一步引申一下，我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。 两种索引的应用场合 动作描述 聚集索引 非聚集索引 经常对列进行分组排序 √ √ 返回某个范围内的数据 √ × 一个或者极少不同的值 × × 小数目的不同值 √ × 大数目的不同值 × √ 频繁更新的列 × √ 频繁更新索引列 × √ 外键列 √ √ 主键列 √ √","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"索引","slug":"索引","permalink":"http://tankcat2.com/tags/索引/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"<刀锋>观后感","slug":"daofeng","date":"2017-04-20T12:11:31.000Z","updated":"2017-11-27T13:25:51.525Z","comments":true,"path":"2017/04/20/daofeng/","link":"","permalink":"http://tankcat2.com/2017/04/20/daofeng/","excerpt":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.” “因为人不论男男女女，都不仅仅是他们自身；他们也是自己出生的乡土，学步的农场或城市公寓，儿时玩的游戏，私下听来的山海经，吃的饭食，上的学校，关心的运动，吟哦的诗章，和信仰的上帝。这一切东西把他们造成现在这样，而这些东西都不是道听途说就可以了解的，你非得和那些人生活过。要了解这些，你就得是这些。 ” 无论中英文，都是一流的文字，解释了各种文化之间的冲突，以及冲突误解的永恒性。 很少有外国作品上让我读得这么舒服，这完全归功于周旭良老师的翻译功底，整本书翻译地非常好，读起来如沐春风。书写得很平淡，但每个角色都很有意思，拉里最为迷人。很奇怪，刚开始看的时候我脑子里对拉里的想象，居然是血战钢锯岭里的戴斯蒙特，这里也仅是人物形象。拉里一直追寻的答案，等同于追求终极真理，而这个问题最终都会归结到理性与精神的绝对满足。真的很难以想象，拉里这样的人，现实中又有多少，他们的生活又是怎样的？这种绝对的内心的泰然平和，我生生世世估计也做不到吧。","text":"先摘抄一段刀锋里面我很喜欢的一段话，“For men and women are not only themselves; they are also the region in which they were born, the city apartment or the farm in which they learnt to walk, the games they played as children, the tales they overheard, the food they ate, the schools they attended, the sports they followed, the poets they read and the God they believed in. It is all these things that have made them what they are, and these are the things that you can’t come to know by hearsay, you can only know them if you have lived them. You can only know them if you are them.” “因为人不论男男女女，都不仅仅是他们自身；他们也是自己出生的乡土，学步的农场或城市公寓，儿时玩的游戏，私下听来的山海经，吃的饭食，上的学校，关心的运动，吟哦的诗章，和信仰的上帝。这一切东西把他们造成现在这样，而这些东西都不是道听途说就可以了解的，你非得和那些人生活过。要了解这些，你就得是这些。 ” 无论中英文，都是一流的文字，解释了各种文化之间的冲突，以及冲突误解的永恒性。 很少有外国作品上让我读得这么舒服，这完全归功于周旭良老师的翻译功底，整本书翻译地非常好，读起来如沐春风。书写得很平淡，但每个角色都很有意思，拉里最为迷人。很奇怪，刚开始看的时候我脑子里对拉里的想象，居然是血战钢锯岭里的戴斯蒙特，这里也仅是人物形象。拉里一直追寻的答案，等同于追求终极真理，而这个问题最终都会归结到理性与精神的绝对满足。真的很难以想象，拉里这样的人，现实中又有多少，他们的生活又是怎样的？这种绝对的内心的泰然平和，我生生世世估计也做不到吧。","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"毛姆","slug":"毛姆","permalink":"http://tankcat2.com/tags/毛姆/"},{"name":"刀锋","slug":"刀锋","permalink":"http://tankcat2.com/tags/刀锋/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"<简明美国史>观后感","slug":"historyUSA","date":"2017-04-16T12:11:31.000Z","updated":"2017-11-27T13:23:47.390Z","comments":true,"path":"2017/04/16/historyUSA/","link":"","permalink":"http://tankcat2.com/2017/04/16/historyUSA/","excerpt":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。 有人说陈勤老师的这本美国史写得实在是太简太浅显，但是对我这种历史水平只停留在高中课本上的”史盲“来说，基本上是够了，从脉络上了解美国自1620年《五月花号公约》至2016年奥巴马最后的执政之间所发生的重大历史事件，这其中涵盖了美国从英属殖民地开始，到1776年《独立宣言》宣告独立，到1860年林肯领导南北战争，再到一战、二战、冷战，以及至今美国发生的种种。读完全书的第一感受是，陈勤老师应当是亲美派的，书中给我描述的美国是一个有趣、鲜活、有人味的美国，虽然对历史变革中发生的流血事件只是轻描淡写地带过，但还是能些许体会到”世界何尝不简单，历史从来不温柔“这一面。读完一遍脑海中对美国的历史线还是稍有混乱，有时间自己再整理整理。","text":"相对于厚重的、教科书式的历史文献，这是一本薄薄的，轻松的普及读本，没有平板数据，没有经济图表，却把把美国历史的端倪，黑暗，辉煌，血腥，光明清晰地勾勒出来。 有人说陈勤老师的这本美国史写得实在是太简太浅显，但是对我这种历史水平只停留在高中课本上的”史盲“来说，基本上是够了，从脉络上了解美国自1620年《五月花号公约》至2016年奥巴马最后的执政之间所发生的重大历史事件，这其中涵盖了美国从英属殖民地开始，到1776年《独立宣言》宣告独立，到1860年林肯领导南北战争，再到一战、二战、冷战，以及至今美国发生的种种。读完全书的第一感受是，陈勤老师应当是亲美派的，书中给我描述的美国是一个有趣、鲜活、有人味的美国，虽然对历史变革中发生的流血事件只是轻描淡写地带过，但还是能些许体会到”世界何尝不简单，历史从来不温柔“这一面。读完一遍脑海中对美国的历史线还是稍有混乱，有时间自己再整理整理。","categories":[{"name":"Reviews","slug":"Reviews","permalink":"http://tankcat2.com/categories/Reviews/"}],"tags":[{"name":"历史","slug":"历史","permalink":"http://tankcat2.com/tags/历史/"}],"keywords":[{"name":"Reviews","slug":"Reviews","permalink":"http://tankcat2.com/categories/Reviews/"}]},{"title":"再见我的暴力女王","slug":"evil","date":"2017-02-27T12:11:31.000Z","updated":"2017-11-27T13:24:27.371Z","comments":true,"path":"2017/02/27/evil/","link":"","permalink":"http://tankcat2.com/2017/02/27/evil/","excerpt":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了； 到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她； 没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。 今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。 最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。","text":"初二的时候，老妈同事的儿子来我家排练吹笛子，给我讲了生化危机3，当时没记住名字； 后来在家里的电脑上翻到了，还是没字幕英文版的，就这样看完了； 到了高二，周末回家，把第一部第二部给补完了，没看过瘾，导致后来第二部反复拿出来看，可能看了有十多遍了，里面的角色很鲜明，很喜欢吉尔，喜欢短发帅气的她； 没过多久，第四部就上映了，在网上看过一遍之后才去老文化馆那边的电影院再看一遍，记得那次的3D眼睛还是硬纸片做的；第五部也是在网上看的枪版，越来越没趣。 今天，和实验室的小伙伴一起看了终章，看完有点失落，追了这么多年的欧美暴力女王，就这么结束了。我不说这是情怀，有点装逼，但可能也是因为生化3，开始了我喜欢丧尸类型片子之路。等网上出了终章的未删减版，我要再刷一波。 最后，刚刚在知乎上看到“如何评价生化危机6”里面有个回答说，我觉得最大的彩蛋是我旁边的哥们儿看到女主骑着摩托绝尘而去的时候，突然说了一句，她真该进复联。。。","categories":[{"name":"Reviews","slug":"Reviews","permalink":"http://tankcat2.com/categories/Reviews/"}],"tags":[{"name":"生化危机","slug":"生化危机","permalink":"http://tankcat2.com/tags/生化危机/"}],"keywords":[{"name":"Reviews","slug":"Reviews","permalink":"http://tankcat2.com/categories/Reviews/"}]},{"title":"Kafka快速入门","slug":"kafka_quickstart","date":"2017-02-27T12:11:31.000Z","updated":"2017-11-27T13:21:45.075Z","comments":true,"path":"2017/02/27/kafka_quickstart/","link":"","permalink":"http://tankcat2.com/2017/02/27/kafka_quickstart/","excerpt":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf 下载Kafka 我使用的是kafka_2.10-0.10.2.1版本 12tar -xvzf kafka_2.10-0.10.2.1cd kafka_2.10-0.10.2.1/config 接下来进行参数配置：server.properties 123456789vim server.properties...# 修改broker.id,全局唯一# 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔zookeeper.connect=192.168.115:2181# 设置话题的删除,默认值为falsedelete.topic.enable=true# 设置数据日志路径log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs 启动 Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的： 1zkServer.sh start 接着启动Kafka: 1bin/kafka-server-start.sh config/server.properties 创建topic 使用下面的命令创建名为single_node的topic，副本数为1，分区数为1，命令执行结束后，kafka-logs路径下就会生成一个single_node-0的文件夹。 1bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node 发布与消耗数据 执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。 1234bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_nodewxtzfi love u 执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据： 1234bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginningwxtzfi love u 以上均是单机版的Kafka配置与使用。","text":"翻译自kafka documentation的quick start 部分。 下载Zookeeper 我使用的是zookeeper-3.4.6版本 12tar -xvzf zookeeper-3.4.6.tgzcd zookeeper-3.4.6/conf 将zoo_example.cfg改名为zoo.cfg，并在/etc/profile中设置环境变量： 123vim /etc/profileexport ZK_HOME=/home/admin/zookeeper-3.4.6export PATH=$PATH:$ZK_HOME/bin:$ZK_HOME/conf 下载Kafka 我使用的是kafka_2.10-0.10.2.1版本 12tar -xvzf kafka_2.10-0.10.2.1cd kafka_2.10-0.10.2.1/config 接下来进行参数配置：server.properties 123456789vim server.properties...# 修改broker.id,全局唯一# 修改zookeeper.connect，形式为host:port，多个数据项用逗号分隔zookeeper.connect=192.168.115:2181# 设置话题的删除,默认值为falsedelete.topic.enable=true# 设置数据日志路径log.dirs=/home/admin/kafka_2.10-0.10.2.1/kafka-logs 启动 Kafka使用Zookeeper，所以需要先启动Zookeeper，我没有使用Kafka内置的： 1zkServer.sh start 接着启动Kafka: 1bin/kafka-server-start.sh config/server.properties 创建topic 使用下面的命令创建名为single_node的topic，副本数为1，分区数为1，命令执行结束后，kafka-logs路径下就会生成一个single_node-0的文件夹。 1bin/kafka-topics.h --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic single_node 发布与消耗数据 执行下面的命令创建producer进程，从标准输入中获取数据，并发送到Kafka集群中的single_node这个topic中，默认地，每一行将作为单独的一条信息发送出去。 1234bin/kafka-console-producer.sh --broker-list localhost:9092 --topic single_nodewxtzfi love u 执行下面的命令创建consumer进程，消耗指定topic的数据，这里就是标准输出的数据： 1234bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic single_node --from-beginningwxtzfi love u 以上均是单机版的Kafka配置与使用。","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://tankcat2.com/tags/Kafka/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","slug":"Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems","date":"2017-01-17T05:11:31.000Z","updated":"2017-11-27T13:13:22.777Z","comments":true,"path":"2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","link":"","permalink":"http://tankcat2.com/2017/01/17/Robust and Skew-resistant Parallel Joins in Shared-Nothing Systems/","excerpt":"并行连接处理的两种基本框架 hash-based 基于哈希，如下图所示，分为四个步骤： partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集； distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R_{ik}$ 和 $S_{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$R_k=\\bigcup_{i=1}^{n}R_{ik}$,$S_k=\\bigcup_{i=1}^{n}S_{ik}$; build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表； prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果. duplication-based 基于副本，如下图所示，分为三个步骤： duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$R_k=\\bigcup_{i=1}^{n}R_i=R$即为全集R； build构建，构建哈希表，与hash-based相似； prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似. PRPD连接算法 PRPD定义：partial redistribution &amp; partition duplication，即将hash-based和duplication-based相结合. 处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S_{loc}$是倾斜数据子集，$S_{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R_{dup}$是与$S_{loc}$连接键值相同的数据子集，$R_{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S_{redis}$进行连接. 存在的问题： global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布； broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加. 本文提出算法PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进. Semijoin-based 连接 semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行不完全连接查询，即查找到匹配的数据行就返回，不再继续查找. semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤： 类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线); 对各个节点中的数据集$S_i$在属性b上做投影操作得到$\\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中； 每个计算节点k收到数据集S的key 子集$\\pi_b(S_{ik})$，和数据集R的子集$R_k=\\bigcup_{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)； 各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果. 特点： 由于投影操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以解决数据倾斜； 第2和第3步骤，只传输key和能连接上的元组，因此减轻了网络传输代价. 对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量. Query-based 连接 根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下： 若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value； 返回的value和本地的S数据集做最后的实际连接操作，输出连接结果. 特点： 对于高选择性的连接处理，优势大，减轻网络通信负载； 对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证的序列以便最后的连接处理，因此可能降低处理速度. 折中综合：通过一个计数器来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based. 性能问题本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷. PRPQ连接算法 PRPQ定义：partial redistribution &amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤： R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)； Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)； Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$R_k=\\bigcup_{i=1}^{n}R_{ik}$建立哈希表，(1). 对接收到的集合$\\bigcup_{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\\pi_b(h_{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i； Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\\bigcup$第4部分的连接结果. 特点： 与query-based算法相比 当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性. 继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. 与PRPD算法相比 最主要的区别在于，使用query而不是duplication操作. PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价. 对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载.","text":"并行连接处理的两种基本框架 hash-based 基于哈希，如下图所示，分为四个步骤： partition划分，将原先每个节点上存储的$R_i$和$S_i$按照连接属性键的哈希值进行划分，比如图中，将第一个节点(大的实线矩形)中的$R_1$和$S_1$分别划分为k个子集； distribution分发，根据连接属性键的哈希值，将上面的子集分发到另外一个空闲节点上，比如图中，将每个节点中的第k个子集$R_{ik}$ 和 $S_{ik}$ 同时分发到一个空闲节点上，那么这个空闲节点存储的数据为$R_k=\\bigcup_{i=1}^{n}R_{ik}$,$S_k=\\bigcup_{i=1}^{n}S_{ik}$; build构建，在空闲节点中，对数据集$R_k$进行扫描，并对它构建一个存储在内存中的哈希表； prob检测，在空闲节点中，对数据集$S_k$进行遍历，判断每一条数据的键值是否存在于上面构建的哈希表中，并输出连接结果. duplication-based 基于副本，如下图所示，分为三个步骤： duplication复制，针对每个节点，将其中存储的数据集$R_i$广播到其他所有并行节点上(不是空余节点)，这样在广播操作结束后，所有节点上的数据集$R_k=\\bigcup_{i=1}^{n}R_i=R$即为全集R； build构建，构建哈希表，与hash-based相似； prob检测，遍历另外一个数据集，输出连接结果，与hash-based相似. PRPD连接算法 PRPD定义：partial redistribution &amp; partition duplication，即将hash-based和duplication-based相结合. 处理流程，如下图所示：处理数据集R和S的连接，假设R是均匀分布，S是倾斜分布. 将每个节点中存储的S划分为两部分，$S_{loc}$是倾斜数据子集，$S_{redis}$是剩余的非倾斜数据子集.前者保留才原节点中不动，后者需要根据连接键值重新分发到一个空余节点中，类似与hash-based中的distribution操作. 同样，将每个节点中存储的R划分为两部分，$R_{dup}$是与$S_{loc}$连接键值相同的数据子集，$R_{redis}$是剩余的数据子集. 前者需要广播到其余所有的原节点中，类似于duplication-based中的duplication操作，后者需要根据连接键值重新分发到空余节点中，按照hash-based的最后两步，与$S_{redis}$进行连接. 存在的问题： global skew，涉及到的对数据集S和R的划分需要预先获取每个节点上的倾斜键值的分布； broadcasting，数据子集R的广播操作对网络负载施压，并且广播量将随着节点数量的增加而增加. 本文提出算法PRPQ是基于两个可有效处理数据倾斜的分布式连接算法，semijoin-based和query-based. 基于这两者，提出改进. Semijoin-based 连接 semi-join的定义：半连接，从一个表中返回的行与另一个表中数据进行不完全连接查询，即查找到匹配的数据行就返回，不再继续查找. semijoin-based连接，如下图所示. 数据集R和S在各自的属性a和b上做连接操作，分为以下四步骤： 类似于hash-based中的第1,2两步，将各个节点中的数据集$R_i$按照连接属性的哈希值进行切分，再将元组分发到各自对应的空闲计算节点中(图中的红色虚线); 对各个节点中的数据集$S_i$在属性b上做投影操作得到$\\pi_b(S_i)$，根据哈希值将这些属性b的unique key分发到计算节点中； 每个计算节点k收到数据集S的key 子集$\\pi_b(S_{ik})$，和数据集R的子集$R_k=\\bigcup_{i=1}^nR_{ik}$，对这两个子集做连接操作，将能连接上的R元组回发到各自的原节点i上(图中的③号线)； 各个原节点接收到retrieval返回的R集元组，与本地存储的S集元组做最后实际的连接操作，输出结果. 特点： 由于投影操作，S数据集只考虑unique key，而不考虑key的粒度，因此可以解决数据倾斜； 第2和第3步骤，只传输key和能连接上的元组，因此减轻了网络传输代价. 对于高选择性的连接，第2步和第3步中，S集的key和retrieval的R集元组交叠的数据量较大，仍然可能带来很大的网络通信量. Query-based 连接 根据semijoin-based的第三个特点(存在的问题)，对第3和第4步进行改进，则有query-based连接算法.改进如下： 若存在连接上的key和R集元组，则只返回value，而不是整个元组；若没有数据能连接上，则返回值为null的value； 返回的value和本地的S数据集做最后的实际连接操作，输出连接结果. 特点： 对于高选择性的连接处理，优势大，减轻网络通信负载； 对于低选择性的连接处理，存在问题，对于第3步没有能连接上的key，需要给返回的value赋值为null，以保证的序列以便最后的连接处理，因此可能降低处理速度. 折中综合：通过一个计数器来统计第3步骤中null出现的比例，从而动态地选择适合的方法，即当null比例较低时，使用query-based，否则使用semijoin-based. 性能问题本文比较推崇直接在内存中进行连接计算，而不使用基于磁盘的计算框架比如MapReduce. 因此网络通信成本至关重要.当处理大规模的连接操作，上述两种方法都可能遭遇无法接受的网络通信负荷. PRPQ连接算法 PRPQ定义：partial redistribution &amp; partial query，将hash-based和query-based相结合，如下图所示，分为四步骤： R distribution，与hash-based类似，将各个节点i上存储的数据集$R_i$根据连接属性a的哈希值，重新分发到一个空余计算节点上(图中红色虚线①)； Push query keys，将各个节点i上存储的数据集$S_i$划分为两部分，低数据倾斜部分$S_i^{‘}$和高数据倾斜部分$h_i$. 根据连接属性b的哈希值，同时将$S_i^{‘}$的元组和$h_i$的投影unique key集合$\\pi_b(h)$重新分发到对应的计算节点上(图中紫色虚线②)； Return queried values，在每个计算节点k上，与hash-based的第3步类似，对集合$R_k=\\bigcup_{i=1}^{n}R_{ik}$建立哈希表，(1). 对接收到的集合$\\bigcup_{i=1}^{n}S_{ik}^{’}$进行遍历，并查找哈希表，直接输出连接结果；(2). 对接收到的key集合$\\pi_b(h_{ik})$也遍历并查找路由表，如果没有匹配的key，则将retrieval的value置为null，若有匹配的key，则返回对应R的value.所有返回的value和节点k接收到key的顺序一致，并返回发送到原节点i； Result lookup，接收到计算节点返回的value集合之后，在原节点中遍历value，并和本地存储的数据集S的高倾斜部分h进行连接，输出连接结果：若value为null，则继续扫描下一个；若不为空，则必定存在一个R和S的元组能连接上. 因此，最终的连接结果是第3步骤的部分结果$\\bigcup$第4部分的连接结果. 特点： 与query-based算法相比 当处理的数据集包含大量倾斜程度低的数据时，在网络上传送的query key以及对应的value的规模将相当小. 在倾斜程度为0的情况下，即为hash-based算法的实现.因此，PRPQ算法有效地弥补了query-based算法的缺点，提高了鲁棒性. 继承了query-based算法的优点，处理倾斜程度高的数据集时，大大减少网络通信量，因为高倾斜的元组并没有直接在网络上传输，而仅仅传输其unique key. 与PRPD算法相比 最主要的区别在于，使用query而不是duplication操作. PRPQ涉及到的数据划分(第2步骤对S数据集进行倾斜程度的划分)，只定性分析局部的倾斜度，而不需要全局的；而PRPD需要获取全局数据集S的倾斜分布信息.关于如何定义全局倾斜，PRPD在连接操作之前将倾斜程度高的元组均匀分发到所有节点上.这个预处理操作会带来额外的通信代价. 对于倾斜程度中等mid-skew的元组，如何确定问题，PRPD使用广播的操作，可能导致节点负荷超载. 算法实现每个节点上skew元组的提取是基于局部倾斜量化，因此引入一个阈值参数，即当一个key出现的次数超过该阈值，则视这个key为skewed. 下面先整理如何处理阈值参数，再整理PRPQ算法的具体实现. 局部数据倾斜有很多方法可实现局部数据倾斜的快速监测，比如采样，扫描等.但是这些与本文的思路无关，所以本文仅仅在每个节点中对key的出现次数进行计数，按照降序排列，并保存到文件中. 在每一次的参数测试中，每个节点预先读取出现次数超过t的key，写入一个ArrayList中，并视它们为skew key. PRPQ具体实现具体算法和前面的四个步骤一一对应，如下： 在每个原节点中，将所有的元组读取到一个ArrayList中，处理数据集R的元组. 首先初始化一个R_c，用于收集分组的元组，R_c的初始化大小为计算节点的数量.接着，各个线程读取ArrayList中的R集元组，根据连接key的哈希值对元组进行分组.最后，将分好组的元组分发到对应的计算节点中(算法中的here表示当前计算节点的id). 根据给定的阈值参数t，对数据集S进行划分，倾斜的key被读入一个hashset，并且所有对应的元组被存储到一个hashmap中，剩余的非倾斜元组存储到一个$S^{’}_c$中.接着对hashmap进行投影操作，将所有的unique keys保存到key_c中.最后将key_c和$S^{’}_c$按照key的哈希值分发到对应的计算节点上. 在计算节点中，对接收到的R集元组建立一个哈希表T’，对数据集$S’$元组进行遍历，并查找哈希表，若有匹配的key，则输出连接结果.同时遍历key集key_c，并查找哈希表，若不存在匹配的key，则返回值为null的value到对应的原节点，否则返回实际key对应的value. 倾斜元组的连接结果可以通过遍历查询返回的value集合，若value为null，则不存在能连接上的S集元组，否则输出最终连接结果. 实验对比数据集的选取：用作基准的数据集模仿决策支援系统下的连接操作.数据集R的cardinality为64M，数据集S的cardinality为1GB.由于数据仓储中数据一般以面向列的形式存储，所以实验中将数据格式设置为的键值对，其中key和value均是8字节整型. 工作负载的选取：设置数据集R和S之间存在外键的关系，保持R的主键的unique，而在S中为对应的外键增加skew.除此之外，若S是统一分布的，它们中的每一个以相同的概率匹配关系R中的元组.对于倾斜的元组，它们的unique key在节点间均匀分布，并且每一个均能与R匹配上.下表给出了数据集S的分布情况. S key distribution Partition Size Zipf skew=0,1,1.4 均匀evenly 512M Linear f(r)=46341-r,23170 排序范围sort-range 1GB,2GB Zipf分布中，skew=0表示统一分布，skew=1表示排名前十的key占据总量14%，skew=1.4表示排名前十的key占据总量68%.线性分布中，使用f(r)来描述key的分布情况，其中f(r)=46341-r表示频率最高的key出现46341次，频率第二的key出现46340次.使用该函数生成的数据集可以看作low-skewed的数据集.f(r)=23170表示所有的key都是均匀分布的，但是重复次数较高.f(r)对应的两个数据集均为1GB的大小，有46341个unique key. R和S在计算节点中的分布情况：R均匀分布在所有的节点上，而S使用均匀和排序范围分布.均与分布保证每个计算节点上skewed元组的数量相同；排序范围分布是先将所有的元组按照键的频率排序，然后等分成大小一样的块，再将块按照次序分配到每个计算节点上.因此每个节点上skewed元组的数量差距可能会比较大. 实验共从运行时间、网络通信、负载均衡、可扩展性四个方面来进行比较.这里只就运行时间稍作整理. 运行时间记录Hash-based算法、PRPD、PRPQ和query-based算法的运行时间，如下图所示.当S是均匀分布(第一组数据skew=0)，Hash、PRPD和PRPQ算法的性能相近，远远优于Query算法；当S是low skewed时，PRPD和PRPQ均比另外两种算法快；当S是high skewed时，Hash算法性能最差，而其余三种性能相近，则可得出结论，其余PRPD、PRPQ和Query可以较好地处理数据倾斜.随着skew程度的增加，Hash算法的执行时间增长剧烈，而Query算法呈现下降趋势.而PRPD和PRPQ算法呈现平稳的下降趋势. 上图展示是选择最佳频率阈值t的性能，原文中关于不同阈值的实验这里不再整理，基本情况是无论t值如何变化以及分区计划如何，PRPQ的运行时间是低于PRPD的.","categories":[{"name":"Papers","slug":"Papers","permalink":"http://tankcat2.com/categories/Papers/"}],"tags":[{"name":"Join","slug":"Join","permalink":"http://tankcat2.com/tags/Join/"}],"keywords":[{"name":"Papers","slug":"Papers","permalink":"http://tankcat2.com/categories/Papers/"}]},{"title":"使用Storm遇到的问题以及解决方案","slug":"stormproblems","date":"2016-12-30T05:45:31.000Z","updated":"2017-11-27T13:14:15.109Z","comments":true,"path":"2016/12/30/stormproblems/","link":"","permalink":"http://tankcat2.com/2016/12/30/stormproblems/","excerpt":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错： 12345678910111213142016-12-30 12:41:17.269 b.s.event [ERROR] Error when processing eventjava.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: File '/home/admin/stormdata/data/supervisor/localstate/1480504905565' does not exist at backtype.storm.utils.LocalState.partialSnapshot(LocalState.java:118) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.utils.LocalState.get(LocalState.java:126) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.local_state$ls_local_assignments.invoke(local_state.clj:83) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:321) ~[storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.applyToHelper(AFn.java:154) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:626) ~[clojure-1.6.0.jar:?] at clojure.core$partial$fn__4228.doInvoke(core.clj:2468) ~[clojure-1.6.0.jar:?] at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.6.0.jar:?] at backtype.storm.event$event_manager$fn__7258.invoke(event.clj:40) [storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?] at java.lang.Thread.run(Thread.java:744) [?:1.7.0_45] 找不到’/home/admin/stormdata/data/supervisor/localstate/1480504905565’这个文件夹，网上找了下原因，给出的答案是stop the server without previously stop the supervisor，就是说可能是由于不正常关机造成状态不一致，具体原因不知，解决方案是删除stormdata/data/supervisor整个目录即可.","text":"集群中有3台服务器执行 storm supervisor命令后自动退出，supervisor起不来，后来在 logs目录下的supervisor.log日志文件中查到以下报错： 12345678910111213142016-12-30 12:41:17.269 b.s.event [ERROR] Error when processing eventjava.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: File '/home/admin/stormdata/data/supervisor/localstate/1480504905565' does not exist at backtype.storm.utils.LocalState.partialSnapshot(LocalState.java:118) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.utils.LocalState.get(LocalState.java:126) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.local_state$ls_local_assignments.invoke(local_state.clj:83) ~[storm-core-0.10.0.jar:0.10.0] at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:321) ~[storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.applyToHelper(AFn.java:154) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:626) ~[clojure-1.6.0.jar:?] at clojure.core$partial$fn__4228.doInvoke(core.clj:2468) ~[clojure-1.6.0.jar:?] at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.6.0.jar:?] at backtype.storm.event$event_manager$fn__7258.invoke(event.clj:40) [storm-core-0.10.0.jar:0.10.0] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?] at java.lang.Thread.run(Thread.java:744) [?:1.7.0_45] 找不到’/home/admin/stormdata/data/supervisor/localstate/1480504905565’这个文件夹，网上找了下原因，给出的答案是stop the server without previously stop the supervisor，就是说可能是由于不正常关机造成状态不一致，具体原因不知，解决方案是删除stormdata/data/supervisor整个目录即可. 在集群环境下日志清理，自己写了一个脚本clear-log.sh，主要是删除apache-storm-XXX下的logs文件里的日志文件，如下： 1234567STORM_HOME=/home/admin/apache-storm-0.10.0 HOSTS_FILE=/home/admin/hosts.txtcat $HOSTS_FILE | while read line do ssh $line \"rm -rf $STORM_HOME/logs/*\" &lt; /dev/null doneecho \"remove log files...done\" 其中STORM_HOME是storm的安装路径，hosts.txt是集群中各个节点的地址，我自己的配置如下： 1234567891011121314admin@10.11.1.53admin@10.11.1.40admin@10.11.1.41admin@10.11.1.42admin@10.11.1.45admin@10.11.1.46admin@10.11.1.51admin@10.11.1.53admin@10.11.1.54admin@10.11.1.55admin@10.11.1.56admin@10.11.1.58admin@10.11.1.60admin@10.11.1.64 编辑完之后执行chmod +x clear-log.sh命令使得该文件获得可执行权限，再执行./clear-log.sh运行该脚本即可.","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"关于苏打绿的题库","slug":"knowledgebase","date":"2016-10-26T08:26:31.000Z","updated":"2017-11-27T13:19:34.859Z","comments":true,"path":"2016/10/26/knowledgebase/","link":"","permalink":"http://tankcat2.com/2016/10/26/knowledgebase/","excerpt":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴","text":"1.团队成员 6个人。 吴青峰：主唱，1982.8.30，台湾台北，国立政治大学中文系，钢琴、口琴、口风琴、打击乐器、长笛 谢馨仪：贝斯手，1982.4.16，台湾台北，国立政治大学科技管理研究所，贝斯、钢琴、摇滚吉他、古筝 史俊威：鼓手，1979.8.26，台湾，国立政治大学社会系，吉他、鼓、口琴 龚钰祺：键盘手+中提琴手,1980.12.16，台湾，国立台北艺术大学音乐研究所，中提琴、钢琴、电子琴 刘家凯：电子吉他手，1982.2.5，台湾台北，国立政治大学心理系、国立阳明大学脑科学研究所，吉他 何景扬：木吉他手，1982.4.4，台湾，国立政治大学公共行政研究所，吉他、乌克丽丽 2. 重要时间节点 2001年成立于校园，2003年确立6人阵容，2004年5月，苏打绿正式出道，发行第一张单曲《空气中的视听与幻觉》。 2005年，苏打绿发行了《SodaGreen》。 2006年，苏打绿发行专辑《小宇宙》。 2007年11月，苏打绿发行专辑《无与伦比的美丽》。 2008年5月，苏打绿发行专辑《陪我歌唱》。 韦瓦第计划：2009年5月8日，苏打绿发行了第五张专辑《春·日光》；2009年9月11日，苏打绿发行专辑《夏·狂热》；2013年9月18日，苏打绿发行专辑《秋：故事》，该专辑成为2013年度iTunes Store最受欢迎专辑；2015年9月23日，苏打绿发行专辑《冬·未了》。 2011年 2011年11月11日，苏打绿发行专辑《你在烦恼什么》。 2014年，苏打绿开始了十周年世界巡回演唱会。 3. 唱片公司 2004-2009，林浩哲音乐社； 2009-至今，环球唱片 4. “游乐园鱼丁糸”比赛题第一轮 单选题（以下各题四个选项中,只有一个选项正确）： EP《空气中的视听与幻觉》碟的颜色为：DA、白B、红C、蓝D、墨绿 迄今为止仍未引进内地的苏打绿正式专辑是：AA、无与伦比的美丽 B、小宇宙 C、同名专辑 D、陪我歌唱 青峰创作的第一首歌是：DA、空气中的视听与幻觉 B、降落练习存在孪生基因 C、后悔莫及 D、窥 03年海洋音乐祭时，谁迟到了？CA、家凯 B、阿福 C、小威D、青峰 家凯、林暐哲法国街头赛跑谁赢了？AA、家凯B、林暐哲C、都赢D、无法评判 图片题（见附件）：这张是苏打绿在大陆第一场演唱会（615北京场）的新闻图片，请问方框里面的人是谁？CA、茶水 B、lfxfox C、博博鱼 D、和尚 苏打绿第一任的团长是谁？AA.小威 B.阿福 C.馨仪 D.青峰 苏打绿第一张同名专辑共有几首歌？DA、2 B、3 C、10 D、11 苏打绿一共来过上海几次？BA、1 B、2 C、3 D、4 苏打绿的第三张单曲是：CA、空气中的视听与幻觉 B、飞鱼 C、Believe in music D、迟到千年 苏打绿成军时间：BA、2000 B、2001 C、2004 D、2005 “我想到你离开了以後，我们的城市好寂寞”选自苏打绿的哪首歌曲？BA、无与伦比的美丽 B、雨中的操场 C、相信 D、城市 单曲《飞鱼》中有几首歌？DA、1 B、2 C、3 D、4 苏打绿首次来内地时青峰裤子上的油漆是怎么来的：CA、裤子本身印有的 B、被团员泼的 C、自己粉刷房间墙壁弄的D、不知道 以下苏打绿未上过的台湾综艺节目是：BA、娱乐百分百 B、康熙来了 C、国光帮帮忙 D、大学生了没 第18届金曲奖最佳作曲人奖是因为哪首歌？BA、小宇宙 B、小情歌 C、频率 D、飞鱼 红包场中，谁是王子造型的？DA、青峰 B、家凯 C、阿龚 D、馨仪 小情歌是为了哪个艺人的唱片公司收歌而写？CA、刘若英 B、江美琪 C、徐若瑄 D、杨乃文 《爱人动物》是以下哪部电影的主题曲？BA、情非得已之生存之道 B、Juno C、囧男孩 D、海角七号 「无与伦比的美丽」小巨蛋演唱会是几月几号？CA、2007.11.1 B、2007.11.2 C、2007.11.3 D、2007.11.13 第二轮 单选题： 1~5 BDB(D)CD 6~10 CCCCB 11~15DCCBC 16~2 0ABADD 多选题： 1.BDEF 2.CD 3.BCDE 4.AC 5.ACD 6.BCD 7.ABC 8.ABC 9.ACD 10.DE 第三轮 模拟题1单选题： 王菀之的首张国语创作专辑中多少首歌是由青峰作词？CA、3 B、4 C、5 D、6 615北京演唱会开场歌曲是什么？AA、无与伦比的美丽 B、小宇宙 C、小情歌 D、白日出没的月球 1224广州演唱会最后一首歌曲是：DA、频率 B、小情歌 C、这天 D、陪我歌唱 苏打绿夺得了第几届海洋音乐祭的陪审团大奖？BA、4 B、5 C、6 D、7 家凯是因为想参加什么音乐节而加入苏打绿的？BA、春浪 B、春天呐喊 C、海洋音乐祭 D、简单生活 第一期空气中的视听与幻觉广播是06年几月几号？BA、0906 B、1007 C、1017 D、1104 苏打绿的第一首抒情歌是：AA、 频率 B、无言歌 C、背着你 D、小情歌 以下哪首是政大吉他社社歌？ CA、天天想你 B、我的未来不是梦 C、我呼吸我感觉我存在 D、和天一样高 除了小巨蛋演唱会之外，哪一个影像保存了苏打绿《是我的海》的live演出？DA、好友音乐会 B、周日狂热夜 C、摇滚风城音乐祭 D、 Taiwan Roc 在小巨蛋DVD中，青峰一共talking了多少次（不包括唱歌中以及预报下一首歌）？BA、4次 B、5次 C、6次 D、7次 《Air》里的Bass是谁弹的？DA、家凯 B、馨怡 C、阿福 D、阿龚 《搜包包》中谁全程站在家凯的面前？AA、tirtir B、小威 C、青峰 D、将将 《当代歌坛》第一次出现专写苏打绿的文章是哪一期？CA、342期B、370期C、363期D、389期 创作给别人的歌中，青峰最喜欢哪首？AA、女爵B、多希望你在C、穿墙人D、爱与奇异果 苏打绿中谁一直没有拿到高中毕业证书：DA、家凯 B、阿龚 C、阿福 D、小威 在豆瓣公开现身过的苏打绿团员是：CA、家凯 B、阿龚 C、阿福 D、小威 苏打绿现在的六人编制的第一次演出是在哪里？BA、春天呐喊 B、政大金旋奖 C、西门町 D、海洋音乐祭 多选题： 下面不属于919上海演唱会安可曲目的是：DEA、相信 B、这天 C、女爵 D、是我的海》 E、白日出没的月球 “搁浅”一词出现在以下哪些歌曲中：ABCEA、吵 B、迟到千年 C、蓝眼睛 D、无与伦比的美丽 E、漂浮 青峰曾为以下哪些艺人写歌？ACDEA、左光平 B、尚雯婕 C、张韶涵 D、刘若英 E、王菀之 填空（每题5分，共5题） “各站停靠”演唱会上，作为舞台背景的钟显示的时间是22:15__ 2010年青峰和小威一起庆生时，其他团员送他们的礼物是_小时候照片拼贴出来的相框__ 阿福在自由发挥的歌曲_欢迎光临__的MV中客串演出了一个角色 各站停靠台中场的小礼物是_阿福面纸__ 苏打绿出道时候的第一支广告是__蔡康永_推荐的广播广告 模拟题2 单选（以下各题，只有一个正确选项。每题5分，共10题） 苏打绿二度踏上小巨蛋举办“日光狂热”演唱会前，媒体特地准备多样运动器材帮忙训练体力。结果，阿福在哪样器械上竟然输给了馨仪？CA.滚轮 B.握力棒 C.哑铃 D.铁饼 青峰对自己人生过程进行总结的歌是 DA.交响梦 B.融雪之前 C.相信 D.近未来 苏打绿、王菀之、方大同合作的903拉阔演唱会上，他们约定好各自代表的颜色，分别是 DA.黑、白、绿 B.黑、绿、白C.绿、黑、白 D.绿、白、黑 被媒体拍到跟青峰一起看电影，并且对镜头比中指的女友人是 AA.小兔 B.张悬 C.娃娃 D.阿纯 歌词本的字型让“吴青峰写到手心手背都是肉”的专辑是 CA. 苏打绿同名 B.春•日光 C.陪我歌唱 D.无与伦比的美丽 这是2010年5月苏打绿参加利物浦音乐节的返国记者会照片。请问青峰的这件衬衫，还在以下哪场公开表演中穿过？DA.成都热波音乐节 B.北京原创A8颁奖礼 C.西安草莓音乐节 D.西湖音乐节 在《日光》MV中，和大家一起采茶的团员是：AA.阿龚 B.小威 C.馨仪 D.家凯 来源于青峰日记的歌词是:CA.近未来 B.女爵 C.困在 D.早点回家 苏打绿一共参与过几次简单生活节:CA.1次 B.2次 C.3次 D.4次 阿福入伍前的饯别仪式上，第一个给阿福剪头发的团员是:AA.青峰 B.馨仪 C.小威 D.家凯 多选（以下各题有两个或两个以上的正确选项，多选、漏选、错选均不得分。每题5分，共5题） 在“最会睡”系列中，暐哲老师睡觉的地点有：ABCDEA.馨仪家 B.伦敦录音室 C.草地 D.公司地板 E.会议室沙发 The Wall“多希望你在”系列演出的歌单中包含：ABCEA.疼惜我的吻 B.吵 C.我在欧洲打电话给你 D.寂寞拥挤E.再见 “维瓦第计划”的歌词中，没有出现的意象是:BCA.萤火虫 B.羔羊 C.海豚 D.白鸽 E.落叶 以下歌曲中，苏打绿有公开表演多个语言版本的是ACEA.Oh Oh Oh Oh B.飞鱼 C.无眠 D.日光 E.小情歌 游乐园的官方周边包括BCDEA. .海报B.T恤 C.贴纸 D.徽章 E.年历 填空题 《小宇宙》这首歌标题的灵感来自陈黎的《小宇宙》_。 《小情歌》在被徐若瑄拒绝后，又被_江美琪_拒绝。 _2005_年9_月3_日发行首张同名专辑《苏打绿》及进行全省签唱会表演。 青峰的小学时期，每天早上跟着跳的早操音乐是 我的未来不是梦。 无美丽电台的封面除了苏打绿六个人外还有谁？ 将将 5.获奖情况 2016 第27届台湾金曲奖 最佳国语专辑奖 冬未了 （获奖） 2016 第27届台湾金曲奖 最佳乐团奖 （获奖） 2016 第27届台湾金曲奖 最佳编曲人奖 痛快的哀艳 （获奖） 2015 第五届阿比鹿音乐奖 最受欢迎唱片 冬未了 （获奖） 2014 第36届十大中文金曲 全年最高销量歌手 秋:故事 （获奖） 2013 第7届无线咪咕汇音乐盛典 年度最受欢迎组合 韦瓦第计划 （获奖） 2012 Hito流行音乐奖 hito乐团 （获奖） 2011 第1届全球流行音乐金榜 年度最佳乐团 （获奖） 2011 第1届全球流行音乐金榜 年度20大金曲 十年一刻 （获奖） 2010 第10届华语音乐传媒大奖 最佳乐队 春·日光、夏/狂热 （获奖） 2010 MusicRadio中国TOP排行榜 年度最受欢迎乐团 （获奖） 2010 MusicRadio中国TOP排行榜 年度最佳乐团 （获奖） 2010 新加坡金曲奖 最佳创作歌手 夏/狂热 （获奖） 2010 新加坡金曲奖 最佳乐团 （获奖） 2010 第21届金曲奖 最佳音乐录像带奖 日光 （获奖） 2010 第21届金曲奖 最佳编曲人 日光 （提名） 2010 第21届金曲奖 最佳乐团 春·日光 （提名） 2010 第21届金曲奖 最佳乐团 夏/狂热 （提名） 2009 第32届香港十大中文金曲颁奖典礼 全国最佳组合 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊组合/乐团 （获奖） 2009 MY Astro 至尊流行榜颁奖典礼 至尊金曲20 狂热 （获奖） 2009 第4届A8原创中国音乐盛典 年度最佳原创乐团 （获奖） 2009 MUSIC RADIO中国TOP排行榜 港台年度最佳乐团 （获奖） 2009 全球华语歌曲排行榜 地区杰出歌手奖 春·日光 （获奖） 2009 全球华语歌曲排行榜 年度20大金曲 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳编曲人 春·日光 （获奖） 2009 全球华语歌曲排行榜 最佳乐团 春·日光 （获奖） 2009 新城国语力颁奖礼 国语力亚洲乐团 （获奖） 2009 新城国语力颁奖礼 国语力歌曲 日光 （获奖） 2009 新城国语力颁奖礼 国语力乐团 （获奖） 2009 新浪网络盛典 年度最佳乐团 （获奖） 2008 第8届华语音乐传媒大奖 年度国语专辑 无与伦比的美丽 （获奖） 2008 第8届华语音乐传媒大奖 最佳编曲人 白日出没的月球 （获奖） 2008 第8届华语音乐传媒大奖 最佳乐队 无与伦比的美丽 （获奖） 2008 香港新城国语颁奖礼 新城国语歌曲 陪我歌唱 （获奖） 2008 香港新城国语颁奖礼 国语乐团 （获奖） 2008 第9届CCTV/MTV音乐盛典 港台年度最佳组合 （获奖） 2008 新城劲爆颁奖礼 新城全国乐迷投选劲爆突破表现大奖 （获奖） 2008 新加坡金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2008 第19届金曲奖 最佳音乐录影带导演奖 左边 （提名） 2008 第19届金曲奖 最佳年度歌曲 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳编曲人 无与伦比的美丽 （提名） 2008 第19届金曲奖 最佳乐团 无与伦比的美丽 （获奖） 2007 新加坡金曲奖 最佳乐团 小宇宙 （获奖） 2007 第44届金马奖 最佳原创电影歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳年度歌曲 小情歌 （提名） 2007 第18届金曲奖 最佳国语专辑 小宇宙 （提名） 2007 第18届金曲奖 最佳乐团 小宇宙 （获奖） 2006 第17届金曲奖 最佳编曲人 Oh Oh Oh Oh （提名） 2004 MTV百万乐团挑战赛 网路最佳人气乐团 （获奖） 2004 第5届海洋音乐祭 评审团大赏 （获奖） 2002 政大第19届金旋奖 乐团组冠军 空气中的视听与幻觉 （获奖） 2001 政大第18届金旋奖 乐团组最佳人气奖 （获奖） 6. 老吴写给别人的歌那英我的幸福刚刚好 林忆莲 寂寞拥挤 张惠妹 掉了，你和我的时光 李玟 想你的夜 莫文蔚 看着，老掉牙 容祖儿 在时间面前 江蕙 你讲的话 刘若英 没道理 陶晶莹 翔 蔡依林 彩色相片，栅栏间隙偷窥你，迷幻 杨丞琳 带我走，少年维特的烦恼，下个转弯是你吗，被自己绑架，一小节休息 张韶涵 最近好吗，刺情，蓝眼睛 王心凌 从未到过的地方 范玮琪 坏了良心 范晓萱 开机关机 杨乃文 女爵 许茹芸 爱人动物，飞行时光，I will be with you，最难的是相遇，现在该怎么好，我留下的一个生活 蔡健雅 极光，费洛蒙 周笔畅 别忘了 谢安琪 再见 张悬 两者 徐佳莹 乐园 魏如萱 被雨伤透，困在，开机关机 尚雯婕 什么？什么！ 吉克隽逸 我唱故我在 袁泉 等 吴映洁 一直 刘容嘉 没有人爱 潘玮仪 不同 路嘉欣 穿墙人，当我继续唱 王菀之 学会，迷湖，冬梦，是爱，爱与奇异果 旅行团 Bye Bye VOX玩声乐团 让我做你的家，朱古力 TFBOYS 小精灵 谭咏麟 超越，糖衣陷阱，蓝侬梦，魔毯，算爱，未知 张信哲 柔软 陈奕迅 放弃治疗，这样的一个麻烦，谋情害命 林俊杰 独舞，爱的鼓励，裂缝中的阳光，不存在的情人 萧敬腾 以爱之名，多希望你在 杨宗纬 想对你说 萧煌奇 下个街角 信 你存在，我记得，给自己的信 左光平 心里有鬼","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"遇见小公举","slug":"jielun","date":"2016-10-23T13:21:31.000Z","updated":"2017-11-27T13:22:09.900Z","comments":true,"path":"2016/10/23/jielun/","link":"","permalink":"http://tankcat2.com/2016/10/23/jielun/","excerpt":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。 我记不得听周杰伦的第一首歌是什么。小学六年级的时候我妈同事的女儿买了一个MP3,需要分外安电池的那种，里面有两首杰伦的歌，一首发如雪，一首夜曲。我借过来听，很喜欢这两首歌的旋律，后来搜到了歌词，就经常买花花绿绿的本子抄写歌词。到了初中，很多我很不喜欢的男生疯狂地迷恋周杰伦，可能因为女孩子成熟得早，特别反感他们自以为帅气的非主流风格（还有一个原因是很多喜欢周杰伦的人同时喜欢着许嵩）。本来因为杰伦不清楚的发音，我对他是无感的，但是因为一些烦人的粉丝，对歌手本人也没什么好感。后来到了大学，某个逗比室友对此和我惊人的相似。初中的时候我可能更多的是在追日漫，喜欢主题曲，还记得初二家里买电脑之前，星空卫视每天晚上6点放犬夜叉，我就拿着复读机录那首change the world。 到了高一，学校是明令禁止使用电子产品的，我用充饭卡的钱偷偷买了一个mp3,列了一个歌单让前桌的男生回家帮我下点歌，哪知道他全给我下的周杰伦。很幸运的是，他给我下的都是一些慢节奏情歌，最长的电影，给我一首歌的时间，甜甜的，说好的幸福呢，彩虹，七里香之类的。但也没有因为这些歌而粉上杰伦，还是听歌不看人的状态。 后来的后来，杰伦当了好声音的导师，看了一两期，发现他其实很个很可爱的人。再到现在，他的新专辑床边故事，那首前世情人，告白气球和now you see me，让我感觉，这就是从前那个酷酷的杰伦呀，那种我以前不屑的风格原来这么奇妙。于是一点没犹豫地在网易云上买了数字专辑。今天大老远从上海跑到合肥听周杰伦的演唱会，虽然位子很不好，看不到人也算了，屏幕也看不到；虽然室外的音响效果也有点让人失望，那些快歌基本听不清歌词；虽然排队很长，座位坐得很乱…但是那些我自己很熟悉的旋律响起的时候，所有的举动只剩下舞动荧光棒，跟着一起唱。让我印象很最深刻的是点歌环节，点到的第三个女孩子，杰伦问她是和谁一起来的，她说她一个人来的。当时杰伦愣了一下，然后安慰她说，全场的观众都是她的朋友，都陪着她听唱歌。当时我特别想去拥抱那个女生。一个人去看演唱会，身边都是情侣或者闺蜜团。你的注意力本该只放在爱豆身上，可无法避免的，有些场景，有些歌词就是会触动你内心那块最柔软的地方。这种经历我体验过。错过了可以有多一点杰伦的年少时光有些遗憾，但也很幸运，我开始路转粉了，未来的路，还可以相伴而行。💗💗💗 ps:当然啦，如果能遇见一个喜欢人的一起去苏打绿的after summer，那么会更加幸运~~~","text":"周杰伦是身边很多同龄人的偶像，他们应该从小学或者初中的时候就开始追他。同时期的还有蔡依林、张韶涵、林俊杰那些人，很奇怪当时我顶多是对他们的几首歌感兴趣，比如欧若拉，曹操，并没有萌生追星的概念。 我记不得听周杰伦的第一首歌是什么。小学六年级的时候我妈同事的女儿买了一个MP3,需要分外安电池的那种，里面有两首杰伦的歌，一首发如雪，一首夜曲。我借过来听，很喜欢这两首歌的旋律，后来搜到了歌词，就经常买花花绿绿的本子抄写歌词。到了初中，很多我很不喜欢的男生疯狂地迷恋周杰伦，可能因为女孩子成熟得早，特别反感他们自以为帅气的非主流风格（还有一个原因是很多喜欢周杰伦的人同时喜欢着许嵩）。本来因为杰伦不清楚的发音，我对他是无感的，但是因为一些烦人的粉丝，对歌手本人也没什么好感。后来到了大学，某个逗比室友对此和我惊人的相似。初中的时候我可能更多的是在追日漫，喜欢主题曲，还记得初二家里买电脑之前，星空卫视每天晚上6点放犬夜叉，我就拿着复读机录那首change the world。 到了高一，学校是明令禁止使用电子产品的，我用充饭卡的钱偷偷买了一个mp3,列了一个歌单让前桌的男生回家帮我下点歌，哪知道他全给我下的周杰伦。很幸运的是，他给我下的都是一些慢节奏情歌，最长的电影，给我一首歌的时间，甜甜的，说好的幸福呢，彩虹，七里香之类的。但也没有因为这些歌而粉上杰伦，还是听歌不看人的状态。 后来的后来，杰伦当了好声音的导师，看了一两期，发现他其实很个很可爱的人。再到现在，他的新专辑床边故事，那首前世情人，告白气球和now you see me，让我感觉，这就是从前那个酷酷的杰伦呀，那种我以前不屑的风格原来这么奇妙。于是一点没犹豫地在网易云上买了数字专辑。今天大老远从上海跑到合肥听周杰伦的演唱会，虽然位子很不好，看不到人也算了，屏幕也看不到；虽然室外的音响效果也有点让人失望，那些快歌基本听不清歌词；虽然排队很长，座位坐得很乱…但是那些我自己很熟悉的旋律响起的时候，所有的举动只剩下舞动荧光棒，跟着一起唱。让我印象很最深刻的是点歌环节，点到的第三个女孩子，杰伦问她是和谁一起来的，她说她一个人来的。当时杰伦愣了一下，然后安慰她说，全场的观众都是她的朋友，都陪着她听唱歌。当时我特别想去拥抱那个女生。一个人去看演唱会，身边都是情侣或者闺蜜团。你的注意力本该只放在爱豆身上，可无法避免的，有些场景，有些歌词就是会触动你内心那块最柔软的地方。这种经历我体验过。错过了可以有多一点杰伦的年少时光有些遗憾，但也很幸运，我开始路转粉了，未来的路，还可以相伴而行。💗💗💗 ps:当然啦，如果能遇见一个喜欢人的一起去苏打绿的after summer，那么会更加幸运~~~","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"周杰伦","slug":"周杰伦","permalink":"http://tankcat2.com/tags/周杰伦/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"My Favorite Band","slug":"sodagreen","date":"2016-10-17T12:02:31.000Z","updated":"2017-11-27T13:13:31.669Z","comments":true,"path":"2016/10/17/sodagreen/","link":"","permalink":"http://tankcat2.com/2016/10/17/sodagreen/","excerpt":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name. Pure, artistic, indie, free, soft and simple. All of these are my impression of Sodagreen. The band is well known for its lead vocalist and songwriter Wu Tsing-Fong, who is excellent for his poetic lyrics, unique performing style and wide vocal range. As a typical Virgo, he is a paranoia that is disproportionate to an idol. He never does what an idol should do. He is unwilling to please the fans and doesn’t like to participate in the announcement program. To be a qualified artist is very hard; to be an artist who can satisfy all the fans is harder. I still remember the live show in Spring Wave Music And Art Festival this year. Sodagreen was arranged to the final appearance and didn’t finish all the songs in that the organizer advanced the end of the show and turned off the microphone domineeringly. Wu reluctantly left in the dark, but insisted on singing the rest of the songs through Weibo. One of my favorite albums is “Summer/Fever”. Whenever I feel sad, I will listen to this album, which has inspiring power. It was released on September 11, 2009 and is their fifth full-length studio album. It is the second of the band’s Vivaldi Project, a planned series of four albums representing the four seasons respectively. The recording of this album took place in London and the songs were mostly written by the lead vocalist Wu. The album contains Britpop elements and lyrical references to the supernatural, Faust, Madame Butterfly, Don Quixote and the Greek god Dionysus. Among the songs of this album, I like “The Sound That Remains” best. In the lyrics, it draws an analogy between the sound of cicadas and the flood of public opinion, which narrows our horizon. I think the metaphor of the song is what Sodagreen has being teaching us: Don’t always mind about what other people think of you and just be free to pursue the self-value realization. The Sodagreen’s last round of road show “After Summer” before their temporarily overturn has launched. I wish I could grab a ticket for the live show in Shanghai!","text":"Sodagreen is a Taiwanese indie band formed in 2001. Its member has been unchanged since 2003: lead vocals Wu Tsing-Fong, guitarist Liu Jia-Kai, guitarist Ho Jing-Yang, keyboardist Kung Yu-Chi, bass guitarist Hsieh Shin-Yi and drummer Shih Jun-Wei. The band was originally named by Shih and Wu affixed his favorite color, green, to the name. Pure, artistic, indie, free, soft and simple. All of these are my impression of Sodagreen. The band is well known for its lead vocalist and songwriter Wu Tsing-Fong, who is excellent for his poetic lyrics, unique performing style and wide vocal range. As a typical Virgo, he is a paranoia that is disproportionate to an idol. He never does what an idol should do. He is unwilling to please the fans and doesn’t like to participate in the announcement program. To be a qualified artist is very hard; to be an artist who can satisfy all the fans is harder. I still remember the live show in Spring Wave Music And Art Festival this year. Sodagreen was arranged to the final appearance and didn’t finish all the songs in that the organizer advanced the end of the show and turned off the microphone domineeringly. Wu reluctantly left in the dark, but insisted on singing the rest of the songs through Weibo. One of my favorite albums is “Summer/Fever”. Whenever I feel sad, I will listen to this album, which has inspiring power. It was released on September 11, 2009 and is their fifth full-length studio album. It is the second of the band’s Vivaldi Project, a planned series of four albums representing the four seasons respectively. The recording of this album took place in London and the songs were mostly written by the lead vocalist Wu. The album contains Britpop elements and lyrical references to the supernatural, Faust, Madame Butterfly, Don Quixote and the Greek god Dionysus. Among the songs of this album, I like “The Sound That Remains” best. In the lyrics, it draws an analogy between the sound of cicadas and the flood of public opinion, which narrows our horizon. I think the metaphor of the song is what Sodagreen has being teaching us: Don’t always mind about what other people think of you and just be free to pursue the self-value realization. The Sodagreen’s last round of road show “After Summer” before their temporarily overturn has launched. I wish I could grab a ticket for the live show in Shanghai!","categories":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}],"tags":[{"name":"苏打绿","slug":"苏打绿","permalink":"http://tankcat2.com/tags/苏打绿/"}],"keywords":[{"name":"Jottings","slug":"Jottings","permalink":"http://tankcat2.com/categories/Jottings/"}]},{"title":"Streaming Similarity Self-Join","slug":"sssj","date":"2016-08-23T05:45:31.000Z","updated":"2017-11-27T13:15:24.916Z","comments":true,"path":"2016/08/23/sssj/","link":"","permalink":"http://tankcat2.com/2016/08/23/sssj/","excerpt":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。 Introduction 介绍在数据库和数据挖掘领域，相关性自连接处理被广泛研究，其应用场景较为广泛，包括剽窃检测、查询优化、协同过滤、重复网页的检测与去除等。若使用暴力解法，其复杂度是\\(O(n^2)\\)。在现实应用中，数据项常以高维稀疏向量的形式呈现，那么相似度的计算即为余弦相似值的计算。为了方便处理，可以将向量归一化，则进一步将问题转换为两个向量的点积。目前解决相似度自连接的算法主要依赖于基于倒排索引的删减技术以及一些数值界限。计算相似度自连接，不仅适用于静态的数据集，在数据流领域也同样适用。这里举例两个现实应用，在数据流环境下进行相似度自连接处理。 趋势监测。社交平台，譬如Twitter，趋势监测算法识别频率陡增的主题标签。更细粒度的趋势监测算法也会识别微博集合。这些集合中的数据项出现的频率增加，并且同时出现某些相同的标签。对于该趋势监测算法，在数据流环境下找出相似的微博十分重要。 近似重复项过滤。同样，在社交平台，譬如Twitter，当某个事件发生时，用户们可能会接收到与该事件相关的近似重复的多条微博。这些微博通常连续地出现。因此，将这些近似重复的微博进行过滤或者分组有利于提高用户体验。 但问题是，关于数据流环境下的相似度自连接处理的研究并不多。这是由于缺乏无限内存：不能将先到来数据项随意删除，因为该数据项可能与未来到的数据项相似。本文引入一个时间因子来解决内存问题。我们设定，只有在到达的时间间隔在指定范围内，两个数据项才有可能相似。我们定义时间依赖的相似性：基于内容的点积与时间因子的乘积，该时间因子会随着时间间隔的增加呈现指数级减小。由于时间因子的存在，当某些数据项的到达时间超出一定范围，我们可以将这些数据项删除。下图表达了这个idea。 如上图所示，标记有时间戳的文档以数据流的形式源源不断到达。时间轴上方的文档包含相似的内容，标记为红色。在所有两两相似的文档对中，我们只关心到达时间相近的文档对。因此，图中所有的4-选-2的文档对里，只有两对文档被选中(用蓝色箭头标记)。与已有的相似性自连接处理算法类似，本文的算法也依赖于索引技术。已有的算法使用不同类型的索引过滤，以减少通过索引返回的潜在配对项数量。按照这种语义，我们定义时间过滤来，与时间依赖的相似性进行关联，以便将旧的索引项从索引列表中删除。本文提出了两种算法框架来解决数据流的相似性自连接处理，均依赖于时间过滤。MB框架使用现成的索引技术，在运行过程中以流水线的方式创建两个索引，随着时间的推移丢弃旧索引。STR框架对现有的索引技术进行调整，将时间过滤的因素内嵌到其中。此外，本文结合已有的索引技术AP和L2AP，设计了一种可处理流式数据的索引算法L2,它存在以下四点优势： 有效减少潜在相似数据项的配对数量； 不需要收集数据流的统计信息； 使用轻量级的索引维持； 当数据项变“旧”时，可以迅速丢弃。","text":"Abstract 摘要在数据流环境下，系统处理的数据是源源不断流进的数据项。本文研究的问题是，数据流相似性的连接处理(SSSJ)，即在数据流中找出所有的两两数据项对，它们的相似度超过一个给定的阈值。解决这个问题最简单的构想是能拥有无限大的内存空间，但目前来讲，这是不能获得的。因此为了解决这个问题，本文提出了一个概念，时间依赖的相似性，两个数据项到达的时间间隔越大，它们的相似度越低。在此概念的基础上，本文设计两种算法框架：① 微批次处理(MiniBatch,简称MB)，使用目前已有的基于索引的过滤技术；② 流处理(Streaming,简称STR),在索引的基础上增加时间过滤，并在算法中集成一种新的基于时间的边界处理。除此之外，本文基于L2AP的索引设计一种新的适用于数据流环境的索引算法，即L2。 Introduction 介绍在数据库和数据挖掘领域，相关性自连接处理被广泛研究，其应用场景较为广泛，包括剽窃检测、查询优化、协同过滤、重复网页的检测与去除等。若使用暴力解法，其复杂度是\\(O(n^2)\\)。在现实应用中，数据项常以高维稀疏向量的形式呈现，那么相似度的计算即为余弦相似值的计算。为了方便处理，可以将向量归一化，则进一步将问题转换为两个向量的点积。目前解决相似度自连接的算法主要依赖于基于倒排索引的删减技术以及一些数值界限。计算相似度自连接，不仅适用于静态的数据集，在数据流领域也同样适用。这里举例两个现实应用，在数据流环境下进行相似度自连接处理。 趋势监测。社交平台，譬如Twitter，趋势监测算法识别频率陡增的主题标签。更细粒度的趋势监测算法也会识别微博集合。这些集合中的数据项出现的频率增加，并且同时出现某些相同的标签。对于该趋势监测算法，在数据流环境下找出相似的微博十分重要。 近似重复项过滤。同样，在社交平台，譬如Twitter，当某个事件发生时，用户们可能会接收到与该事件相关的近似重复的多条微博。这些微博通常连续地出现。因此，将这些近似重复的微博进行过滤或者分组有利于提高用户体验。 但问题是，关于数据流环境下的相似度自连接处理的研究并不多。这是由于缺乏无限内存：不能将先到来数据项随意删除，因为该数据项可能与未来到的数据项相似。本文引入一个时间因子来解决内存问题。我们设定，只有在到达的时间间隔在指定范围内，两个数据项才有可能相似。我们定义时间依赖的相似性：基于内容的点积与时间因子的乘积，该时间因子会随着时间间隔的增加呈现指数级减小。由于时间因子的存在，当某些数据项的到达时间超出一定范围，我们可以将这些数据项删除。下图表达了这个idea。 如上图所示，标记有时间戳的文档以数据流的形式源源不断到达。时间轴上方的文档包含相似的内容，标记为红色。在所有两两相似的文档对中，我们只关心到达时间相近的文档对。因此，图中所有的4-选-2的文档对里，只有两对文档被选中(用蓝色箭头标记)。与已有的相似性自连接处理算法类似，本文的算法也依赖于索引技术。已有的算法使用不同类型的索引过滤，以减少通过索引返回的潜在配对项数量。按照这种语义，我们定义时间过滤来，与时间依赖的相似性进行关联，以便将旧的索引项从索引列表中删除。本文提出了两种算法框架来解决数据流的相似性自连接处理，均依赖于时间过滤。MB框架使用现成的索引技术，在运行过程中以流水线的方式创建两个索引，随着时间的推移丢弃旧索引。STR框架对现有的索引技术进行调整，将时间过滤的因素内嵌到其中。此外，本文结合已有的索引技术AP和L2AP，设计了一种可处理流式数据的索引算法L2,它存在以下四点优势： 有效减少潜在相似数据项的配对数量； 不需要收集数据流的统计信息； 使用轻量级的索引维持； 当数据项变“旧”时，可以迅速丢弃。 Related Work 相关工作关于数据流上的相似性自连接，已有的研究很少。与之相关的主要就是相似性自连接，这个课题的研究相对广泛，由Chaudhuri等人首次在文献A primitive operator for similarity joins in data cleaning中提出，此后涌现出大量相关算法研究，与本文最为相关的是由谷歌的Bayardo提出的AP算法(Scaling up all pairs similarity search)以及Anastasiu与Karpis提出的L2AP算法(L2AP: Fast Cosine Similarity Search With Prefix \\(L_2\\) Norm Bounds)。有关这两个算法，下文会进行阐述。 Problem Statement 问题陈述我们定义数据项是m维的行向量，向量中的元素是实数值。我们定义\\(sim(x,y)\\)为计算向量\\(x\\)和向量\\(y\\)相似性的函数，并设定所有的向量均被归一化为单位向量，即\\(||x||_2=||x||=\\sqrt{\\sum_{j=1}^{m}x_j^2}=1\\)。则数据项的相似度计算可以简化为两个向量的点积，如下公式所示。$$sim(x,y)=dot(x,y)=xy^T=\\\\sum_{j=1}^{m}=x_j\\\\cdot y_j$$其中，\\(x_j\\)是向量\\(x\\)的第\\(j\\)个元素。在现实应用中，维数m通常较高，并且向量较为稀疏(向量中数值为0的元素相当多)。因此通常使用\\((j,x[j])\\)的集合来表示一个向量\\(x\\),并有\\(x[j]&gt;0,j=1…m\\)。在非数据流环境下，我们指定一个向量数据集\\(D=\\\\{x_1,…,x_n\\\\}\\)，同L2AP论文一致，我们使用\\(x^{’}=x^{’}_p = &lt; x_1,…,x_p,0,…,0 &gt; \\)来表示向量\\(x\\)的前缀，并使用\\(vm_x\\)来表示其中向量\\(x\\)所有元素的最大值，使用\\(\\sum_{x}=\\sum_{j}x_j\\)表示向量\\(x\\)所有元素之和，使用\\(|x|\\)来表示向量\\(x\\)的大小或者非零元素的个数（注意与向量的长度\\(||x||\\)之间的区别），使用\\(m_j\\)表示集合\\(D\\)中所有向量第\\(j\\)个元素的最大值，所有的\\(m_j\\)组合成向量\\(m\\)。在标准的all-pairs 相似查询问题(相似性自连接)中，给定一个向量集合和一个相似度阈值\\(\\theta\\),目标是找出所有的向量对\\((x,y)\\)满足\\(sim(x,y)\\geq\\theta\\)。在数据流环境下，每一个数据项被标记有其到达时间\\(t(x)\\)，则数据流可以表示为\\(S = &lt; …,(x_i,t(x_i)),(x_{i+1},t(x_{j+1})),… &gt; \\)。因此，我们在定义两个向量的相似度时不仅要考虑点积，还要考虑它们到达的时间之差\\(\\triangle t_{xy}=|t(x)-t(y)|\\)。则给定两个标有时间戳的向量\\(x\\)和\\(y\\),则它们的时间依赖相似度为$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}$$ 其中\\(\\lambda\\)是一个时间衰减参数。当\\(\\triangle t_{xy}=0\\)或者\\(\\lambda=0\\)时，时间依赖相似性回归到标准的相似性计算；当\\(\\triangle t_{xy}\\)趋于无穷大时，相似度为0。综上，我们可以给出SSSJ的问题定义，如下所示。 给定具有时间戳的向量流\\(S\\)，相似性阈值\\(\\theta\\)以及时间衰减因子\\(\\lambda\\),输出所有的向量对\\(x,y)\\)，满足\\(sim(x,y)\\geq\\theta\\)。 此外，根据向量的第二范式可知，\\(dot(x,y)\\leq 1\\),则有$$sim_{\\triangle t}(x,y)=dot(x,y)\\cdot e^{-\\lambda|t(x)-t(y)|}\\leq e^{-\\lambda|t(x)-t(y)|}$$ 因此，\\(\\triangle t_{xy}\\geq \\lambda ^{-1}log \\theta ^{-1}\\)表明\\(sim_{\\triangle t}(x,y)&lt;\\theta\\)，意味着对于给定的向量，不可能与在$$\\Gamma=\\lambda ^{-1}log \\theta ^{-1}$$ 个时间单位之前到达的向量相似。相应地，我们可以将“比\\(\\Gamma\\)旧”的向量删除，并称\\(\\Gamma\\)为时间期限。 Overview of the Approach 方法的高层次概述本文提出了两个算法框架，MB-IDX和STR-IDX，其中IDX是在静态数据集上解决all-pairs相似查询问题的索引技术。为了使算法框架的阐述更加清晰，我们首先回顾一下该索引技术的概述。所有的索引技术均是基于倒排索引设计的，为m个列表的集合\\(I = \\\\{I_1,I_2,…,I_m\\\\}\\)，每一个列表\\(I_j\\)由序列对\\((\\iota (x),x_j)\\)组成，其中\\(x_j\\)是向量\\(x\\)中第\\(j\\)个元素，且\\(x_j \\neq 0\\)；\\(\\iota (x)\\)是指向向量\\(x\\)的引用。所有的索引技术均是在检索相似对的同时逐个创建索引的。具体来讲，初始化时定义一个空索引，迭代地处理数据集\\(D\\)中的向量。对于每一个最新处理的向量\\(x\\)，我们从索引中检索已经存在的且与\\(x\\)相似的向量\\(y\\)，输出相似对\\((x,y)\\)。接着，将向量\\(x\\)中某些不为0的元素插入到索引中。创建索引与检索相似对的过程可以归纳为以下三个步骤： index construction(IC)：向索引列表\\(I\\)中增加新的向量； candidate generation(CG)：使用索引列表来生成潜在的相似配对项； candidate verification(CV)：计算所有潜在配对项之间的相似度，输出超过阈值的数据对。 针对这三项步骤，本文提出了三个原语操作，分为是： \\((I,P) \\leftarrow IndConstr-IDX(D,\\theta)\\)：给定向量数据集\\(D\\)，相似度阈值\\(\\theta\\)，IndConstr-IDX返回结果集\\(P=\\\\{(x,y)\\\\}\\)；同时，IndConstr-IDX创建索引列表\\(I\\)。 \\(C \\leftarrow CandGen-IDX(I,x,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)和相似度阈值\\(\\theta\\)，CandGen-IDX返回与向量\\(x\\)潜在配对的向量集合\\(C=\\\\{y\\\\}\\)。 \\(P \\leftarrow CandVer-IDX(I,x,C,\\theta)\\)：给定索引列表\\(I\\)，向量\\(x\\)，潜在向量集合\\(C\\)和相似度阈值\\(\\theta\\)，CandVer-IDX返回符合要求的相似度配对项集合\\(P=\\\\{(x,y)\\\\}\\)。 MB-IDX与STR-IDX均依赖于IDX索引，并通过增加时间过滤因子使得该索引适用于数据流环境。这两个算法框架的区别在于时间过滤因子是如何在索引中调整与设置的。MB-IDX将IDX视为黑盒，使用时间过滤因子来创建IDX的独立实例对象，并在其失效时进行丢弃；相反地，STR-IDX直接应用时间过滤因子，适时地调整索引。关于STR-IDX的处理过程会在下文进行阐述，这里先介绍MB-IDX的处理过程。MB-IDX在时间间隔\\(\\Gamma\\)内运行，在第\\(k\\)个时间间隔里，从数据流中读取向量，并将它们缓存在\\(W\\)中。在这个时间间隔的末尾，调用IndConstr-IDX来检索\\(W\\)中所有的相似对，并对\\(W\\)创建索引列表\\(I\\)。在第\\(k+1\\)个时间间隔内，重置缓存\\(W\\)，重新从数据流中读取向量。此时MB-IDX针对最新读取的向量\\(x\\)查询索引列表\\(I\\)，找出前一个时间间隔到达的并与\\(x\\)相似的向量。相似对的计算通过调用CandGen-IDX和CandVer-IDX实现。在第\\(k+1\\)个时间间隔的末尾，重置索引列表\\(I\\)，并在该时间间隔到达的所有向量之间查询出相似对。下图给出MB-IDX的伪代码。 Filtering Framework 过滤框架对于每一种索引机制，我们描述它的三个基本原语(IC,CG,CV)，并讨论如何对其进行调整以适用于数据流(见下文的STR-IDX算法)。为了使文章自成一体，接下来的每一种索引，我们先展示其在静态数据集上的操作过程，然后再阐述在MB的使用和STR框架中的调整。 Inverted Index 倒排索引最简单的是不包含“优化删减索引项”的倒排索引。在所有的索引机制中，最直观的是如果两个向量相似，则它们必须至少有一个共同的坐标。因此两个相似的向量可在某些索引列表\\(I_j\\)中存在。在IC操作中，对于每一个新读取的向量\\(x\\)，需要将所有的坐标元素插入到索引中。在CG操作中，我们只用索引列表\\(I\\)检索与向量\\(x\\)相似的潜在向量。特别地，这些潜在向量\\(y\\)所在的索引列表中必定存在向量\\(x\\)的非0坐标元素。在CV操作中，输出相似度大于阈值的。 MB framework(MB-INV)三个操作IndConstr-INV、CandGen-INV和CandVer-INV直接按照上述的过程实现，这里就不细节讨论。 STR framework(STR-INV)关于INV索引的阐述是用于解决静态数据集的相似自连接问题的，这里我们考虑流式数据的相关性自连接。我们在将向量\\(x\\)的坐标元素按照向量的到达顺序插入到索引列表中。则索引列表\\(I_j\\)按照时间戳\\(t(x)\\)对索引项\\((\\iota (x),x_j)\\)进行排序。在列表中就时间维持顺序较为容易。则我们也按照相同的顺序处理数据，每次操作时只需要将向量的坐标元素插入到列表的尾部即可。 All-pairs Indexing Scheme all-pairs索引机制Bayard在INV的基础上作出了改进，提出了AP算法，减少了索引列表的大小：无需对向量\\(x\\)中的所有坐标元素建立索引，只需要保证索引列表中存在至少一个向量\\(y\\)的坐标元素与\\(x\\)的相同。类似于INV，AP在处理一个向量时逐步建立索引列表。如以下算法InConstr-AP所示，处理向量\\(x\\)时，按照事先预先指定的顺序扫描。这里维护了一个pscore变量，用于表示向量\\(x\\)的前缀与数据集中任意一个变量相似度的上界。AP使用向量\\(m\\)，即数据集的所有维的最大坐标元素值组成的向量，来计算这个上界。只要pscore小于阈值\\(\\theta\\)，则目前扫描到的向量\\(x\\)坐标值可以不加入索引列表，并且不用担心造成相似配对的遗失。一旦pscore超过阈值，则将向量\\(x\\)的剩余坐标值全部添加进索引中，而前缀\\(x^{’}\\)则添加进未加索引集合R中。 潜在配对项的生成算法如以下CandGen-AP所示，为与向量\\(x\\)相似的向量\\(y\\)的大小（非0元素个数）指定一个下界\\(sz_1\\)。如果向量\\(y\\)的大小小于这个下界，则该向量不可能与向量\\(x\\)相似，如算法第7行所示。此外，还维护了一个变量\\(rs_1\\),用于指定向量\\(x\\)和向量\\(y\\)相似度的上界。当该上界小于阈值时，停止将新的向量加入潜在配对项映射表C中，如算法第8行所示。映射表C存储了潜在配对的向量以及已扫描的部分点积值。 最后，判别最终符合要求的相似配对项操作如算法CandVer-AP所示，使用已计算的部分相似度(C中存储的值)与没有加入索引的前缀计算得到最终的相似度。 由于数据流形式的AP算法，包括MB和STR，性能均较低，所以这里我们不深入讨论。 L2-based Indexing Scheme L2AP索引机制L2AP是由Anastasiu和Karypis两人提出的、目前最优的解决all-pairs相似性自连接的算法。该算法在AP的基础上使用了更为严密的\\(l_2-bound\\)界限，不仅可以减少索引列表的大小，还可以减少潜在配对项的数量和fully-computed similarity的数量。L2AP主要借助柯西不等式，得到\\(dot(x,y) \\leq ||x|| \\cdot ||y||\\)。这个不等式同样适用于向量的前缀。我们假设向量已归一化，即\\(||y||=1\\),则有$$dot(x^{’},y) \\leq ||x^{’}|| \\cdot ||y|| \\leq ||x^{’}||$$ 另外，当对向量\\(x\\)建立索引时，L2AP对pscore的值进行存储，以\\(\\iota (x)\\)为键将pscore保存在映射表Q中，见算法第15行；同时，L2AP也存储了向量\\(x\\)前缀的长度，如算法第16行所示的索引列表中的三元数据项。Q和\\(||x_j||\\)这两个额外信息均存储在倒排表中，在算法CendVer-L2AP中用于减少潜在的配对向量数量。 在算法CendVer-L2AP中，给定一个向量\\(x\\),我们从后往前扫描它的元素，并累计它后缀的相似值。我们定义一个变量remsore，用于维护向量\\(x\\)的剩余部分的相似值。remscore结合AP中的\\(rs_1\\)和使用存储在倒排表中的\\(||y_j^{’}||\\)的\\(rs_2\\),表示当前处理向量的前缀与倒排索引中的任意一个向量之间相似度的上界。关于L2AP算法的更多细节可以参考原论文。 Improved L2-Based Indexing Scheme 改进的L2索引机制本文在L2AP的基础上进行调整，提出了适用于数据流环境的L2算法。L2AP结合了许多不同的界限：从算法AP中继承的界限(比如IC中的\\(b_1\\)和CG中的\\(rs_1\\))，新提出的基于\\(l_2\\)范式的界限(比如IC中的\\(b_2\\)和CG中的\\(rs_2\\))。我们可以观察到基于\\(l_2\\)范式的界限比AP中的界限更为有效，在大多情况下更为严密，此结论可在Anastasiu和Karypis的论文实验部分得到验证。除此之外，AP算法在索引中使用到了静态数据的统计信息，而L2AP算法只依赖于当前被建立索引的向量。这表明在使用\\(l_2\\)范式的界限时，不需要维持向量\\(m(t)\\)，因此也不需要重新建立索引。因此L2算法只使用\\(l_2\\)范式的界限，丢弃AP中的界限。 为了阐述在数据流环境下的运行，我们需要引入额外的符号定义。首先此处的输入数据是以向量的数据流\\(S\\)，其次最重要的变动是向量\\(m\\)，它的坐标元素将随着时间的推移发生改变。以下三个操作是STR框架下的L2算法使用，主回路是IndConstr-L2-STR算法。","categories":[{"name":"Papers","slug":"Papers","permalink":"http://tankcat2.com/categories/Papers/"}],"tags":[{"name":"Stream","slug":"Stream","permalink":"http://tankcat2.com/tags/Stream/"}],"keywords":[{"name":"Papers","slug":"Papers","permalink":"http://tankcat2.com/categories/Papers/"}]},{"title":"Leiningen安装","slug":"lein","date":"2016-07-18T05:09:31.000Z","updated":"2017-11-27T13:18:36.876Z","comments":true,"path":"2016/07/18/lein/","link":"","permalink":"http://tankcat2.com/2016/07/18/lein/","excerpt":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下： Make sure you have a Java JDK version 6 or later. Download the lein script from the stable branch of this project. Place it on your $PATH. (~/bin is a good choice if it is on your path.) Set it to be executable. (chmod 755 ~/bin/lein) Run it. 这段安装教程是摘录自Leiningen，翻译如下： 安装Java JDK，确保其版本是6.0以及6.0之后的； 下载lein脚本 在/bin目录下新建一个名为lein的文件，将上述脚本拷贝进去并保存； 运行命令以下命令使得lein为可执行文件: 1$ chmod 755 ~/bin/lein 运行lein，下载leiningen-xxx-standalone.jar(xxx为版本) 1$ lein 由于网络连接的问题，下载过程中可能出现错误，比如：1It&apos;s also possible that you&apos;re behind a firewall and haven&apos;t set HTTP_PROXY and HTTPS_PROXY. 解决方案：删除掉~/.lein目录后，执行export HTTP_CLIENT=&quot;wget --no-check-certificate -O&quot;，然后重新执行lein即可。","text":"Leiningen作为Clojure的项目创建和管理工具，在Ubuntu系统下的安装过程如下： Make sure you have a Java JDK version 6 or later. Download the lein script from the stable branch of this project. Place it on your $PATH. (~/bin is a good choice if it is on your path.) Set it to be executable. (chmod 755 ~/bin/lein) Run it. 这段安装教程是摘录自Leiningen，翻译如下： 安装Java JDK，确保其版本是6.0以及6.0之后的； 下载lein脚本 在/bin目录下新建一个名为lein的文件，将上述脚本拷贝进去并保存； 运行命令以下命令使得lein为可执行文件: 1$ chmod 755 ~/bin/lein 运行lein，下载leiningen-xxx-standalone.jar(xxx为版本) 1$ lein 由于网络连接的问题，下载过程中可能出现错误，比如：1It&apos;s also possible that you&apos;re behind a firewall and haven&apos;t set HTTP_PROXY and HTTPS_PROXY. 解决方案：删除掉~/.lein目录后，执行export HTTP_CLIENT=&quot;wget --no-check-certificate -O&quot;，然后重新执行lein即可。","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Leiningen","slug":"Leiningen","permalink":"http://tankcat2.com/tags/Leiningen/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]},{"title":"Storm组件和拓扑结构","slug":"storm component","date":"2016-07-16T02:06:31.000Z","updated":"2017-11-27T13:15:08.248Z","comments":true,"path":"2016/07/16/storm component/","link":"","permalink":"http://tankcat2.com/2016/07/16/storm component/","excerpt":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。 Storm总体结构 Storm的术语比较多，本文涉及的有Spout，Bolt，Stream，Tuple，Stream Grouping，Topology。下面简单介绍一下。 Tuple：包含了一个或者多个键值对的列表。默认情况下，元组tuple中的域可以是整型(integer)等基本类型对象。也可以通过定义可序列化(实现Serializable接口)的对象来实现自定义的元组类型。 Stream：Storm中最核心的概念，在分布式环境下并行创建、处理的元祖Tuple序列。在声明数据流的时候要给定一个有效的id，若不显示指定，则系统默认会给数据流定义一个名为”default”的id。 Spout：消息生产者，是数据流的数据来源，充当一个采集器的角色，连接到外部的数据源，并将数据转化为一个个tuple。 Bolt：消息处理者，封装了数据处理逻辑，将一个或者多个数据流作为输入，对数据实施运算后，选择性地输出一个或者多个数据流。几种典型的功能有数据过滤、连接、聚合和数据库读写等。 Stream Grouping：定义了消息分发策略，即定义了一个数据流中的tuple如何分发给topology中的不同bolt的任务实例。 Topology：用于封装一个实时计算应用程序的逻辑，类似于Hadoop中的Job。由Stream Grouping连接起来的Spout和Bolt的有向无环图，处理的是源源不断的消息流。 Storm编程基础 Storm的源码共分为三层，分为如下： Storm最上层的所有接口均是用Java定义的。 上层绝大多数接口的逻辑是用Clojure实现的。 最底层的数据结构是用Thrift定义的。Thrift是Apache下面的跨语言框架，它可以基于Thrift定义文件产生不同语言的代码，有关详细内容可以参考thrift。 本文主要就上层Java接口介绍Storm的拓扑和组件结构。 拓扑Topology 首先，从最外层开始，举一个如何创建topology的例子。代码中的对象下面会一一介绍。123456789TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"spout1\", new Spout1(), 1);builder.setSpout(\"spout2\", new Spout2(), 5);builder.setBolt(\"bolt\", new Bolt1(), 3) .directGrouping(\"spout1\", \"stream1\") .shuffleGrouping(\"spout2\");Config conf = new Config();conf.setDebug(true);StormSubmitter.submitTopology(\"my-topology\", conf, builder.createTopology()); TopologyBuilder第一行代码定义了一个TopologyBuilder对象。TopologyBuilder是一个工具类，用于构造topology。Topology最底层实际上是Thrift的一个数据结构，分为一下2个部分： StormTopology定义了Topology的组成，包括Spout和Bolt，每个Spout或者Bolt都有全局唯一的id。目前为止，StateSpoutSpec还没有在设计代码中用到。 12345struct StormTopology&#123; 1: required map&lt;string, SpoutSpec&gt; spouts; 2: required map&lt;string, Bolt&gt; bolts; 3: required map&lt;string, StateSpoutSpec&gt; state_spouts;&#125; TopologySummary定义了用户提交的Topology的基本情况，例如该topology分布在几个工作进程上，使用了多少个线程，有多少个任务实例。这些数据主要供Nimbus使用，以返回UI请求的数据。 123456789struct TopologySummary&#123; 1: required string id; 2: required string name; 3: required i32 num_tasks; 4: required i32 num_executors; 5: required i32 num_workers; 6: required i32 uptime_secs; 7: required string status;&#125; 由于Topology在Thrift中过于描述化的特性不便于直接使用，所以TopologyBuilder进行了上层的封装，提供了更加方便的构建方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public class TopologyBuilder &#123; private Map&lt;String, IRichBolt&gt; _bolts = new HashMap&lt;&gt;(); private Map&lt;String, IRichSpout&gt; _spouts = new HashMap&lt;&gt;(); private Map&lt;String, ComponentCommon&gt; _commons = new HashMap&lt;&gt;(); private Map&lt;String, StateSpoutSpec&gt; _stateSpouts = new HashMap&lt;&gt;(); ... public StormTopology createTopology() &#123; Map&lt;String, Bolt&gt; boltSpecs = new HashMap&lt;&gt;(); Map&lt;String, SpoutSpec&gt; spoutSpecs = new HashMap&lt;&gt;(); maybeAddCheckpointSpout(); for(String boltId: _bolts.keySet()) &#123; IRichBolt bolt = _bolts.get(boltId); bolt = maybeAddCheckpointTupleForwarder(bolt); ComponentCommon common = getComponentCommon(boltId, bolt); boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common)); &#125; for(String spoutId: _spouts.keySet()) &#123; IRichSpout spout = _spouts.get(spoutId); ComponentCommon common = getComponentCommon(spoutId, spout); spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common)); &#125; StormTopology stormTopology = new StormTopology(spoutSpecs, boltSpecs,new HashMap&lt;String, StateSpoutSpec&gt;()); stormTopology.set_worker_hooks(_workerHooks); return stormTopology; &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt, Number parallelism_hint)&#123; validateUnusedId(id); initCommon(id, bolt, parallelism_hint); _bolts.put(id, bolt); return new BoltGetter(id); &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125; ... public SpoutDeclarer setSpout(String id, IRichSpout spout, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, spout, parallelism_hint); _spouts.put(id, spout); return new SpoutGetter(id); &#125; public SpoutDeclarer setSpout(String id, IRichSpout spout)&#123; return setSpout(id, spout, null); &#125; ... private void validateUnusedId(String id) &#123; if(_bolts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Bolt has already been declared for id \" + id); &#125; if(_spouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Spout has already been declared for id \" + id); &#125; if(_stateSpouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"State spout has already been declared for id \" + id); &#125; &#125; private ComponentCommon getComponentCommon(String id, IComponent component) &#123; ComponentCommon ret = new ComponentCommon(_commons.get(id)); OutputFieldsGetter getter = new OutputFieldsGetter(); component.declareOutputFields(getter); ret.set_streams(getter.getFieldsDeclaration()); return ret; &#125; private void initCommon(String id, IComponent component, Number parallelism) throws IllegalArgumentException &#123; ComponentCommon common = new ComponentCommon(); common.set_inputs(new HashMap&lt;GlobalStreamId, Grouping&gt;()); if(parallelism!=null) &#123; int dop = parallelism.intValue(); if(dop &lt; 1) &#123; throw new IllegalArgumentException(\"Parallelism must be positive.\"); &#125; common.set_parallelism_hint(dop); &#125; Map conf = component.getComponentConfiguration(); if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf)); _commons.put(id, common); &#125; ... 成员变量_bolts包含了所有的Bolt对象，_spouts包含了所有的spout对象，它们的类型分别是IRichBolt和IRichSpout类型，关于组件的接口下文会慢慢介绍。 spout的thrift底层结构为SpoutSpec，包含了两个成员，一个是实现消息生产者具体逻辑的spout_object对象，另一个是用来描述其输入输出的common对象，具体定义如下： 1234struct SpoutSpec&#123; 1: required ComponentObject spout_object; 2: required ComponentCommon common;&#125; bolt的thrift底层结构为Bolt，结构与SpoutSpec是一致的，这里不再阐述。 _commons包含了所有的Bolt和Spout对象，它在Thrift中的数据结构是ComponentCommon: 123456struct ComponentCommon&#123; 1: required map&lt;GlobalStreamId, Grouping&gt; inputs; 2: required map&lt;string, StreamInfo&gt; streams; 3: optimal i32 parallelism_hint; 4: optimal string json_conf;&#125; ComponentCommon是用来表示Topology的基础组件对象,inputs表示该组件将从哪些GlobalStreamId以何种方式接收数据，其中GlobalStreamId是某个组件上定义的一条流，数据结构如下： 1234struct GlobalStreamId&#123; 1: required string componentId; 2: required string streamId;&#125; GlobalStreamId有两个域，componentId表示该流属于哪一个组件，streamId是流的标识。不同组件之间可以使用相同的streamId。分组方式Grouping决定了组件所发送的消息将以何种方式发送到接收端,Grouping被定义为union类型，即表示节点之间只能采取一种分组方式，其结构定义如下： 12345678910union Grouping&#123; 1: list&lt;string&gt; fields //若为空，则采取global grouping的分组方式; 2: NullStruct shuffle; //随机分组 3: NullStruct all; //全分组，即广播 4: NullStruct none; //无分组，全部发送到同一个任务实例上 5: NullStruct direct; //直接分组，直接发送到指定的任务实例上 6: JavaObject custom_object; 7: binary custom_serialized; 8: NullStruct local_or_shuffle;&#125; ComponentCommon中的streams指定了该组件需要输出的流，它给定了streamId以及StreamInfo。StreamInfo中定义了输出流的字段名列表以及该流的消息分组是否是直接分组方式，其结构定义如下： 1234struct StreamInfo&#123; 1: required list&lt;string&gt; output_fields; 2: required bool direct;&#125; ComponentCommon中的parallelism_hint表示组件的并行度，即有多少个线程，这些线程可分布在不同的进程空间或者机器中，默认值是１。 ComponentCommon中的json_conf保存了与该组件相关的一些设置。 第29~46行定义了setBolt()方法以及其重载。该方法主要功能是定义topology中的bolt对象，并指定其并行度。该方法在运行的过程中首先会通过方法validateUnusedId()检测输入的组件ID是否是唯一的，其次调用initCommon()方法为该组件构建一个ComponentCommon对象，并且只初始化其并行度和配置信息，配置信息被序列化成JSON的形式。setBolt()最后会返回一个BoltGetter对象，将利用其为bolt对象添加输入流信息。关于这个对象后文再具体介绍。 第50~59行定义了setSpout()方法以及其重载。该方法类似于setBolt()方法，也会产生ComponentCommon对象。 第63~73行定义了validateUnusedId()方法，用于检测输入的组件Id是否是唯一的，若不是则抛出异常。 第75~81行定义了getComponentCommon()方法，该方法是在createTopology()创建拓扑的时候被调用的，设置了组件的输出流信息。 第83~96行定义了initCommon()方法，主要是对ComponentCommon对象进行初始化，设置并行度和配置信息。 最后一个也是最为重要的，第8~27行定义了createTopology()方法，根据输入的Bolt和Spout对象创建Topology对象。从第16行和第21行可以看出，在创建拓扑的过程中，Bolt和Spout均为对象序列化后得到的字节数组。","text":"Storm简介 Storm是Twitter开源的一个类似Hadoop的实时数据处理框架，它原来是由BackType开发，后被Twitter收购，将Storm作为Twitter的实时数据分析系统。 Storm总体结构 Storm的术语比较多，本文涉及的有Spout，Bolt，Stream，Tuple，Stream Grouping，Topology。下面简单介绍一下。 Tuple：包含了一个或者多个键值对的列表。默认情况下，元组tuple中的域可以是整型(integer)等基本类型对象。也可以通过定义可序列化(实现Serializable接口)的对象来实现自定义的元组类型。 Stream：Storm中最核心的概念，在分布式环境下并行创建、处理的元祖Tuple序列。在声明数据流的时候要给定一个有效的id，若不显示指定，则系统默认会给数据流定义一个名为”default”的id。 Spout：消息生产者，是数据流的数据来源，充当一个采集器的角色，连接到外部的数据源，并将数据转化为一个个tuple。 Bolt：消息处理者，封装了数据处理逻辑，将一个或者多个数据流作为输入，对数据实施运算后，选择性地输出一个或者多个数据流。几种典型的功能有数据过滤、连接、聚合和数据库读写等。 Stream Grouping：定义了消息分发策略，即定义了一个数据流中的tuple如何分发给topology中的不同bolt的任务实例。 Topology：用于封装一个实时计算应用程序的逻辑，类似于Hadoop中的Job。由Stream Grouping连接起来的Spout和Bolt的有向无环图，处理的是源源不断的消息流。 Storm编程基础 Storm的源码共分为三层，分为如下： Storm最上层的所有接口均是用Java定义的。 上层绝大多数接口的逻辑是用Clojure实现的。 最底层的数据结构是用Thrift定义的。Thrift是Apache下面的跨语言框架，它可以基于Thrift定义文件产生不同语言的代码，有关详细内容可以参考thrift。 本文主要就上层Java接口介绍Storm的拓扑和组件结构。 拓扑Topology 首先，从最外层开始，举一个如何创建topology的例子。代码中的对象下面会一一介绍。123456789TopologyBuilder builder = new TopologyBuilder();builder.setSpout(\"spout1\", new Spout1(), 1);builder.setSpout(\"spout2\", new Spout2(), 5);builder.setBolt(\"bolt\", new Bolt1(), 3) .directGrouping(\"spout1\", \"stream1\") .shuffleGrouping(\"spout2\");Config conf = new Config();conf.setDebug(true);StormSubmitter.submitTopology(\"my-topology\", conf, builder.createTopology()); TopologyBuilder第一行代码定义了一个TopologyBuilder对象。TopologyBuilder是一个工具类，用于构造topology。Topology最底层实际上是Thrift的一个数据结构，分为一下2个部分： StormTopology定义了Topology的组成，包括Spout和Bolt，每个Spout或者Bolt都有全局唯一的id。目前为止，StateSpoutSpec还没有在设计代码中用到。 12345struct StormTopology&#123; 1: required map&lt;string, SpoutSpec&gt; spouts; 2: required map&lt;string, Bolt&gt; bolts; 3: required map&lt;string, StateSpoutSpec&gt; state_spouts;&#125; TopologySummary定义了用户提交的Topology的基本情况，例如该topology分布在几个工作进程上，使用了多少个线程，有多少个任务实例。这些数据主要供Nimbus使用，以返回UI请求的数据。 123456789struct TopologySummary&#123; 1: required string id; 2: required string name; 3: required i32 num_tasks; 4: required i32 num_executors; 5: required i32 num_workers; 6: required i32 uptime_secs; 7: required string status;&#125; 由于Topology在Thrift中过于描述化的特性不便于直接使用，所以TopologyBuilder进行了上层的封装，提供了更加方便的构建方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public class TopologyBuilder &#123; private Map&lt;String, IRichBolt&gt; _bolts = new HashMap&lt;&gt;(); private Map&lt;String, IRichSpout&gt; _spouts = new HashMap&lt;&gt;(); private Map&lt;String, ComponentCommon&gt; _commons = new HashMap&lt;&gt;(); private Map&lt;String, StateSpoutSpec&gt; _stateSpouts = new HashMap&lt;&gt;(); ... public StormTopology createTopology() &#123; Map&lt;String, Bolt&gt; boltSpecs = new HashMap&lt;&gt;(); Map&lt;String, SpoutSpec&gt; spoutSpecs = new HashMap&lt;&gt;(); maybeAddCheckpointSpout(); for(String boltId: _bolts.keySet()) &#123; IRichBolt bolt = _bolts.get(boltId); bolt = maybeAddCheckpointTupleForwarder(bolt); ComponentCommon common = getComponentCommon(boltId, bolt); boltSpecs.put(boltId, new Bolt(ComponentObject.serialized_java(Utils.javaSerialize(bolt)), common)); &#125; for(String spoutId: _spouts.keySet()) &#123; IRichSpout spout = _spouts.get(spoutId); ComponentCommon common = getComponentCommon(spoutId, spout); spoutSpecs.put(spoutId, new SpoutSpec(ComponentObject.serialized_java(Utils.javaSerialize(spout)), common)); &#125; StormTopology stormTopology = new StormTopology(spoutSpecs, boltSpecs,new HashMap&lt;String, StateSpoutSpec&gt;()); stormTopology.set_worker_hooks(_workerHooks); return stormTopology; &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt, Number parallelism_hint)&#123; validateUnusedId(id); initCommon(id, bolt, parallelism_hint); _bolts.put(id, bolt); return new BoltGetter(id); &#125; public BoltDeclarer setBolt(String id, IRichBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt)&#123; return setBolt(id, bolt, null); &#125; public BoltDeclarer setBolt(String id, IBasicBolt bolt, Number parallelism_hint) &#123; return setBolt(id, new BasicBoltExecutor(bolt), parallelism_hint); &#125; ... public SpoutDeclarer setSpout(String id, IRichSpout spout, Number parallelism_hint) &#123; validateUnusedId(id); initCommon(id, spout, parallelism_hint); _spouts.put(id, spout); return new SpoutGetter(id); &#125; public SpoutDeclarer setSpout(String id, IRichSpout spout)&#123; return setSpout(id, spout, null); &#125; ... private void validateUnusedId(String id) &#123; if(_bolts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Bolt has already been declared for id \" + id); &#125; if(_spouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"Spout has already been declared for id \" + id); &#125; if(_stateSpouts.containsKey(id)) &#123; throw new IllegalArgumentException(\"State spout has already been declared for id \" + id); &#125; &#125; private ComponentCommon getComponentCommon(String id, IComponent component) &#123; ComponentCommon ret = new ComponentCommon(_commons.get(id)); OutputFieldsGetter getter = new OutputFieldsGetter(); component.declareOutputFields(getter); ret.set_streams(getter.getFieldsDeclaration()); return ret; &#125; private void initCommon(String id, IComponent component, Number parallelism) throws IllegalArgumentException &#123; ComponentCommon common = new ComponentCommon(); common.set_inputs(new HashMap&lt;GlobalStreamId, Grouping&gt;()); if(parallelism!=null) &#123; int dop = parallelism.intValue(); if(dop &lt; 1) &#123; throw new IllegalArgumentException(\"Parallelism must be positive.\"); &#125; common.set_parallelism_hint(dop); &#125; Map conf = component.getComponentConfiguration(); if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf)); _commons.put(id, common); &#125; ... 成员变量_bolts包含了所有的Bolt对象，_spouts包含了所有的spout对象，它们的类型分别是IRichBolt和IRichSpout类型，关于组件的接口下文会慢慢介绍。 spout的thrift底层结构为SpoutSpec，包含了两个成员，一个是实现消息生产者具体逻辑的spout_object对象，另一个是用来描述其输入输出的common对象，具体定义如下： 1234struct SpoutSpec&#123; 1: required ComponentObject spout_object; 2: required ComponentCommon common;&#125; bolt的thrift底层结构为Bolt，结构与SpoutSpec是一致的，这里不再阐述。 _commons包含了所有的Bolt和Spout对象，它在Thrift中的数据结构是ComponentCommon: 123456struct ComponentCommon&#123; 1: required map&lt;GlobalStreamId, Grouping&gt; inputs; 2: required map&lt;string, StreamInfo&gt; streams; 3: optimal i32 parallelism_hint; 4: optimal string json_conf;&#125; ComponentCommon是用来表示Topology的基础组件对象,inputs表示该组件将从哪些GlobalStreamId以何种方式接收数据，其中GlobalStreamId是某个组件上定义的一条流，数据结构如下： 1234struct GlobalStreamId&#123; 1: required string componentId; 2: required string streamId;&#125; GlobalStreamId有两个域，componentId表示该流属于哪一个组件，streamId是流的标识。不同组件之间可以使用相同的streamId。分组方式Grouping决定了组件所发送的消息将以何种方式发送到接收端,Grouping被定义为union类型，即表示节点之间只能采取一种分组方式，其结构定义如下： 12345678910union Grouping&#123; 1: list&lt;string&gt; fields //若为空，则采取global grouping的分组方式; 2: NullStruct shuffle; //随机分组 3: NullStruct all; //全分组，即广播 4: NullStruct none; //无分组，全部发送到同一个任务实例上 5: NullStruct direct; //直接分组，直接发送到指定的任务实例上 6: JavaObject custom_object; 7: binary custom_serialized; 8: NullStruct local_or_shuffle;&#125; ComponentCommon中的streams指定了该组件需要输出的流，它给定了streamId以及StreamInfo。StreamInfo中定义了输出流的字段名列表以及该流的消息分组是否是直接分组方式，其结构定义如下： 1234struct StreamInfo&#123; 1: required list&lt;string&gt; output_fields; 2: required bool direct;&#125; ComponentCommon中的parallelism_hint表示组件的并行度，即有多少个线程，这些线程可分布在不同的进程空间或者机器中，默认值是１。 ComponentCommon中的json_conf保存了与该组件相关的一些设置。 第29~46行定义了setBolt()方法以及其重载。该方法主要功能是定义topology中的bolt对象，并指定其并行度。该方法在运行的过程中首先会通过方法validateUnusedId()检测输入的组件ID是否是唯一的，其次调用initCommon()方法为该组件构建一个ComponentCommon对象，并且只初始化其并行度和配置信息，配置信息被序列化成JSON的形式。setBolt()最后会返回一个BoltGetter对象，将利用其为bolt对象添加输入流信息。关于这个对象后文再具体介绍。 第50~59行定义了setSpout()方法以及其重载。该方法类似于setBolt()方法，也会产生ComponentCommon对象。 第63~73行定义了validateUnusedId()方法，用于检测输入的组件Id是否是唯一的，若不是则抛出异常。 第75~81行定义了getComponentCommon()方法，该方法是在createTopology()创建拓扑的时候被调用的，设置了组件的输出流信息。 第83~96行定义了initCommon()方法，主要是对ComponentCommon对象进行初始化，设置并行度和配置信息。 最后一个也是最为重要的，第8~27行定义了createTopology()方法，根据输入的Bolt和Spout对象创建Topology对象。从第16行和第21行可以看出，在创建拓扑的过程中，Bolt和Spout均为对象序列化后得到的字节数组。 组件ComponentsBolt接口再回头看创建topology的简单示例，第4行设置bolt的方法中构建了一个类Bolt1的对象实例。该类是一个自定义的Bolt类，可以是实现了Storm定义的Bolt接口，主要有IBolt，IRichBolt，IBasicBolt和IBatchBolt，它们之间的关系如下图所示： IComponent接口IComponent是通用的组件接口，所有的Bolt和Spout都会实现这个接口，其代码如下：1234public interface IComponent extends Serializable&#123; void declareOutputFields(OutputFieldsDeclarer declarer); Map&lt;String, Object&gt; getComponentConfiguration();&#125; 其中，declareOutputFields()的参数为OutputFieldsDeclarer接口，定义了拓扑中每个组件的输出字段声明，每个组件都需要它来指定输出到哪些流、声明输出的字段列表以及指出输出流是否是直接流，代码如下：123456public interface OutputFieldsDeclarer&#123; public void delcare(Fields fields); public void delcare(boolean direct, Fields fields); public void delcareStream(String streamId,Fields fields); public void declareStream(String streamId,boolean direct,Fields fields);&#125; 其中，第5~6行没有明确指定输出流的id，默认使用的是storm的default流。所有的方法中都有一个Fields对象参数，该类用于存储消息的字段名列表，比如一条学生个人信息的scheme为(“stu_name”,”stu_number”,”stu_age”,”stu_sex”)，其所需参数是字段名集合。对于同一条消息，在构建Fields对象时会为其所有的字段建立索引。它的代码定义如下：1234567891011121314151617181920212223242526public class Fields implements Iterable&lt;String&gt;,Serializable&#123; private List&lt;String&gt; _fields; private Map&lt;String,Integer&gt; _index = new HashMap&lt;String,Integer&gt;(); public Fields(String... fields)&#123; this(Arrays.asList(fields)); &#125; public Fields(List&lt;String&gt; fields)&#123; _fields= new ArrayList&lt;String&gt;(fields.size()); for(String field:fields)&#123; if(_fields.contains(field))&#123; throw new IllegalArgumentException(String.format(\"duplicate field '%s'\", field)); &#125; _fields.add(field); &#125; index(); &#125; private void index()&#123; for(int i=0;i&lt;_fields.size();i++)&#123; _index.put(_fields.get(i),i); &#125; &#125; ...&#125; Fields实现了Iterable&lt;String&gt;接口，表明Fields可以遍历存储的字段名列表；也实现了Serializable接口，表明Fields可以被序列化。第2~3行定义了一个保存所有字段名的列表以及一个保存了从字段名到它在字段名列表中位置的映射表。第5~7行的构造函数接收一个可变参数fields，将其转换为列表好调用第9~19行定义的构造函数。第9~19行定义的构造函数首先会检查传入的字段名列表中是否存在重复的字段名，并保存该字段名列表，最后调用index()方法为该字段名列表建立索引。 IBolt接口Bolt是Storm中的基础运行单位，当接收到一条数据时可以不立刻对其进行处理，可以先保存后处理。其生命周期如下： 创建提交Topology时创建IBolt实例并进行序列化操作(见createTopology())； 将序列化的Bolt组件发送给集群中的主节点； 主节点启动工作进程，并在进程中反序列化Bolt组件； 在开始执行任务之前，先调用Bolt的prepare()回调方法进行初始化，然后再具体处理接收的数据。 IBolt的具体代码如下：12345public interface IBolt extends Serializable&#123; void prepare(Map stormConf, TopologyContext context, OutputCollector collector); void execute(Tuple input); void cleanup();&#125; 在实现Bolt的过程中，用户可以编写其构造函数，然而构造函数并不会被实际调用，因为在提交Topology时，系统会调用Topology的构造函数，并将产生的对象序列化成字节数组。每一个节点上的Bolt都是通过反序列化的方式得到的，这可能导致某些成员没有被正确的初始化，因此用户应该将复杂对象的初始化放在prepare()回调方法中。第三个参数是一个OutputCollector类对象，它是Bolt的输出收集器，Bolt处理好的消息都是通过输出收集器发送出去的，不同类型的输出收集器也不同，这里先讲一下IRichBolt的输出收集器，它实现了IOutputCollector接口，是一个代理类。IOutputCollector接口如下：。emit()方法用来向外发送数据，它的返回值是该消息所发送目标的TaskId集合，其输入参数分布是消息将被输出的流Id，输出的消息标记(通常代表该条消息从哪些消息产生的)以及要输出的消息。emitDirect()与emit()类似，主要区别在于它发送的消息只有指定的任务实例才能接收。这个方法要求streamId对应的流必须被定义为直接流。如果下游节点没有接收到该消息，那么此类消息其实并没有真正发送。fail()和ack()用来表示消息是否被真正处理。OutputCollector是IOutputCollector的默认实现类，它实际上是一个代理，包含一个真正工作的IOutputCollector实例，这个对象是在Clojure中定义的。OutputCollector中提供了许多重载方法供用户使用，具体定义可参照OutputCollector。 123456public interface IOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,Collection&lt;Tuple&gt; anchors,List&lt;Object&gt; tuple); void ack(Tuple input); void fail(Tuple input);&#125; 对象在被销毁时，将调用cleanup()回调方法，但是Storm并不保证该方法一定被执行，只有当在本地模式下运行时杀死topology该方法才保证一定能被执行。 execute()方法实现对输入消息的处理。其参数是一个Tuple实例。Tuple是Storm中的主要数据结构，在发送接收消息的过程中，每一条消息实际上都是一个Tuple对象。其接口代码如下：第5~14行的方法用于获取由参数i指定的字段位置的值，如果用户知道该字段对应的类型，就可以调用对应类型的获取方法获取字段的值。第15~24行与之类似，不过这里的方法是根据字段名获取相应的值。123456789101112131415161718192021222324252627282930313233public interface ITuple &#123; public int size(); public int fieldIndex(String field); public boolean contains(String field); public Object getValue(int i); public String getString(int i); public Integer getInteger(int i); public Long getLong(int i); public Boolean getBoolean(int i); public Short getShort(int i); public Byte getByte(int i); public Double getDouble(int i); public Float getFloat(int i); public byte[] getBinary(int i); public Object getValueByField(String field); public String getStringByField(String field); public Integer getIntegerByField(String field); public Long getLongByField(String field); public Boolean getBooleanByField(String field); public Short getShortByField(String field); public Byte getByteByField(String field); public Double getDoubleByField(String field); public Float getFloatByField(String field); public byte[] getBinaryByField(String field); public Fields getFields(); public List&lt;Object&gt; select(Fields selector); public List&lt;Object&gt; getValues(); public GlobalStreamId getSourceGlobalStreamId(); public String getSourceComponent(); public int getSourceTask(); public String getSourceStreamId(); public MessageId getMessageId();&#125; IRichBolt接口IRichBolt同时实现了IComponent和IBolt接口，其含义是一个具有Bolt功能的组件。在实际使用中，IRichBolt是实现Topology组件的主要接口，其定义如下：123public interface IRichBolt extends IBolt,IComponent&#123; &#125; IBasicBolt接口IBasicBolt接口的定义与IBolt基本一致，具体实现要求也与IBolt相同，主要区别为一下两点： 它的输出收集器使用的是BasicOutputCollector，并且该参数被放在了execute方法中而不是prepare中； 它实现了IComponent接口，表明它可以用来定义Topology组件。首先我们来看一下BasicOutputCollector,它是Storm提供的IBasicOutputCollector接口的默认实现。我们看一下该接口的定义。12345public interface IBasicOutputCollector&#123; List&lt;Integer&gt; emit(String streamId,List&lt;Object&gt; tuple); void emitDirect(int taskId,String streamId,List&lt;Object&gt; tuple); void reportError(Throwable t);&#125; 对比IOutputCollector可以看出两者的区别： IBasicOutputCollector中没有ack和fail方法； IBasicOutputCollector的emit和emitDirect方法中没有anchor参数。这样设计的原因是如果使用了IBasicBolt，Storm框架会自动帮用户进行Ack、Fail和Anchor操作，用户自己不需要关心这一点。所以为了确保这种机制能正常运行，避免用户在使用时出错，Storm提供了简化版的IBasicOutputCollector。BasicOutputCollector收集器实际上是OutputCollector的封装类，其中包含了一个OutputCollector类型的成员变量，实际上所有的消息最终都将由这个OutputCollector进行处理。 再回到IBasicBolt，之所以设计这个接口上文已有解释，Storm框架本身帮用户处理了所发出消息的Ack、Fail和Anchor操作，是由执行器BasicBoltExecutor实现的。该类实现了IRichBolt接口，同时还包含了一个IBasicBolt成员变量用于调用的转发，它是基于装饰模式的，定义如下：12345678910111213141516171819202122232425262728293031323334public class BasicBoltExecutor implements IRichBolt &#123; public static final Logger LOG = LoggerFactory.getLogger(BasicBoltExecutor.class); private IBasicBolt _bolt; private transient BasicOutputCollector _collector; public BasicBoltExecutor(IBasicBolt bolt) &#123; _bolt = bolt; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; _bolt.declareOutputFields(declarer); &#125; public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; _bolt.prepare(stormConf, context); _collector = new BasicOutputCollector(collector); &#125; public void execute(Tuple input) &#123; _collector.setContext(input); try &#123; _bolt.execute(input, _collector); _collector.getOutputter().ack(input); &#125; catch(FailedException e) &#123; if(e instanceof ReportedFailedException) &#123; _collector.reportError(e); &#125; _collector.getOutputter().fail(input); &#125; &#125; public void cleanup() &#123; _bolt.cleanup(); &#125; public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return _bolt.getComponentConfiguration(); &#125;&#125; 第3行定义了成员变量_bolt，实现了IBasicBolt接口；第17行设置执行器运行的上下文，它表示经execute方法发送出去的消息都是由输出消息产生的，即输出消息都将标记为输入消息所衍生出来的消息，这是使用IBasicBolt消息跟踪的重要环节。第20行对输入的消息进行Ack操作。这一步意味着基于当前输入消息的处理以及衍生消息的发送已经完成，此时可以对该消息进行Ack操作了。用户实现了IBasicBolt接口的Bolt对象之后，在构建Topology时，Storm会调用TopologyBuilder的setBolt方法设置该Bolt对象。setBolt方法会用BasicBoltExecutor封装用户的实现类，这是Storm自动帮用户完成的，而且它还会调用可接收IRichBolt参数的重载方法完成Bolt对象的设置。这也解释了BasicBoltExecutor需要实现IRichBolt接口的原因。 IBatchBolt接口区别与IBasicBolt接口，IBatchBolt主要用于Storm中的批处理。Storm的事务Topology以及Trident主要是基于IBatchBolt的。相比前面的IBolt、IRichBolt、IBasicBolt，IBatchBolt中多了一个finishBatch()方法，它在一个批处理结束时被调用。此外，IBatchBolt还去除了cleanup()方法，其接口定义如下：12345public interface IBatchBolt&lt;T&gt; extends Serializable,IComponent&#123; void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, T id); void execute(Tuple input); void finishBatch();&#125; prepare()方法用来初始化一个Batch，最后一个参数是通用类型T，它可以用作该Batch的唯一标识。在目前的Storm实现中，每个事务都会对应一个Batch，而每个Batch的数据都会由一个新创建的IBatchBolt对象进行处理，当一个Batch被成功处理后，该Batch对应的IBatchBolt对象将被销毁，因此用户不能通过IBatchBolt对象自身存储需要在多个Batch之间进行共享的数据。第二个参数是IBatchBolt的输出收集器BatchOutputCollector,其代码定义如下：123456789public abstract class BatchOutputCollector&#123; public List&lt;Integer&gt; emit(List&lt;Object&gt; tuple); public List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple); public void emitDirect(int taskId, List&lt;Object&gt; tuple)&#123; emitDirect(taskId, Utils.DEFAULT_STREAM_ID,tuple); &#125; public abstract void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple); public abstract void reportError(Throwable error);&#125; 同IBasicOutputCollector类似，不需要自己去处理Ack、Fail和Anchor这3项操作。Storm提供了BatchOutputCollector的默认实现类BatchOutputCollectorImpl,该类是一个代理类，内部封装了OutputCollector变量，所有的方法都通过调用OutputCollector方法来实现。 finishBatch()方法仅当这批消息被完全处理之后才会被调用。 与IBasicBolt类似，使用IBatchBolt也不需要关心何时该对收到的信息进行Ack等操作，Storm框架内部通过BatchBoltExecutor自动帮我们实现了这些功能。BatchBoltExecutor也实现了IRichBolt接口，它会为每个Batch创建与之对应的BatchBolt对象。同时还实现了FinishedCallBack和TimeoutCallback接口。BatchBoltExecutor的代码实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class BatchBoltExecutor implements IRichBolt, FinishedCallback,TimeoutCallback&#123; public static Logger LOG = LoggerFactory.getLogger(BatchBoltExecutor.class); byte[] _boltSet; Map&lt;Object, IBatchBolt&gt; _openTransactions; Map _conf; TopologyContext _context; BatchOutputCollector _collector; public BatchBoltExecutor(IBatchBolt bolt)&#123; _boltSet= Utils.serialize(bolt); &#125; public void prepare(Map conf, TopologyContext context, OutputCollector collector)&#123; _conf=conf; _context=context; _collector= new BatchOutputCollectorImpl(collector); _openTransactions= new HashMap&lt;Object, IBatchBolt&gt;(); &#125; public void execute(Tuple input)&#123; Object id = input.getValue(0); IBatchBolt bolt = getBatchBolt(id); try&#123; bolt.execute(input); _collector.ack(input); &#125;catch(FailedException e)&#123; LOG.error(\"Failed to process tuple in batch\",e); _collector.fail(input); &#125; &#125; public void cleanup()&#123;&#125; public void finishedId(Object id)&#123; IBatchBolt bolt = getBatchBolt(id); _openTransactions.remove(id); bolt.finishBatch(); &#125; public void timeoutId(Object id)&#123; _openTransactions.remove(id); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer)&#123; newTransactionalBolt().declareOutputFields(declarer); &#125; public Map&lt;String, Object&gt; getComponentConfiguration()&#123; return new newTransactionalBolt().getComponentConfiguration(); &#125; private IBatchBolt getBatchBolt(Object id)&#123; IBatchBolt bolt = _openTransactions.get(id); if(bolt == null)&#123; bolt = new newTransactionalBolt(); bolt.prepare(_conf,_context,_collector,id); _openTransactions.put(id,bolt); &#125; return bolt; &#125; private IBatchBolt newTransactionalBolt()&#123; return (IBatchBolt)Utils.deserialize(_boltSet); &#125;&#125; 第3行是内含的BatchBolt对象的序列化字节数组。第17~27行实现了execute()方法，它规定输入消息的第一列用于标识Batch的id。第29~33行实现了FinishedCallback接口，这里调用finishBatch()方法清理BatchBolt对象。可以看出，BatchBolt对象在不同的Batch之间是不重复使用的。第34~36行实现了TimeoutCallback接口,仅仅将缓存的BatchBolt删除，这对于清理不再使用的BatchBolt对象是很关键的。第52~54行通过反序列化生成一个IBatchBolt对象。Storm通过反序列化对象的方式来弥补不断创建IBatchBolt对象所带来的负担。 Spout接口Storm中与spout相关的接口主要是ISpout和IRichSpout，下图描述了它们之间的关系： ISpout接口接口ISpout定义了Spout应该实现的功能集合:123456789public interface ISpout extends Serializable &#123; void open(Map conf, TopologyContext context, SpoutOutputCollector collector); void close(); void activate(); void deactivate(); void nextTuple(); void ack(Object msgId); void fail(Object msgId);&#125; 关于各个函数的功能，源代码中的注释部分已经给出了详细的描述，这里不再赘述。其中，nextTuple()由于和ack()、nextTuple()是在一个线程被调用的，如果nextTuple阻塞的话，其他方法也将被阻塞。因此，该方法必须是非阻塞的，任何Spout都将使用nextTuple来发送信息。 ISpout的fail和ack方法仅仅给出了发送消息时所对应的MessageId,而没有具体给出消息内容，表明如果要实现消息重传的话，用户需要自己来维护哪些已经发送的消息。 当Spout被设置为活跃或者不活跃时，会分别调用activate()和deactivate()方法将状态通知给用户代码。这样当Spout处于非活跃的状态时，nextTuple不会被调用。 open()方法的第3个参数是Spout的输出收集器SpoutOutputCollector，Storm只定义了一个Spout的输出收集器接口ISpoutOutputCollector，SpoutOutputCollector是它的默认实现类。首先看一下ISpoutOutputCollector的代码定义：emit()方法用来向外发送数据，它的返回值是该消息所有发送目标的任务实例集合。emitDirect()方法的输入列表与emit()类似，主要区别在于使用前者时，只有由参数taskId所指定的任务实例才能接收到这条消息。SpoutOutputCollector类实际上是一个代理类，本身也封装了一个ISpoutOutputCollector对象，所有的操作实际上都是通过该对象来实现的。除此之外，它还提供了一些重载方法。12345public interface ISpoutOutputCollector extends IErrorReporter&#123; List&lt;Integer&gt; emit(String streamId, List&lt;Object&gt; tuple, Object messageId); void emitDirect(int taskId, String streamId, List&lt;Object&gt; tuple, Object messageId); long getPendingCount();&#125; IRich接口IRichSpout需要同时实现IComponent和ISpout接口，因此它是一个具有Spout功能的组件，其定义如下：123public interface IRichSpout extends ISpout,IComponent&#123; &#125;","categories":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}],"tags":[{"name":"Storm","slug":"Storm","permalink":"http://tankcat2.com/tags/Storm/"}],"keywords":[{"name":"Techniques","slug":"Techniques","permalink":"http://tankcat2.com/categories/Techniques/"}]}]}